{"cells": [{"cell_type": "code", "execution_count": 9, "id": "4df257c3", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:05:18.667773Z", "start_time": "2021-11-17T18:05:18.663641Z"}}, "outputs": [], "source": ["from __future__ import print_function\n", "import datetime\n", "import io\n", "import os\n", "import sys\n", "import time\n"]}, {"cell_type": "code", "execution_count": 10, "id": "cc1e87ca", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:05:20.627189Z", "start_time": "2021-11-17T18:05:18.669964Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Defaulting to user installation because normal site-packages is not writeable\n", "Requirement already satisfied: config in /home/c3/.local/lib/python3.7/site-packages (0.5.1)\n"]}], "source": ["!pip install config\n"]}, {"cell_type": "code", "execution_count": 11, "id": "6b8fd63d", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:05:20.635876Z", "start_time": "2021-11-17T18:05:20.630440Z"}}, "outputs": [], "source": ["sys.path.append('/home/c3/.local/lib/python3.7/site-packages/')\n", "import config"]}, {"cell_type": "code", "execution_count": 12, "id": "d30f403d", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:05:20.671361Z", "start_time": "2021-11-17T18:05:20.637969Z"}}, "outputs": [{"data": {"text/plain": ["'koz8n5TKez2RR0hPOWsBR4w/PImkt4yztB7PyVJi99EmxAybu+/U9j+vubTYi280AnqE+DrPLsklG/zY/3bgnQ=='"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["bar = config._BATCH_ACCOUNT_KEY\n", "type(bar)\n", "bar"]}, {"cell_type": "raw", "id": "881d7cc4", "metadata": {}, "source": ["try:\n", "    input = raw_input\n", "except NameError:\n", "    pass"]}, {"cell_type": "code", "execution_count": 13, "id": "ea556217", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:05:21.352334Z", "start_time": "2021-11-17T18:05:20.673512Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["/home/c3/jupyter_root_dir/scratch/Greg/batch\r\n"]}], "source": ["!pwd"]}, {"cell_type": "code", "execution_count": 14, "id": "73657977", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:05:21.980996Z", "start_time": "2021-11-17T18:05:21.354782Z"}}, "outputs": [], "source": ["#!pip install azure-batch==11.0.0\n", "#!pip install azure-storage-blob==12.8.1\n", "from azure.core.exceptions import ResourceExistsError\n", "\n", "from azure.storage.blob import (\n", "    BlobServiceClient,\n", "    BlobSasPermissions,\n", "    generate_blob_sas\n", ")\n", "\n", "from azure.batch import BatchServiceClient\n", "from azure.batch.batch_auth import SharedKeyCredentials\n", "import azure.batch.models as batchmodels"]}, {"cell_type": "code", "execution_count": null, "id": "342a98cb", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": 15, "id": "e84ef89d", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:05:22.018816Z", "start_time": "2021-11-17T18:05:21.983206Z"}}, "outputs": [], "source": ["\n", "\n", "\n", "# Update the Batch and Storage account credential strings in config.py with values\n", "# unique to your accounts. These are used when constructing connection strings\n", "# for the Batch and Storage client objects.\n", "\n", "def query_yes_no(question, default=\"yes\"):\n", "    \"\"\"\n", "    Prompts the user for yes/no input, displaying the specified question text.\n", "\n", "    :param str question: The text of the prompt for input.\n", "    :param str default: The default if the user hits <ENTER>. Acceptable values\n", "    are 'yes', 'no', and None.\n", "    :rtype: str\n", "    :return: 'yes' or 'no'\n", "    \"\"\"\n", "    valid = {'y': 'yes', 'n': 'no'}\n", "    if default is None:\n", "        prompt = ' [y/n] '\n", "    elif default == 'yes':\n", "        prompt = ' [Y/n] '\n", "    elif default == 'no':\n", "        prompt = ' [y/N] '\n", "    else:\n", "        raise ValueError(\"Invalid default answer: '{}'\".format(default))\n", "\n", "    while 1:\n", "        choice = input(question + prompt).lower()\n", "        if default and not choice:\n", "            return default\n", "        try:\n", "            return valid[choice[0]]\n", "        except (KeyError, IndexError):\n", "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n", "\n", "\n", "def print_batch_exception(batch_exception):\n", "    \"\"\"\n", "    Prints the contents of the specified Batch exception.\n", "\n", "    :param batch_exception:\n", "    \"\"\"\n", "    print('-------------------------------------------')\n", "    print('Exception encountered:')\n", "    if batch_exception.error and \\\n", "            batch_exception.error.message and \\\n", "            batch_exception.error.message.value:\n", "        print(batch_exception.error.message.value)\n", "        if batch_exception.error.values:\n", "            print()\n", "            for mesg in batch_exception.error.values:\n", "                print('{}:\\t{}'.format(mesg.key, mesg.value))\n", "    print('-------------------------------------------')\n", "\n", "\n", "def upload_file_to_container(blob_service_client, container_name, file_path):\n", "    \"\"\"\n", "    Uploads a local file to an Azure Blob storage container.\n", "\n", "    :param blob_service_client: A blob service client.\n", "    :type blob_service_client: `azure.storage.blob.BlobServiceClient`\n", "    :param str container_name: The name of the Azure Blob storage container.\n", "    :param str file_path: The local path to the file.\n", "    :rtype: `azure.batch.models.ResourceFile`\n", "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch\n", "    tasks.\n", "    \"\"\"\n", "    blob_name = os.path.basename(file_path)\n", "    blob_client = blob_service_client.get_blob_client(container_name, blob_name)\n", "\n", "    print('Uploading file {} to container [{}]...'.format(file_path,\n", "                                                          container_name))\n", "\n", "    with open(file_path, \"rb\") as data:\n", "        blob_client.upload_blob(data, overwrite=True)\n", "\n", "    sas_token = generate_blob_sas(\n", "        config._STORAGE_ACCOUNT_NAME,\n", "        container_name,\n", "        blob_name,\n", "        account_key=config._STORAGE_ACCOUNT_KEY,\n", "        permission=BlobSasPermissions(read=True),\n", "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=2)\n", "    )\n", "\n", "    sas_url = generate_sas_url(\n", "        config._STORAGE_ACCOUNT_NAME,\n", "        config._STORAGE_ACCOUNT_DOMAIN,\n", "        container_name,\n", "        blob_name,\n", "        sas_token\n", "    )\n", "\n", "    return batchmodels.ResourceFile(\n", "        http_url=sas_url,\n", "        file_path=blob_name\n", "    )\n", "\n", "\n", "def generate_sas_url(\n", "    account_name, account_domain, container_name, blob_name, sas_token\n", "):\n", "    return \"https://{}.{}/{}/{}?{}\".format(\n", "        account_name,\n", "        account_domain,\n", "        container_name,\n", "        blob_name,\n", "        sas_token\n", "    )\n", "\n", "\n", "def create_pool(batch_service_client, pool_id):\n", "    \"\"\"\n", "    Creates a pool of compute nodes with the specified OS settings.\n", "\n", "    :param batch_service_client: A Batch service client.\n", "    :type batch_service_client: `azure.batch.BatchServiceClient`\n", "    :param str pool_id: An ID for the new pool.\n", "    :param str publisher: Marketplace image publisher\n", "    :param str offer: Marketplace image offer\n", "    :param str sku: Marketplace image sku\n", "    \"\"\"\n", "    print('Creating pool [{}]...'.format(pool_id))\n", "\n", "    # Create a new pool of Linux compute nodes using an Azure Virtual Machines\n", "    # Marketplace image. For more information about creating pools of Linux\n", "    # nodes, see:\n", "    # https://azure.microsoft.com/documentation/articles/batch-linux-nodes/\n", "    new_pool = batchmodels.PoolAddParameter(\n", "        id=pool_id,\n", "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n", "            image_reference=batchmodels.ImageReference(\n", "                publisher=\"canonical\",\n", "                offer=\"0001-com-ubuntu-server-focal\",\n", "                sku=\"20_04-lts\",\n", "                version=\"latest\"\n", "            ),\n", "            node_agent_sku_id=\"batch.node.ubuntu 20.04\"),\n", "        vm_size=config._POOL_VM_SIZE,\n", "        target_dedicated_nodes=config._POOL_NODE_COUNT\n", "    )\n", "    batch_service_client.pool.add(new_pool)\n", "\n", "\n", "def create_job(batch_service_client, job_id, pool_id):\n", "    \"\"\"\n", "    Creates a job with the specified ID, associated with the specified pool.\n", "\n", "    :param batch_service_client: A Batch service client.\n", "    :type batch_service_client: `azure.batch.BatchServiceClient`\n", "    :param str job_id: The ID for the job.\n", "    :param str pool_id: The ID for the pool.\n", "    \"\"\"\n", "    print('Creating job [{}]...'.format(job_id))\n", "\n", "    job = batchmodels.JobAddParameter(\n", "        id=job_id,\n", "        pool_info=batchmodels.PoolInformation(pool_id=pool_id))\n", "\n", "    batch_service_client.job.add(job)\n", "\n", "\n", "def add_tasks(batch_service_client, job_id, input_files):\n", "    \"\"\"\n", "    Adds a task for each input file in the collection to the specified job.\n", "\n", "    :param batch_service_client: A Batch service client.\n", "    :type batch_service_client: `azure.batch.BatchServiceClient`\n", "    :param str job_id: The ID of the job to which to add the tasks.\n", "    :param list input_files: A collection of input files. One task will be\n", "     created for each input file.\n", "    :param output_container_sas_token: A SAS token granting write access to\n", "    the specified Azure Blob storage container.\n", "    \"\"\"\n", "\n", "    print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n", "\n", "    tasks = list()\n", "\n", "    for idx, input_file in enumerate(input_files):\n", "\n", "        command = \"/bin/bash -c \\\"cat {}\\\"\".format(input_file.file_path)\n", "        tasks.append(batchmodels.TaskAddParameter(\n", "            id='Task{}'.format(idx),\n", "            command_line=command,\n", "            resource_files=[input_file]\n", "        )\n", "        )\n", "\n", "    batch_service_client.task.add_collection(job_id, tasks)\n", "\n", "\n", "def wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n", "    \"\"\"\n", "    Returns when all tasks in the specified job reach the Completed state.\n", "\n", "    :param batch_service_client: A Batch service client.\n", "    :type batch_service_client: `azure.batch.BatchServiceClient`\n", "    :param str job_id: The id of the job whose tasks should be to monitored.\n", "    :param timedelta timeout: The duration to wait for task completion. If all\n", "    tasks in the specified job do not reach Completed state within this time\n", "    period, an exception will be raised.\n", "    \"\"\"\n", "    timeout_expiration = datetime.datetime.now() + timeout\n", "\n", "    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\"\n", "          .format(timeout), end='')\n", "\n", "    while datetime.datetime.now() < timeout_expiration:\n", "        print('.', end='')\n", "        sys.stdout.flush()\n", "        tasks = batch_service_client.task.list(job_id)\n", "\n", "        incomplete_tasks = [task for task in tasks if\n", "                            task.state != batchmodels.TaskState.completed]\n", "        if not incomplete_tasks:\n", "            print()\n", "            return True\n", "        else:\n", "            time.sleep(1)\n", "\n", "    print()\n", "    raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within \"\n", "                       \"timeout period of \" + str(timeout))\n", "\n", "\n", "def print_task_output(batch_service_client, job_id, encoding=None):\n", "    \"\"\"\n", "    Prints the stdout.txt file for each task in the job.\n", "\n", "    :param batch_client: The batch client to use.\n", "    :type batch_client: `batchserviceclient.BatchServiceClient`\n", "    :param str job_id: The id of the job with task output files to print.\n", "    \"\"\"\n", "\n", "    print('Printing task output...')\n", "\n", "    tasks = batch_service_client.task.list(job_id)\n", "\n", "    for task in tasks:\n", "\n", "        node_id = batch_service_client.task.get(\n", "            job_id, task.id).node_info.node_id\n", "        print(\"Task: {}\".format(task.id))\n", "        print(\"Node: {}\".format(node_id))\n", "\n", "        stream = batch_service_client.file.get_from_task(\n", "            job_id, task.id, config._STANDARD_OUT_FILE_NAME)\n", "\n", "        file_text = _read_stream_as_string(\n", "            stream,\n", "            encoding)\n", "        print(\"Standard output:\")\n", "        print(file_text)\n", "\n", "\n", "def _read_stream_as_string(stream, encoding):\n", "    \"\"\"\n", "    Read stream as string\n", "\n", "    :param stream: input stream generator\n", "    :param str encoding: The encoding of the file. The default is utf-8.\n", "    :return: The file content.\n", "    :rtype: str\n", "    \"\"\"\n", "    output = io.BytesIO()\n", "    try:\n", "        for data in stream:\n", "            output.write(data)\n", "        if encoding is None:\n", "            encoding = 'utf-8'\n", "        return output.getvalue().decode(encoding)\n", "    finally:\n", "        output.close()"]}, {"cell_type": "code", "execution_count": 16, "id": "ba52a380", "metadata": {"ExecuteTime": {"end_time": "2021-11-17T18:08:38.907914Z", "start_time": "2021-11-17T18:05:22.021069Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Sample start: 2021-11-17 18:05:22\n", "\n", "Uploading file /home/c3/jupyter_root_dir/scratch/Greg/batch/taskdata0.txt to container [input]...\n", "Uploading file /home/c3/jupyter_root_dir/scratch/Greg/batch/taskdata1.txt to container [input]...\n", "Uploading file /home/c3/jupyter_root_dir/scratch/Greg/batch/taskdata2.txt to container [input]...\n", "Creating pool [PythonQuickstartPool2]...\n", "Creating job [PythonQuickstartJob2]...\n", "Adding 3 tasks to job [PythonQuickstartJob2]...\n", "Monitoring all tasks for 'Completed' state, timeout in 0:30:00...................................................................................................................................\n", "  Success! All tasks reached the 'Completed' state within the specified timeout period.\n", "Printing task output...\n", "Task: Task0\n", "Node: tvmps_40c580e1f31473552efec26f5c16f79be62ab6ab4c8ffc8d4cdfe7ae79b7b675_d\n", "Standard output:\n", "\ufeffWith support for Linux, Windows Server, SQL Server, Oracle, IBM, and SAP, Azure Virtual Machines gives you the flexibility of virtualization for a wide range of computing solutions\u2014development and testing, running applications, and extending your datacenter. It\u2019s the freedom of open-source software configured the way you need it. It\u2019s as if it was another rack in your datacenter, giving you the power to deploy an application in minutes instead of weeks.\n", "\n", "It\u2019s all about choice for your virtual machines. Choose Linux or Windows. Choose to be on-premises, in the cloud, or both. Choose your own virtual machine image or download a certified pre-configured image in our marketplace. With Virtual Machines, you\u2019re in control.\n", "\n", "Combine the performance of a world-class supercomputer with the scalability of the cloud. Scale from one to thousands of virtual machine instances. Plus, with the growing number of regional Azure datacenters, easily scale globally so you\u2019re closer to where your customers are.\n", "\n", "Keep your budget in check with low-cost, per-minute billing. You only pay for the compute time you use.\n", "\n", "We\u2019ll help you encrypt sensitive data, protect virtual machines from viruses and malware, secure network traffic, and meet regulatory and compliance requirements.\n", "Task: Task1\n", "Node: tvmps_40c580e1f31473552efec26f5c16f79be62ab6ab4c8ffc8d4cdfe7ae79b7b675_d\n", "Standard output:\n", "\ufeffBatch processing began with mainframe computers and punch cards. Today it still plays a central role in business, engineering, science, and other pursuits that require running lots of automated tasks\u2014processing bills and payroll, calculating portfolio risk, designing new products, rendering animated films, testing software, searching for energy, predicting the weather, and finding new cures for disease. Previously only a few had access to the computing power for these scenarios. With Azure Batch, that power is available to you when you need it, without any capital investment.\n", "\n", "Choose the operating system and development tools you need to run your large-scale jobs on Batch. Batch provides a consistent job scheduling and management experience whether you select Windows Server or Linux compute nodes, but lets you take advantage of the unique features of each environment. With Windows, use your existing Windows-based code, including .NET, to run large-scale compute jobs in Azure. With Linux, choose from popular distributions including CentOS, Ubuntu, and SUSE Linux Enterprise Server to run your compute jobs, or use Docker containers to lift and shift your applications. Batch provides SDKs and supports a range of development tools including Python and Java.\n", "\n", "Batch runs the applications that you use on workstations and clusters today. It\u2019s easy to cloud-enable your executables and scripts to scale out. Batch provides a queue to receive the work that you want to run and executes your applications. Describe the data that need to be moved to the cloud for processing, how the data should be distributed, what parameters to use for each task, and the command to start the process. Think about this like an assembly line with multiple applications. Batch makes it easy to share data between steps and manage the execution as a whole.\n", "\n", "You use a workstation today, maybe a small cluster, or you wait in a queue to run your jobs. What if you had access to 16 cores, 100 cores, 10,000 cores, or even 100,000 cores when you needed them, and only had to pay for what you used? With Batch you can. Avoid the bottlenecks and waiting that limit your imagination. What could you do on Azure that you can\u2019t do today?\n", "Task: Task2\n", "Node: tvmps_40c580e1f31473552efec26f5c16f79be62ab6ab4c8ffc8d4cdfe7ae79b7b675_d\n", "Standard output:\n", "\ufeffAzure Storage offers a set of storage services for all your business needs. Choose from Blob Storage (Object Storage) for unstructured data, File Storage for SMB-based cloud file shares, Table Storage for NoSQL data, Queue Storage to reliably store messages, and Premium Storage for high-performance, low-latency block storage for I/O-intensive workloads running in Azure Virtual Machines.\n", "\n", "Storage keeps pace with your growing data needs, delivering petabytes of storage for the largest scenarios. Whether you're building modern applications or a high-scale big data application, Storage can handle it.\n", "\n", "Storage is available in more regions than any other public cloud offering, letting you store your data where it makes the most business sense. Scale up or across data centers as needed, and be closer to your customers for faster access and better performance.\n", "\n", "Storage automatically replicates your data and maintains multiple copies\u2014either in a single region or globally with geo-redundancy\u2014to help guard against unexpected hardware failures.\n", "\n", "Deleting container [input]...\n", "\n", "Sample end: 2021-11-17 18:07:35\n", "Elapsed time: 0:02:13\n", "\n", "Delete job? [Y/n] Y\n", "Delete pool? [Y/n] Y\n", "\n", "Press ENTER to exit...\n"]}], "source": ["\n", "\n", "\n", "if __name__ == '__main__':\n", "\n", "    start_time = datetime.datetime.now().replace(microsecond=0)\n", "    print('Sample start: {}'.format(start_time))\n", "    print()\n", "\n", "    # Create the blob client, for use in obtaining references to\n", "    # blob storage containers and uploading files to containers.\n", "    blob_service_client = BlobServiceClient(\n", "        account_url=\"https://{}.{}/\".format(\n", "            config._STORAGE_ACCOUNT_NAME,\n", "            config._STORAGE_ACCOUNT_DOMAIN\n", "        ),\n", "        credential=config._STORAGE_ACCOUNT_KEY\n", "    )\n", "\n", "    # Use the blob client to create the containers in Azure Storage if they\n", "    # don't yet exist.\n", "    input_container_name = 'input'\n", "    try:\n", "        blob_service_client.create_container(input_container_name)\n", "    except ResourceExistsError:\n", "        pass\n", "\n", "    # The collection of data files that are to be processed by the tasks.\n", "    input_file_paths = [os.path.join('/home/c3/jupyter_root_dir/scratch/Greg/batch/', 'taskdata0.txt'),\n", "                        os.path.join('/home/c3/jupyter_root_dir/scratch/Greg/batch/', 'taskdata1.txt'),\n", "                        os.path.join('/home/c3/jupyter_root_dir/scratch/Greg/batch/', 'taskdata2.txt')]\n", "\n", "    # Upload the data files.\n", "    input_files = [\n", "        upload_file_to_container(blob_service_client, input_container_name, file_path)\n", "        for file_path in input_file_paths]\n", "\n", "    # Create a Batch service client. We'll now be interacting with the Batch\n", "    # service in addition to Storage\n", "    credentials = SharedKeyCredentials(config._BATCH_ACCOUNT_NAME,\n", "        config._BATCH_ACCOUNT_KEY)\n", "\n", "    batch_client = BatchServiceClient(\n", "        credentials,\n", "        batch_url=config._BATCH_ACCOUNT_URL)\n", "\n", "    try:\n", "        # Create the pool that will contain the compute nodes that will execute the\n", "        # tasks.\n", "        create_pool(batch_client, config._POOL_ID)\n", "\n", "        # Create the job that will run the tasks.\n", "        create_job(batch_client, config._JOB_ID, config._POOL_ID)\n", "\n", "        # Add the tasks to the job.\n", "        add_tasks(batch_client, config._JOB_ID, input_files)\n", "\n", "        # Pause execution until tasks reach Completed state.\n", "        wait_for_tasks_to_complete(batch_client,\n", "                                   config._JOB_ID,\n", "                                   datetime.timedelta(minutes=30))\n", "\n", "        print(\"  Success! All tasks reached the 'Completed' state within the \"\n", "              \"specified timeout period.\")\n", "\n", "        # Print the stdout.txt and stderr.txt files for each task to the console\n", "        print_task_output(batch_client, config._JOB_ID)\n", "\n", "    except batchmodels.BatchErrorException as err:\n", "        print_batch_exception(err)\n", "        raise\n", "\n", "    # Clean up storage resources\n", "    print('Deleting container [{}]...'.format(input_container_name))\n", "    blob_service_client.delete_container(input_container_name)\n", "\n", "    # Print out some timing info\n", "    end_time = datetime.datetime.now().replace(microsecond=0)\n", "    print()\n", "    print('Sample end: {}'.format(end_time))\n", "    print('Elapsed time: {}'.format(end_time - start_time))\n", "    print()\n", "\n", "    # Clean up Batch resources (if the user so chooses).\n", "    if query_yes_no('Delete job?') == 'yes':\n", "        batch_client.job.delete(config._JOB_ID)\n", "\n", "    if query_yes_no('Delete pool?') == 'yes':\n", "        batch_client.pool.delete(config._POOL_ID)\n", "\n", "    print()\n", "    input('Press ENTER to exit...')"]}, {"cell_type": "code", "execution_count": null, "id": "24890636", "metadata": {}, "outputs": [], "source": []}], "metadata": {"has_local_update": false, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "py-py-greg-azure-ids", "language": "Python", "name": "py-py-greg-azure-ids"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}, "last_sync_time": "2021-12-01T21:05:26.674884"}, "nbformat": 4, "nbformat_minor": 5}