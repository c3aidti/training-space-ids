{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Advanced Tutorial: ML Scoring Metrics\n", "\n", "This tutorial goes in depth on the various types of `MLScoringMetrics`, and how to use them to score machine learning models.\n", "\n", "You will learn how to:\n", "1. **Call a scoring metric directly**, to test how it works.\n", "1. **Configure scoring metrics on an MLPipe**, used to score after training or to score on new test data.\n", "1. Disable automatic **scoring after training**.\n", "1. **Configure** the behavior of a scoring metric via **parameters** on the metric.\n", "1. Use a scoring metric from the **sklearn** library.\n", "1. Use scoring metrics that do **not require ground truth** to score.\n", "1. Create your own **custom scoring metrics**.\n", "\n", "This is a self-contained notebook, but it will skip the explanation of some of the common setup steps (connecting to c3, creating datasets, creating a model) that were covered in the *Getting Started Tutorial: ML Pipeline* notebook (`TutorialIntroMLPipeline.ipynb`). "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setup\n", "\n", "Before running the notebook below, please make sure you are in the `py-sklearn_3_0_0` kernel (Kernel -> Change kernel -> py-sklearn_3_0_0). If this kernel is not listed, install the kernel (Kernel -> Manage Kernels -> py-sklearn_3_0_0)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Create datasets"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:10:57.432908Z", "start_time": "2020-09-21T15:10:56.934483Z"}}, "outputs": [], "source": ["from sklearn.datasets import load_iris\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:10:57.445905Z", "start_time": "2020-09-21T15:10:57.435055Z"}}, "outputs": [], "source": ["iris = load_iris()\n", "# Tuple of (training X, test X, training y, test y)\n", "datasets_np = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:10:57.464676Z", "start_time": "2020-09-21T15:10:57.449494Z"}}, "outputs": [], "source": ["# Train and test datasets\n", "XTrain, XTest, yTrain, yTest = [c3.Dataset.fromPython(pythonData=ds_np) for ds_np in datasets_np]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Create a model"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:10:57.490029Z", "start_time": "2020-09-21T15:10:57.467325Z"}}, "outputs": [], "source": ["logisticRegression = c3.SklearnPipe(\n", "                        name=\"logisticRegression\",\n", "                        technique=c3.SklearnTechnique(\n", "                            name=\"linear_model.LogisticRegression\",\n", "                            processingFunctionName=\"predict\",\n", "                            hyperParameters={\"random_state\": 42,\n", "                                             \"C\": .1}  # regularization to get errors\n", "                        ),\n", "                     )"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:10:59.586394Z", "start_time": "2020-09-21T15:10:57.492409Z"}}, "outputs": [{"data": {"text/plain": ["c3.SklearnPipe(\n", " name='logisticRegression',\n", " implType='SklearnPipeV7d13',\n", " noTrainScore=False,\n", " typeVersion='7.14.0',\n", " untrainableOverride=False,\n", " technique=c3.SklearnTechnique(\n", "             name='linear_model.LogisticRegression',\n", "             hyperParameters=c3.Mapp<string, any>({'C': 0.1,\n", "                               'random_state': 42}),\n", "             processingFunctionName='predict',\n", "             keepInputColumnIndices=False),\n", " trainedModel=c3.MLTrainedModelArtifact(\n", "                model='eJxlVFtv3EQY9SbbC+69AZqW0AspJS3pkk3bUNqCQ1MIxGVLDRSrDwxjr3dt8Np77HGTfUBKqHabLUIIoVY0XFTaFxDi0nKRQEgrzfwTXvkRMJ5sQcBYtj+fsb/znTnfeHHATd8OPZpEpTCI5J004qoXlkgY14OUBa5+th9YXj3x0jSIIx3awSUU3sHAmL1O07SmF9GQtTBoD8incBJFuyiDakZDrOnagzJmcYi1s8auqd//GJk5bBckNIN1s8YvN/KxYm+SQC1gJIiYl7hek2H9sr1Ngn8DJHWprLCO+8yCvVHOuCFNUzLvBXWfQa8oLKFRNW6QlFHmYYN5yF4rwTQOL3kJNtpr8uqcWj3FJnu9jBt0gQQyPTabVXtDDmQhC4hKjC1KA81YjK1KpczhxKmHbaZm6/J5niaKKWHY3lVEEXkrdlIMVVT2sEwSyoIY91fszWq25lGWyTWUKgkeMIvqNcUmMTzoVoNQLnx+1UndY4QylujY8S88aDTjhOUmZaGnY9geyVNnjWar5MaJVyJKA00S2iJZgzIfO9vYZeEhtT4k8dw4SlmSuQwjHTxsYbervtajqvpIxx5Ta2Ovv8MfVrKJK/vBTbGvjUcsjCqhXpSD2N/BoxYOKDcdPKamQik5KmOsg4MWDl3B4xbGx8yCOdjG4T5TlbWasvSS6pbacTxhambhCiYslMfMQZXtFCYrlcrcn3Koi6kxHHG6/gF7p/bfIb43+tE0jvqS+JiFKYYnHeW3G3s1guP+bl/pesqXNZ2wcFLVZBY7OOWX87zX80x3j50Wt0VHHh/y3pkRfv2C+GKvuGbwr8/xT/kt8e55cYf3mmKFf7WwRfwkPuG9UXFtYV5c5ov8B2M//4B/VhU/t8RHvEfn+B3RvSR6/K7hisvi6pC4Kb4Ubd67WBDL+rh4X3QN/qtkWt7Dv4v4Cu8VTxamxHv7pKDt/PNZscR7TNw28HQu6hkLBsO0o1rvn02BZ+8pO50rm7Fwpr/az63qGpWvm3xpWNwaHx2a5r/RN8VN/o34VvxoSEk3Ji6Kj98YFot4PieZtfACw4uOMj5Se4Ng7h6DmTOctfBSzlBoo+KvWhgcxblVC1+2cF5a6P/fPEuZl++oE/LEKznbqxZeY7jg2FvzNuv/hYjcZvk/Bq+rdpooTR4plWFnTukvHieaKg==',\n", "                description='trained from '\n", "                             'technique:linear_model.LogisticRegression',\n", "                parameters=c3.Mapp<string, any>({'classes_': c3.Arry<double>([0.0, 1.0, 2.0]),\n", "                             'coef_': c3.Arry<[double]>([c3.Arry<double>([-0.27775737946725976,\n", "                                        0.33013184982013777,\n", "                                        -1.0824610092140385,\n", "                                        -0.4368323019365681]),\n", "                                       c3.Arry<double>([0.07618352572247437,\n", "                                        -0.30431899858216327,\n", "                                        0.07421057151922518,\n", "                                        -0.171169290222248]),\n", "                                       c3.Arry<double>([0.2015738537447856,\n", "                                        -0.025812851237974256,\n", "                                        1.0082504376948132,\n", "                                        0.6080015921588162])]),\n", "                             'intercept_': c3.Arry<double>([4.784350903268499,\n", "                                            1.3082712902743046,\n", "                                            -6.092622193542777]),\n", "                             'n_features_in_': 4,\n", "                             'n_iter_': c3.Arry<int>([58])})))"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["# NotVerify: result\n", "trainedLr = logisticRegression.train(input=XTrain, targetOutput=yTrain)\n", "trainedLr"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Generate output for scoring from model"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:00.706514Z", "start_time": "2020-09-21T15:10:59.588997Z"}}, "outputs": [], "source": ["predictTest = trainedLr.process(input=XTest)"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:00.998580Z", "start_time": "2020-09-21T15:11:00.709329Z"}}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>prediction</th>\n", "      <th>0</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>7</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>8</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>9</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>10</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>11</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>12</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>13</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>14</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>15</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>16</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>17</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>18</th>\n", "      <td>1.0</td>\n", "      <td>1.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>19</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>20</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>21</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>22</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>23</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>24</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>25</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>26</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>27</th>\n", "      <td>2.0</td>\n", "      <td>2.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>28</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>29</th>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["    prediction    0\n", "0          1.0  1.0\n", "1          0.0  0.0\n", "2          2.0  2.0\n", "3          1.0  1.0\n", "4          1.0  1.0\n", "5          0.0  0.0\n", "6          1.0  1.0\n", "7          2.0  2.0\n", "8          1.0  1.0\n", "9          1.0  1.0\n", "10         2.0  2.0\n", "11         0.0  0.0\n", "12         0.0  0.0\n", "13         0.0  0.0\n", "14         0.0  0.0\n", "15         1.0  1.0\n", "16         2.0  2.0\n", "17         1.0  1.0\n", "18         1.0  1.0\n", "19         2.0  2.0\n", "20         0.0  0.0\n", "21         2.0  2.0\n", "22         0.0  0.0\n", "23         2.0  2.0\n", "24         2.0  2.0\n", "25         2.0  2.0\n", "26         2.0  2.0\n", "27         2.0  2.0\n", "28         0.0  0.0\n", "29         0.0  0.0"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["concatResult = c3.Dataset.concatenate(tensors=[predictTest, yTest], dimension=1)\n", "c3.Dataset.toPandas(dataset=concatResult)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Using Scoring Metrics Directly\n", "\n", "C3 defines several built-in machine learning scoring metrics, such as `MLAccuracyMetric`, `MLF1ScoreMetric`, `MLRSquaredMetric`.  All these scoring metrics implement a common interface, defined in the abstract Type `MLScoringMetric`.\n", "\n", "To see a list of available ML scoring metrics, run `c3ShowType(MLScoringMetric)` at the Javascript client console, and navigate to Types that mix in the `MLScoringMetric` Type using the hyperlinks under \"Used by\".  You may need to navigate a few levels down the inheritance hierarchy to see several of the built-in metrics (many are under the \"Used by\" hyperlinks under `MLScoringMetricWithTruth`)."]}, {"cell_type": "raw", "metadata": {}, "source": ["# Switch Jupyter cell type to \"Code\" to execute\n", "# If you know the name of the MLScoringMetric, you can call `help()` directly on the Type. \n", "help(c3.MLAccuracyMetric)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To use a scoring metric, you have to create an instance of the Type first."]}, {"cell_type": "code", "execution_count": 8, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:01.981266Z", "start_time": "2020-09-21T15:11:01.003957Z"}}, "outputs": [{"data": {"text/plain": ["1.0"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["accuracy_metric = c3.MLAccuracyMetric()  # this metric works for multi-class classification\n", "accuracy_metric.score(output=predictTest, targetOutput=yTest)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Configure scoring metrics on an `MLPipe`\n", "If you wish to score the machine learning model in an `MLPipe`, you need to configure a scoring metric (or multiple scoring metrics) on the `MLPipe` first.  "]}, {"cell_type": "code", "execution_count": 9, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:02.008751Z", "start_time": "2020-09-21T15:11:01.985096Z"}}, "outputs": [{"data": {"text/plain": ["c3.SklearnPipe(\n", " name='logisticRegression',\n", " implType='SklearnPipeV7d13',\n", " noTrainScore=False,\n", " scoringMetrics=c3.Mapp<string, MLScoringMetric>({'MLAccuracyMetric': c3.MLAccuracyMetric()}),\n", " typeVersion='7.14.0',\n", " untrainableOverride=False,\n", " technique=c3.SklearnTechnique(\n", "             name='linear_model.LogisticRegression',\n", "             hyperParameters=c3.Mapp<string, any>({'C': 0.1,\n", "                               'random_state': 42}),\n", "             processingFunctionName='predict',\n", "             keepInputColumnIndices=False),\n", " trainedModel=c3.MLTrainedModelArtifact(\n", "                model='eJxlVFtv3EQY9SbbC+69AZqW0AspJS3pkk3bUNqCQ1MIxGVLDRSrDwxjr3dt8Np77HGTfUBKqHabLUIIoVY0XFTaFxDi0nKRQEgrzfwTXvkRMJ5sQcBYtj+fsb/znTnfeHHATd8OPZpEpTCI5J004qoXlkgY14OUBa5+th9YXj3x0jSIIx3awSUU3sHAmL1O07SmF9GQtTBoD8incBJFuyiDakZDrOnagzJmcYi1s8auqd//GJk5bBckNIN1s8YvN/KxYm+SQC1gJIiYl7hek2H9sr1Ngn8DJHWprLCO+8yCvVHOuCFNUzLvBXWfQa8oLKFRNW6QlFHmYYN5yF4rwTQOL3kJNtpr8uqcWj3FJnu9jBt0gQQyPTabVXtDDmQhC4hKjC1KA81YjK1KpczhxKmHbaZm6/J5niaKKWHY3lVEEXkrdlIMVVT2sEwSyoIY91fszWq25lGWyTWUKgkeMIvqNcUmMTzoVoNQLnx+1UndY4QylujY8S88aDTjhOUmZaGnY9geyVNnjWar5MaJVyJKA00S2iJZgzIfO9vYZeEhtT4k8dw4SlmSuQwjHTxsYbervtajqvpIxx5Ta2Ovv8MfVrKJK/vBTbGvjUcsjCqhXpSD2N/BoxYOKDcdPKamQik5KmOsg4MWDl3B4xbGx8yCOdjG4T5TlbWasvSS6pbacTxhambhCiYslMfMQZXtFCYrlcrcn3Koi6kxHHG6/gF7p/bfIb43+tE0jvqS+JiFKYYnHeW3G3s1guP+bl/pesqXNZ2wcFLVZBY7OOWX87zX80x3j50Wt0VHHh/y3pkRfv2C+GKvuGbwr8/xT/kt8e55cYf3mmKFf7WwRfwkPuG9UXFtYV5c5ov8B2M//4B/VhU/t8RHvEfn+B3RvSR6/K7hisvi6pC4Kb4Ubd67WBDL+rh4X3QN/qtkWt7Dv4v4Cu8VTxamxHv7pKDt/PNZscR7TNw28HQu6hkLBsO0o1rvn02BZ+8pO50rm7Fwpr/az63qGpWvm3xpWNwaHx2a5r/RN8VN/o34VvxoSEk3Ji6Kj98YFot4PieZtfACw4uOMj5Se4Ng7h6DmTOctfBSzlBoo+KvWhgcxblVC1+2cF5a6P/fPEuZl++oE/LEKznbqxZeY7jg2FvzNuv/hYjcZvk/Bq+rdpooTR4plWFnTukvHieaKg==',\n", "                description='trained from '\n", "                             'technique:linear_model.LogisticRegression',\n", "                parameters=c3.Mapp<string, any>({'classes_': c3.Arry<double>([0.0, 1.0, 2.0]),\n", "                             'coef_': c3.Arry<[double]>([c3.Arry<double>([-0.27775737946725976,\n", "                                        0.33013184982013777,\n", "                                        -1.0824610092140385,\n", "                                        -0.4368323019365681]),\n", "                                       c3.Arry<double>([0.07618352572247437,\n", "                                        -0.30431899858216327,\n", "                                        0.07421057151922518,\n", "                                        -0.171169290222248]),\n", "                                       c3.Arry<double>([0.2015738537447856,\n", "                                        -0.025812851237974256,\n", "                                        1.0082504376948132,\n", "                                        0.6080015921588162])]),\n", "                             'intercept_': c3.Arry<double>([4.784350903268499,\n", "                                            1.3082712902743046,\n", "                                            -6.092622193542777]),\n", "                             'n_features_in_': 4,\n", "                             'n_iter_': c3.Arry<int>([58])})))"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["# NotVerify: result\n", "# The scoringMetricList argument below can take a list of multiple scoring metric objects\n", "trainedLr.scoringMetrics = c3.MLScoringMetric.toScoringMetricMap(\n", "                              scoringMetricList=[c3.MLAccuracyMetric()])\n", "trainedLr"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`c3.MLScoringMetric.toScoringMetricMap()` above is a convenience function, to generate the keys to the `scoringMetrics` C3 Mapp field by calling the `MLScoringMetric.toString()` member function on the scoring metric object.  Using consistent names for the scoring metric (by calling `MLScoringMetric.toString()`) will make it easier to compare scores across different models / `MLPipe`s."]}, {"cell_type": "code", "execution_count": 10, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:02.031069Z", "start_time": "2020-09-21T15:11:02.011162Z"}}, "outputs": [{"data": {"text/plain": ["'MLAccuracyMetric'"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["c3.MLAccuracyMetric().toString()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Scoring on test set"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:03.818788Z", "start_time": "2020-09-21T15:11:02.035330Z"}}, "outputs": [{"data": {"text/plain": ["c3.Mapp<string, double>({'MLAccuracyMetric': 1.0})"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["# Now you can call score() on the MLPipe\n", "scoresMapp = trainedLr.score(input=XTest, targetOutput=yTest)\n", "scoresMapp"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that the key to the score is the same as the key to the scoring metric the score was generated from."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Scoring after training\n", "If you configure a scoring metric on an `MLPipe` and set `MLPipe.noTrainScore = False` (the default value), when you call `MLPipe.train()` the scoring metric will be used to score the `MLPipe` on the entire training Dataset."]}, {"cell_type": "code", "execution_count": 12, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:06.607156Z", "start_time": "2020-09-21T15:11:03.821060Z"}}, "outputs": [{"data": {"text/plain": ["c3.SklearnPipe(\n", " name='logisticRegression',\n", " implType='SklearnPipeV7d13',\n", " noTrainScore=False,\n", " scoringMetrics=c3.Mapp<string, anyof(MLScoringMetric<any,any>)>({'MLAccuracyMetric': c3.MLAccuracyMetric()}),\n", " trainingScores=c3.Mapp<string, double>({'MLAccuracyMetric': 0.95}),\n", " typeVersion='7.14.0',\n", " untrainableOverride=False,\n", " technique=c3.SklearnTechnique(\n", "             name='linear_model.LogisticRegression',\n", "             hyperParameters=c3.Mapp<string, any>({'C': 0.1,\n", "                               'random_state': 42}),\n", "             processingFunctionName='predict',\n", "             keepInputColumnIndices=False),\n", " trainedModel=c3.MLTrainedModelArtifact(\n", "                model='eJxlVFtv3EQY9SbbC+69AZqW0AspJS3pkk3bUNqCQ1MIxGVLDRSrDwxjr3dt8Np77HGTfUBKqHabLUIIoVY0XFTaFxDi0nKRQEgrzfwTXvkRMJ5sQcBYtj+fsb/znTnfeHHATd8OPZpEpTCI5J004qoXlkgY14OUBa5+th9YXj3x0jSIIx3awSUU3sHAmL1O07SmF9GQtTBoD8incBJFuyiDakZDrOnagzJmcYi1s8auqd//GJk5bBckNIN1s8YvN/KxYm+SQC1gJIiYl7hek2H9sr1Ngn8DJHWprLCO+8yCvVHOuCFNUzLvBXWfQa8oLKFRNW6QlFHmYYN5yF4rwTQOL3kJNtpr8uqcWj3FJnu9jBt0gQQyPTabVXtDDmQhC4hKjC1KA81YjK1KpczhxKmHbaZm6/J5niaKKWHY3lVEEXkrdlIMVVT2sEwSyoIY91fszWq25lGWyTWUKgkeMIvqNcUmMTzoVoNQLnx+1UndY4QylujY8S88aDTjhOUmZaGnY9geyVNnjWar5MaJVyJKA00S2iJZgzIfO9vYZeEhtT4k8dw4SlmSuQwjHTxsYbervtajqvpIxx5Ta2Ovv8MfVrKJK/vBTbGvjUcsjCqhXpSD2N/BoxYOKDcdPKamQik5KmOsg4MWDl3B4xbGx8yCOdjG4T5TlbWasvSS6pbacTxhambhCiYslMfMQZXtFCYrlcrcn3Koi6kxHHG6/gF7p/bfIb43+tE0jvqS+JiFKYYnHeW3G3s1guP+bl/pesqXNZ2wcFLVZBY7OOWX87zX80x3j50Wt0VHHh/y3pkRfv2C+GKvuGbwr8/xT/kt8e55cYf3mmKFf7WwRfwkPuG9UXFtYV5c5ov8B2M//4B/VhU/t8RHvEfn+B3RvSR6/K7hisvi6pC4Kb4Ubd67WBDL+rh4X3QN/qtkWt7Dv4v4Cu8VTxamxHv7pKDt/PNZscR7TNw28HQu6hkLBsO0o1rvn02BZ+8pO50rm7Fwpr/az63qGpWvm3xpWNwaHx2a5r/RN8VN/o34VvxoSEk3Ji6Kj98YFot4PieZtfACw4uOMj5Se4Ng7h6DmTOctfBSzlBoo+KvWhgcxblVC1+2cF5a6P/fPEuZl++oE/LEKznbqxZeY7jg2FvzNuv/hYjcZvk/Bq+rdpooTR4plWFnTukvHieaKg==',\n", "                description='trained from '\n", "                             'technique:linear_model.LogisticRegression',\n", "                parameters=c3.Mapp<string, any>({'classes_': c3.Arry<double>([0.0, 1.0, 2.0]),\n", "                             'coef_': c3.Arry<[double]>([c3.Arry<double>([-0.27775737946725976,\n", "                                        0.33013184982013777,\n", "                                        -1.0824610092140385,\n", "                                        -0.4368323019365681]),\n", "                                       c3.Arry<double>([0.07618352572247437,\n", "                                        -0.30431899858216327,\n", "                                        0.07421057151922518,\n", "                                        -0.171169290222248]),\n", "                                       c3.Arry<double>([0.2015738537447856,\n", "                                        -0.025812851237974256,\n", "                                        1.0082504376948132,\n", "                                        0.6080015921588162])]),\n", "                             'intercept_': c3.Arry<double>([4.784350903268499,\n", "                                            1.3082712902743046,\n", "                                            -6.092622193542777]),\n", "                             'n_features_in_': 4,\n", "                             'n_iter_': c3.Arry<int>([58])})))"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["# NotVerify: result\n", "logisticRegression.scoringMetrics = c3.MLScoringMetric.toScoringMetricMap(\n", "                                        scoringMetricList=[c3.MLAccuracyMetric()])\n", "# logisticRegression.noTrainScore defaults to False.  You may also set it explicitly.\n", "trainedLr = logisticRegression.train(input=XTrain, targetOutput=yTrain)\n", "trainedLr  # notice that trainedLr.trainingScores is now populated"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The scores computed from the training data are stored in a field on the `MLPipe`, because they are a characteristic of the model.  We chose NOT to store any scores from the test data on the `MLPipe` because the test data is no intrinsically part of the model."]}, {"cell_type": "code", "execution_count": 13, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:07.692494Z", "start_time": "2020-09-21T15:11:06.609932Z"}}, "outputs": [{"data": {"text/plain": ["c3.SklearnPipe(\n", " name='logisticRegression',\n", " implType='SklearnPipeV7d13',\n", " noTrainScore=True,\n", " scoringMetrics=c3.Mapp<string, anyof(MLScoringMetric<any,any>)>({'MLAccuracyMetric': c3.MLAccuracyMetric()}),\n", " typeVersion='7.14.0',\n", " untrainableOverride=False,\n", " technique=c3.SklearnTechnique(\n", "             name='linear_model.LogisticRegression',\n", "             hyperParameters=c3.Mapp<string, any>({'C': 0.1,\n", "                               'random_state': 42}),\n", "             processingFunctionName='predict',\n", "             keepInputColumnIndices=False),\n", " trainedModel=c3.MLTrainedModelArtifact(\n", "                model='eJxlVFtv3EQY9SbbC+69AZqW0AspJS3pkk3bUNqCQ1MIxGVLDRSrDwxjr3dt8Np77HGTfUBKqHabLUIIoVY0XFTaFxDi0nKRQEgrzfwTXvkRMJ5sQcBYtj+fsb/znTnfeHHATd8OPZpEpTCI5J004qoXlkgY14OUBa5+th9YXj3x0jSIIx3awSUU3sHAmL1O07SmF9GQtTBoD8incBJFuyiDakZDrOnagzJmcYi1s8auqd//GJk5bBckNIN1s8YvN/KxYm+SQC1gJIiYl7hek2H9sr1Ngn8DJHWprLCO+8yCvVHOuCFNUzLvBXWfQa8oLKFRNW6QlFHmYYN5yF4rwTQOL3kJNtpr8uqcWj3FJnu9jBt0gQQyPTabVXtDDmQhC4hKjC1KA81YjK1KpczhxKmHbaZm6/J5niaKKWHY3lVEEXkrdlIMVVT2sEwSyoIY91fszWq25lGWyTWUKgkeMIvqNcUmMTzoVoNQLnx+1UndY4QylujY8S88aDTjhOUmZaGnY9geyVNnjWar5MaJVyJKA00S2iJZgzIfO9vYZeEhtT4k8dw4SlmSuQwjHTxsYbervtajqvpIxx5Ta2Ovv8MfVrKJK/vBTbGvjUcsjCqhXpSD2N/BoxYOKDcdPKamQik5KmOsg4MWDl3B4xbGx8yCOdjG4T5TlbWasvSS6pbacTxhambhCiYslMfMQZXtFCYrlcrcn3Koi6kxHHG6/gF7p/bfIb43+tE0jvqS+JiFKYYnHeW3G3s1guP+bl/pesqXNZ2wcFLVZBY7OOWX87zX80x3j50Wt0VHHh/y3pkRfv2C+GKvuGbwr8/xT/kt8e55cYf3mmKFf7WwRfwkPuG9UXFtYV5c5ov8B2M//4B/VhU/t8RHvEfn+B3RvSR6/K7hisvi6pC4Kb4Ubd67WBDL+rh4X3QN/qtkWt7Dv4v4Cu8VTxamxHv7pKDt/PNZscR7TNw28HQu6hkLBsO0o1rvn02BZ+8pO50rm7Fwpr/az63qGpWvm3xpWNwaHx2a5r/RN8VN/o34VvxoSEk3Ji6Kj98YFot4PieZtfACw4uOMj5Se4Ng7h6DmTOctfBSzlBoo+KvWhgcxblVC1+2cF5a6P/fPEuZl++oE/LEKznbqxZeY7jg2FvzNuv/hYjcZvk/Bq+rdpooTR4plWFnTukvHieaKg==',\n", "                description='trained from '\n", "                             'technique:linear_model.LogisticRegression',\n", "                parameters=c3.Mapp<string, any>({'classes_': c3.Arry<double>([0.0, 1.0, 2.0]),\n", "                             'coef_': c3.Arry<[double]>([c3.Arry<double>([-0.27775737946725976,\n", "                                        0.33013184982013777,\n", "                                        -1.0824610092140385,\n", "                                        -0.4368323019365681]),\n", "                                       c3.Arry<double>([0.07618352572247437,\n", "                                        -0.30431899858216327,\n", "                                        0.07421057151922518,\n", "                                        -0.171169290222248]),\n", "                                       c3.Arry<double>([0.2015738537447856,\n", "                                        -0.025812851237974256,\n", "                                        1.0082504376948132,\n", "                                        0.6080015921588162])]),\n", "                             'intercept_': c3.Arry<double>([4.784350903268499,\n", "                                            1.3082712902743046,\n", "                                            -6.092622193542777]),\n", "                             'n_features_in_': 4,\n", "                             'n_iter_': c3.Arry<int>([58])})))"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["# NotVerify: result\n", "logisticRegression.noTrainScore = True\n", "trainedLr = logisticRegression.train(input=XTrain, targetOutput=yTrain)\n", "trainedLr # notice that trainedLr.trainingScores is NOT populated"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> *NOTE*: `MLPipe.train()` will fail if the `MLPipe` is configured with a scoring metric that requires ground truth and you do not pass the `targetOutput` argument to `train()` (i.e. calling `MLPipe.train(input)` for *unsupervised* learning).  The `targetOutput` argument to `train()` serves as the ground truth for scoring."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Configuring parameters on a scoring metric\n", "Some scoring metrics have parameters that configure how they score.  These parameters will be defined as fields on the Type.  For example, try `c3ShowType(MLPrecisionMetric)` in the Javascript client console."]}, {"cell_type": "raw", "metadata": {}, "source": ["# Switch Jupyter cell type to \"Code\" to execute\n", "help(c3.MLPrecisionMetric)  # You should see the `threshold` field"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:07.710813Z", "start_time": "2020-09-21T15:11:07.698411Z"}}, "outputs": [], "source": ["# Modify data for binary classification, so we can apply binary classification metric\n", "yBinaries_np = datasets_np[2:]  # yTrain, yTest\n", "for yBinary_np in yBinaries_np:\n", "    yBinary_np[yBinary_np > 0] = 1  # Manipulate numpy data to only have 2 classes\n", "yTrainBinary, yTestBinary = [c3.Dataset.fromPython(pythonData=ds_np) for ds_np in yBinaries_np]"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:10.409277Z", "start_time": "2020-09-21T15:11:07.717291Z"}}, "outputs": [], "source": ["# Train a new model for binary classification \n", "binaryLr = c3.SklearnPipe(\n", "               name=\"logisticRegression\",\n", "               technique=c3.SklearnTechnique(\n", "                   name=\"linear_model.LogisticRegression\",\n", "                   processingFunctionName=\"predict_proba\",  # to compute the precision, we use predicted probabilities\n", "                   hyperParameters={\"random_state\": 42,\n", "                                    \"C\": 0.001},  # strong regularization to get errors\n", "               ),\n", "               scoringMetrics=c3.MLScoringMetric.toScoringMetricMap(\n", "                   scoringMetricList=[c3.MLPrecisionMetric(threshold=0.1)])\n", "            )\n", "trainedLr = binaryLr.train(input=XTrain, targetOutput=yTrainBinary)"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:12.102585Z", "start_time": "2020-09-21T15:11:10.411587Z"}}, "outputs": [{"data": {"text/plain": ["c3.Mapp<string, double>({'MLPrecisionMetric, threshold=0.1': 0.6666666666666666})"]}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": ["trainedLr.score(input=XTest, targetOutput=yTestBinary)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:15.726032Z", "start_time": "2020-09-21T15:11:12.105523Z"}, "scrolled": true}, "outputs": [{"data": {"text/plain": ["c3.Mapp<string, double>({'MLAccuracyMetric': 0.6666666666666666,\n", " 'MLPrecisionMetric, threshold=0.0': 0.6666666666666666,\n", " 'MLPrecisionMetric, threshold=0.61': 0.6896551724137931})"]}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": ["# Score Trained Model over multiple scoring metrics\n", "trainedLr.scoringMetrics = c3.MLScoringMetric.toScoringMetricMap(\n", "                               scoringMetricList=[c3.MLPrecisionMetric(), c3.MLPrecisionMetric(threshold=0.61), c3.MLAccuracyMetric()])\n", "trainedLr.score(input=XTest, targetOutput=yTestBinary)"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:17.301283Z", "start_time": "2020-09-21T15:11:15.728712Z"}}, "outputs": [{"data": {"text/plain": ["c3.Mapp<string, double>({'MLPrecisionMetric, threshold=2.0': 0.0})"]}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": ["# Change the scoring metric to an impossible to cross threshold\n", "trainedLr.scoringMetrics = c3.MLScoringMetric.toScoringMetricMap(\n", "                               scoringMetricList=[c3.MLPrecisionMetric(threshold=2)])\n", "trainedLr.score(input=XTest, targetOutput=yTestBinary)  # Should give a score of 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Category of a scoring metric"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We saw in a the previous sections that we can configure an `MLPipe` to compute scoring metrics during training or on a test dataset. \n", "We also saw that those metrics can be computed:\n", "- on target labels (like `MLAccuracyMetric`); in that case, we set the `processingFunctionName` of the pipe to `predict`; or\n", "- on output probabilities (like `MLPrecisionMetric`); in that case, we set the `processingFunctionName` to `predict_proba`.\n", "\n", "You may have noticed that in the last example, we computed the two \"kinds\" of scoring metrics with the same pipe and the same `processingFunctionName` (here `predict_proba`). So we should expect the results to be incorrect (in fact, the metric `MLAccuracyMetric` would throw an error if it had received probabilities as an input). "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In fact, the function `SklearnPipe.score()` is aware of the kind of scoring metrics it has to evaluate, and it overrides the `processingFunctionName` to compute the correct input for each metric.\n", "This is encoded in the type `MLScoringMetricCategory`:"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:17.322045Z", "start_time": "2020-09-21T15:11:17.304101Z"}}, "outputs": [{"data": {"text/plain": ["'PREDICTION'"]}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": ["c3.MLAccuracyMetric().category()  # this scoring metric expects target labels (or PREDICTIONS)"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:17.342052Z", "start_time": "2020-09-21T15:11:17.324562Z"}}, "outputs": [{"data": {"text/plain": ["'PROBABILITY'"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["c3.MLPrecisionMetric(threshold=0.5).category()  # this scoring metric expects probabilities"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The `MLPipe` that mix-in `SklearnApiCommon` (e.g. `SklearnPipe`, `XgBoostPipe`, `LightGbmPipe`, `CatBoostPipe` and `StatsModelsTsaPipe`) leverage that information to change the `processingFunctionName` during scoring, if needed."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Category override\n", "\n", "It is possible to override that behavior with the parameter `MLScoringMetric.categoryOverride`:"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:17.363877Z", "start_time": "2020-09-21T15:11:17.345401Z"}}, "outputs": [{"data": {"text/plain": ["'PREDICTION'"]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": ["precision_metric = c3.MLPrecisionMetric(threshold=0.61, categoryOverride=\"PREDICTION\")\n", "precision_metric.category()"]}, {"cell_type": "code", "execution_count": 22, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:21.008466Z", "start_time": "2020-09-21T15:11:17.367827Z"}}, "outputs": [{"data": {"text/plain": ["c3.Mapp<string, double>({'MLAccuracyMetric': 0.6666666666666666,\n", " 'MLPrecisionMetric, threshold=0.61': 0.6896551724137931,\n", " 'MLPrecisionMetric, threshold=0.61, categoryOverride=PREDICTION': 0.6666666666666666})"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": ["trainedLr.scoringMetrics = {\n", "    'MLPrecisionMetric, threshold=0.61': c3.MLPrecisionMetric(threshold=0.61),\n", "    'MLPrecisionMetric, threshold=0.61, categoryOverride=PREDICTION': precision_metric,\n", "    'MLAccuracyMetric': c3.MLAccuracyMetric()\n", "}\n", "trainedLr.score(input=XTest, targetOutput=yTestBinary)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the example above, the precision with the category override is not computed correctly since labels (0/1) were used instead of probabilities."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Data selection for scoring"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In some cases, the output of an `MLPipe` has not the same \"shape\" as the target output used to train the pipe.\n", "A simple example is the case of a binary classification pipe with its processing function name set to `predict_proba`: the target output is a dataset with one column and N rows and the output is a dataset with two columns and N rows (the first column is the probability of the class 0 and the second one is the probability of the class 1).\n", "\n", "We need a way to select the correct column to compute the score, for example if we want to compute the precision.\n", "\n", "To do so, we can use the field `dataSelector` on `MLSklearnCommonTechnique` (which is subtyped by `SklearnTechnique`, `XgBoostTechnique`, etc.).\n", "It is a mapping between a processing function name and an array of column indices: when a scoring metric requires that processing function name, these columns will be extracted and fed to the function `MLScoringMetric.score()`."]}, {"cell_type": "code", "execution_count": 23, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:23.871741Z", "start_time": "2020-09-21T15:11:21.018322Z"}}, "outputs": [{"data": {"text/plain": ["c3.Mapp<string, double>({'MLPrecisionMetric, threshold=0.1': 0.6666666666666666})"]}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": ["binaryLr = c3.SklearnPipe(\n", "               name=\"logisticRegression\",\n", "               technique=c3.SklearnTechnique(\n", "                   name=\"linear_model.LogisticRegression\",\n", "                   processingFunctionName=\"predict_proba\",  # to compute the precision, we use predicted probabilities\n", "                   hyperParameters={\"random_state\": 42,\n", "                                    \"C\": 0.001},  # strong regularization to get errors\n", "                   dataSelectorForScoring={\n", "                       \"predict_proba\": [1]  # use the column of index 1 to compute the scores (probabilities of class 1)\n", "                   }\n", "               ),\n", "               scoringMetrics=c3.MLScoringMetric.toScoringMetricMap(\n", "                   scoringMetricList=[c3.MLPrecisionMetric(threshold=0.1)])\n", "            )\n", "trainedLr = binaryLr.train(input=XTrain, targetOutput=yTrainBinary)\n", "trainedLr.trainingScores"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that for the scoring metrics that subtype `MLBinaryScoringMetricWithTruth` (e.g. `MLPrecisionMetric`), when the value of the argument `output` in `MLBinaryScoringMetricWithTruthngMetric.score(output, targetOutput)` is a dataset with two columns, then the column of index 1 is used to compute the score (so the field `dataSelectorForScoring` is superfluous in that specific case)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Using scoring metrics from `sklearn`\n", "Scikit-learn also provides several [pre-defined scoring metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).  If you choose to use these, you can configured the pre-defined Type `SklearnScoringMetric` to call scikit-learn to score your model."]}, {"cell_type": "raw", "metadata": {}, "source": ["# Switch Jupyter cell type to \"Code\" to execute\n", "help(c3.SklearnScoringMetric)  # Alternately, run c3ShowType(SklearnScoringMetric) on the Javascript console"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We will use the [F-Beta scoring metric](https://en.wikipedia.org/wiki/F1_score#Definition) to demonstrate how to use `SklearnScoringMetric` when there are configurable parameters.  For brevity, we will score directly using `yTest` and `predictTest` computed above, but `SklearnScoringMetric` can also be saved in `MLPipe.scoringMetrics`."]}, {"cell_type": "code", "execution_count": 24, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:23.883346Z", "start_time": "2020-09-21T15:11:23.874818Z"}}, "outputs": [], "source": ["# Documentation for this metric at\n", "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score\n", "from sklearn.metrics import fbeta_score  # Used below to demonstrate we are calling sklearn, getting same values"]}, {"cell_type": "code", "execution_count": 25, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:23.897392Z", "start_time": "2020-09-21T15:11:23.887455Z"}}, "outputs": [{"data": {"text/plain": ["c3.SklearnScoringMetric(\n", " parameters=c3.Mapp<string, any>({'beta': 2, 'average': 'macro'}),\n", " name='fbeta_score',\n", " higherIsWorse=False)"]}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": ["# Notice that we must set `higherIsWorse` below, used by implementation of `higherIsBetter()`\n", "fbeta = c3.SklearnScoringMetric(name='fbeta_score', higherIsWorse=False,\n", "                                parameters={'beta': 2, 'average': 'macro'})\n", "fbeta"]}, {"cell_type": "code", "execution_count": 26, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:25.377457Z", "start_time": "2020-09-21T15:11:23.900390Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["sklearn directly:     1.0\n", "SklearnScoringMetric: 1.0\n"]}], "source": ["# Compare with sklearn's fbeta_score\n", "print('sklearn directly:     ' + \n", "      str(fbeta_score(c3.Dataset.toPandas(dataset=yTest),\n", "                      c3.Dataset.toPandas(dataset=predictTest), beta=2, average='macro')))\n", "print('SklearnScoringMetric: ' +\n", "      str(fbeta.score(targetOutput=yTest, output=predictTest)))"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:26.933797Z", "start_time": "2020-09-21T15:11:25.379934Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["sklearn directly:     1.0\n", "SklearnScoringMetric: 1.0\n"]}], "source": ["# Change beta to demonstrate that parameters are actually applied\n", "fbeta1000 = c3.SklearnScoringMetric(name='fbeta_score', higherIsWorse=False,\n", "                                    parameters={'beta': 1000, 'average': 'macro'})\n", "print('sklearn directly:     ' + \n", "      str(fbeta_score(c3.Dataset.toPandas(dataset=yTest),\n", "                      c3.Dataset.toPandas(dataset=predictTest), beta=1000, average='macro')))\n", "print('SklearnScoringMetric: ' +\n", "      str(fbeta1000.score(targetOutput=yTest, output=predictTest)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> *NOTE*: `SklearnScoringMetric.toString()` will embed parameter values into the autogenerated name, used by `MLScoringMetric.toScoringMetricMap()` as keys to the returned C3 Mapp.  This allows you to configure 2 F-Beta metrics with different Beta on the same `MLPipe` for scoring, because they will have distinct names."]}, {"cell_type": "code", "execution_count": 28, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:26.956146Z", "start_time": "2020-09-21T15:11:26.936563Z"}}, "outputs": [{"data": {"text/plain": ["'SklearnScoringMetric:fbeta_score, beta=1000, average=macro'"]}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": ["fbeta1000.toString()  # The auto-generated name contains parameter values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Some `MLScoringMetric`s support the `predict_proba` processing function:"]}, {"cell_type": "code", "execution_count": 29, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:26.986804Z", "start_time": "2020-09-21T15:11:26.959155Z"}}, "outputs": [], "source": ["# Preparing pipe and datasets\n", "gbt = c3.SklearnPipe(\n", "    name=\"gbt\",\n", "    technique=c3.SklearnTechnique(\n", "        name=\"linear_model.LogisticRegression\",\n", "        processingFunctionName=\"predict\",\n", "        dataSelectorForScoring={\n", "            \"predict_proba\": [1],\n", "        },\n", "    ),\n", "    scoringMetrics=c3.MLScoringMetric.toScoringMetricMap([\n", "        c3.SklearnScoringMetric(\n", "            name='roc_auc_score',\n", "            categoryOverride=c3.MLScoringMetricCategory.PROBABILITY,\n", "        ),\n", "    ]),\n", ")\n", "dataset_X = c3.Dataset(**{\n", "    \"m_data\": [\n", "        5, 4,\n", "        55, 5,\n", "        5, 4,\n", "        55, 5\n", "    ],\n", "    \"indices\":{\n", "        \"0\":['id0','id1','id2','id3'],\n", "        \"1\":['feat0','feat1']\n", "    },\n", "    \"shape\":[4,2]\n", "})\n", "\n", "dataset_y = c3.Dataset(**{\n", "    \"m_data\":[\n", "        0,\n", "        1,\n", "        1,\n", "        1,\n", "    ],\n", "    \"indices\":{\n", "        \"0\":['id0','id1','id2','id3'],\n", "        \"1\":['target', 'target2']\n", "    },\n", "    \"shape\":[4,1]\n", "})\n", "\n", "y_true = c3.Dataset.toNumpy(dataset_y).flatten()"]}, {"cell_type": "code", "execution_count": 30, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:35.087049Z", "start_time": "2020-09-21T15:11:26.990827Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["roc_auc_score on c3 platform during training is 0.8333333333333333\n", "roc_auc_score on c3 platform is 0.8333333333333333\n"]}], "source": ["# Comparing c3 and native roc_auc_score\n", "\n", "trainedPipe = gbt.train(dataset_X, dataset_y)\n", "y_scores = trainedPipe.process(dataset_X).flattenedData()\n", "\n", "print('roc_auc_score on c3 platform during training is ' + str(trainedPipe.trainingScores['SklearnScoringMetric:roc_auc_score, categoryOverride=PROBABILITY']))\n", "print('roc_auc_score on c3 platform is ' + str(trainedPipe.score(dataset_X, dataset_y)['SklearnScoringMetric:roc_auc_score, categoryOverride=PROBABILITY']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that `gbt` is defined with `processingFunctionName='predict'`. But `predict_proba` is used for scoring because `SklearnScoringMetric` metric defined with `categoryOverride=c3.MLScoringMetricCategory.PROBABILITY`, which indicates that this scoring metric is working with probabilities.\n", "\n", "> When using an `SklearnScoringMetric` with an `MLPipe`, it is recommended to always specify a `categoryOverride` since the category of a scikit-learn scoring metric cannot be known statically.\n", "\n", "In the example above, the `roc_auc_score` expects probabilities, so we use the category `PROBABILITY`; we also specify the data selector for scoring (to extract the column of index 1 corresponding to the probabilities of the positive class).\n", "\n", "For testing with native Sklearn we need to modify pipe to return probabilities:"]}, {"cell_type": "code", "execution_count": 31, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:42.871868Z", "start_time": "2020-09-21T15:11:35.089115Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["roc_auc_score on native Sklearn 0.8333333333333333\n", "roc_auc_score on c3 platform during training is 0.8333333333333333\n", "roc_auc_score on c3 platform is 0.8333333333333333\n"]}], "source": ["# Pipe that \n", "gbt = c3.SklearnPipe(\n", "    name=\"gbt\",\n", "    technique=c3.SklearnTechnique(\n", "        name=\"linear_model.LogisticRegression\",\n", "        processingFunctionName=\"predict_proba\",\n", "        dataSelectorForScoring={\n", "            \"predict_proba\": [1],\n", "        },\n", "    ),\n", "    scoringMetrics=c3.MLScoringMetric.toScoringMetricMap([\n", "        c3.SklearnScoringMetric(\n", "            name='roc_auc_score',\n", "            categoryOverride=c3.MLScoringMetricCategory.PROBABILITY,\n", "        ),\n", "    ]),\n", ")\n", "\n", "trainedPipe = gbt.train(dataset_X, dataset_y)\n", "y_scores = trainedPipe.process(dataset_X).flattenedData()\n", "\n", "from sklearn.metrics import roc_auc_score\n", "print('roc_auc_score on native Sklearn ' + str(roc_auc_score(y_true, y_scores[1::2]))) \n", "print('roc_auc_score on c3 platform during training is ' + str(trainedPipe.trainingScores['SklearnScoringMetric:roc_auc_score, categoryOverride=PROBABILITY']))\n", "print('roc_auc_score on c3 platform is ' + str(trainedPipe.score(dataset_X, dataset_y)['SklearnScoringMetric:roc_auc_score, categoryOverride=PROBABILITY']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Using scoring metrics without ground truth\n", "There are a few scoring metrics that do not require ground truth to score. In case of such metrics just pass `null` to the `targetOuput` argument of `score` method, as example: `MLScoringMetric.score(output, null, input, spec)`.\n", "\n", "As noted in the documentation of `MLScoringMetric` (e.g. run `c3ShowType(MLScoringMetric)` at JS console), only ONE of the optional methods should be implemented by a descendant Type.  `MLPipe.score()` will dispatch the implemented optional method on the scoring metric to score the `MLPipe`.  Defining multiple optional methods on one `MLScoringMetric` will make the dispatch ambiguous.\n", "\n", "An example of a pre-defined Type scoring without ground truth is `SklearnScoringMetricNoTruth`, which can be configured to work with the [Silhouette Coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) or the [Calinski-Harabaz Score (Variance Ratio Criterion)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabaz_score.html#sklearn.metrics.calinski_harabaz_score)."]}, {"cell_type": "code", "execution_count": 32, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:42.883178Z", "start_time": "2020-09-21T15:11:42.874306Z"}}, "outputs": [{"data": {"text/plain": ["array([1., 0., 2., 1., 1., 0., 1., 2., 1., 1., 2., 0., 0., 0., 0., 1., 2.,\n", "       1., 1., 2., 0., 2., 0., 2., 2., 2., 2., 2., 0., 0.])"]}, "execution_count": 32, "metadata": {}, "output_type": "execute_result"}], "source": ["c3.Dataset.toNumpy(dataset=predictTest).ravel()"]}, {"cell_type": "code", "execution_count": 33, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:43.874803Z", "start_time": "2020-09-21T15:11:42.885756Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["sklearn directly:     0.5538417178134319\n", "SklearnScoringMetricNoTruth: 0.5538417178134319\n"]}], "source": ["# NotVerify: stdout\n", "from sklearn.metrics import silhouette_score\n", "# Notice that we must set `higherIsWorse` below, used by implementation of `higherIsBetter()`\n", "silhouette = c3.SklearnScoringMetricNoTruth(name='silhouette_score', higherIsWorse=False,\n", "                                            parameters={'random_state': 1})\n", "# Below, predictTest is converted to a 1d numpy array to suppress an annoying warning\n", "print('sklearn directly:     ' + \n", "      str(silhouette_score(X=c3.Dataset.toPandas(dataset=XTest),\n", "                           labels=c3.Dataset.toNumpy(dataset=predictTest).ravel(), random_state=1)))\n", "print('SklearnScoringMetricNoTruth: ' +\n", "      str(silhouette.score(output=predictTest, input=XTest)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can save a \"no ground truth\" scoring metric on an MLPipe, and use it to score on the training data."]}, {"cell_type": "code", "execution_count": 34, "metadata": {"ExecuteTime": {"end_time": "2020-09-21T15:11:46.779905Z", "start_time": "2020-09-21T15:11:43.877525Z"}}, "outputs": [{"data": {"text/plain": ["c3.SklearnPipe(\n", " name='logisticRegression',\n", " implType='SklearnPipeV7d13',\n", " noTrainScore=False,\n", " scoringMetrics=c3.Mapp<string, anyof(MLScoringMetric<any,any>)>({'SklearnScoringMetricNoTruth:silhouette_score, random_state=1': c3.SklearnScoringMetricNoTruth(\n", "                                                                                   parameters=c3.Mapp<string, any>({'random_state': 1}),\n", "                                                                                   name='silhouette_score',\n", "                                                                                   higherIsWorse=False)}),\n", " trainingScores=c3.Mapp<string, double>({'SklearnScoringMetricNoTruth:silhouette_score, random_state=1': 0.6821905014050882}),\n", " typeVersion='7.14.0',\n", " untrainableOverride=False,\n", " technique=c3.SklearnTechnique(\n", "             name='linear_model.LogisticRegression',\n", "             hyperParameters=c3.Mapp<string, any>({'random_state': 42}),\n", "             processingFunctionName='predict',\n", "             keepInputColumnIndices=False),\n", " trainedModel=c3.MLTrainedModelArtifact(\n", "                model='eJxlVElv1DAYzXSBEsq+lVL2rWxpp0Ap6wAFCgQGMJtvJsl4JimZZF7iUCqEAIkp9IjEieUIXEDigoTEAWnyT7hy4R+A7RYQwgfH+Wy/933ve8nDNi+9HXIniawwiOST1eMKDy0WxrUgFYFnXphdEF5LeJoGcWTC2PEIhfto66dzDcNo8MgJxSTaaZt8C4fQQTvkopI5ITqnabtcizjEnLFS7/C3732je2hBhkYxd6z0w9CDLpBTNRAsiARPPN4Q6HpKl8jgnwBLPUdmWMM8u0C75Y4XOmnKJnhQ8wXMso4lTlSJ6ywVjuCYb++kc2QwjcM7PEE37VTZudVaigW0S67rzl0WSHgstCt0vgpkoQiYBsYiXYOTiRiLdZUSw41TjiW2QU35PuEkmikRWDqtiSI2HrsplpU1elhkiSOCGMvLdKHerXJHZFJDWSXDCrtDH9NsMoaVXiUIpfBqNlmNC+YIkZhY9U88qDfiRKgmZSE30UP7FHRWb0xaXpxwi+kanCRxJllWd4SP1U30EqzR+rCEe3GUiiTzBPqmsJZgnadvm1FFXzKx3jaa2OCv8nt02cyTfvBSbGxiE8FmXSiPVBBbprCVYJvupovteiuUJUdF9E9hB8HOJ9hFsLvfLthtTeyZZaqIyYZM3dJuqY5gwDbswhMMEhT77XaNdgRD5XL5/E859GQbAnvdaX+b9sS/I/9Ywj5fEu4nGBY44Oo+ezGvMoz463xdz0Ff5nKI4LDKpWB3TOGIX1R4ljzbetH6mj/vGr+Vvyzde7A8f2eP5+9bX4/lX/I3+ccrrU+F45vy92br7UAl/1DCUUV2jKAkcNzVVvhrUpz4zXhSMY4SnNKMTZye4VNOyF+3nl3LH7de5Z978oc4o/DGCM4KnHO15pG2JcP532C2ArtAcHEWrOzPqBfsw6UZ9S4TXJHq+f/rRrRuyszKKriq2K4RXBe44dLFqsOzPwAmHa4+b9zUnRy0hvZaRdDMtX4B73tPlg==',\n", "                description='trained from '\n", "                             'technique:linear_model.LogisticRegression',\n", "                parameters=c3.Mapp<string, any>({'classes_': c3.Arry<double>([0.0, 1.0]),\n", "                             'coef_': c3.Arry<[double]>([c3.Arry<double>([0.4277596555939397,\n", "                                        -0.8879756463320371,\n", "                                        2.213535196321941,\n", "                                        0.9184797518750012])]),\n", "                             'intercept_': c3.Arry<double>([-6.2427855183236405]),\n", "                             'n_features_in_': 4,\n", "                             'n_iter_': c3.Arry<int>([28])})))"]}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": ["# NotVerify: result\n", "binaryLr = c3.SklearnPipe(\n", "               name=\"logisticRegression\",\n", "               technique=c3.SklearnTechnique(\n", "                   name=\"linear_model.LogisticRegression\",\n", "                   processingFunctionName=\"predict\",\n", "                   hyperParameters={\"random_state\": 42}\n", "               ),\n", "               scoringMetrics=c3.MLScoringMetric.toScoringMetricMap(\n", "                   scoringMetricList=[silhouette])  # use the silhouette metric we created above\n", "            )\n", "trainedLr = binaryLr.train(input=XTrain, targetOutput=yTrainBinary)\n", "trainedLr  # Score on training data stored in `trainingScores` field"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Creating your own custom scoring metrics\n", "You may create your own custom scoring metric by extending the `MLScoringMetric` interface Type and implementing the methods (overriding the default implementation of methods like `toString()` as necessary).  Alternately (and preferably), you may extend the `MLScoringMetricWithTruth` abstract Type, which provides an implementation of `score()` that will call `scoreDouble()` or `scoreDoubleNested()` (if `output` is multidimensional), which you would have to implement.  It may be easier to implement `scoreDouble()` (`scoreDoubleNested()`) than `score()`, which is overloaded for different input Types and will likely get new overloads in the future. \n", "\n", "There is a scoring spec `MLScoreSpec` that can be passed to `MLScoringMetric.score()`. You can use you own spec to customize behaviour of `MLScoringMetric.score()` then do scoring. If you passing spec to `MLPipe.score()` it will be passed to `MLScoringMetric.score()`.\n", "\n", "Read the documentation on `MLScoringMetric` or `MLScoringMetricWithTruth` carefully (e.g. run `c3ShowType(MLScoringMetric)` at JS console), to see what your custom scoring metric needs to implement.\n", "\n", "Below is an example custom metric's `c3typ` declaration, followed by its `js` implementation, to serve as a reference when you implement your own scoring metric Type."]}, {"cell_type": "raw", "metadata": {}, "source": ["/**\n", " * Example of how to define a custom scoring metric that takes in a ground truth for use by ML pipelines.\n", " * This custom metric is not a good metric, just a simple illustration.\n", " *\n", " * The javascript implementation of this metric is in ExampleMLScoringMetric.js .\n", " */\n", "@beta\n", "type ExampleMLScoringMetric mixes MLScoringMetricWithTruth {\n", "\n", "  /**\n", "   * @inheritdoc\n", "   * @see MLScoringMetric#scoreDouble\n", "   *\n", "   * A custom scoring function that returns the maximum absolute error across all entries,\n", "   * `max_i(|truth[i] - prediction[i]|)`.\n", "   *\n", "   * If your implementation is in python, use the decorator `@py(env='mlutils')` instead of the suffix `js server`.\n", "   */\n", "  scoreDouble: ~ js server\n", "\n", "  higherIsBetter: ~ inline js all\n", "}"]}, {"cell_type": "raw", "metadata": {}, "source": ["var logger = C3.logger(\"ExampleMLScoringMetric\");\n", "var errmsg;\n", "\n", "/**\n", " * Implements max_i(|truth[i] - prediction[i]|)\n", " */\n", "function scoreDouble(truth, prediction) {\n", "  if (truth.length != prediction.length) {\n", "    errmsg = \"length of truth array: \" + truth.length + \" does not equal the length of prediction array: \" +\n", "             prediction.length;\n", "    logger.error(errmsg);\n", "    throw new Error(errmsg);\n", "  }\n", "\n", "  var score = Number.NEGATIVE_INFINITY;\n", "  for (var i = 0; i < truth.length; i++) {\n", "    score = Math.max(score, Math.abs(truth[i] - prediction[i]));\n", "  }\n", "\n", "  if (!Number.isFinite(score)) {\n", "    return 0;  // No elements in list, just return 0\n", "  }\n", "  return score;\n", "}\n", "\n", "function higherIsBetter() {\n", "  return false;\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is the end of the tutorial, but check back in the future.  If there is enough demand from users, we will implement a pre-defined Type `MLLambdaMetric` that allows you to use C3 lambdas as scoring metrics (DATA-2678), and update this notebook to show how to use it."]}], "metadata": {"has_local_update": true, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "py-sklearn_3_0_0", "language": "Python", "name": "py-sklearn_3_0_0"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.12"}, "last_sync_time": "2020-09-21T13:14:58.765424"}, "nbformat": 4, "nbformat_minor": 2}