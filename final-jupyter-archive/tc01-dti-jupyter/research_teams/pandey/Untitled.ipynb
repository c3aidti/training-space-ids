{"cells": [{"cell_type": "code", "execution_count": 3, "id": "0b4e8874", "metadata": {"ExecuteTime": {"end_time": "2022-02-15T19:21:26.706222Z", "start_time": "2022-02-15T19:21:06.411700Z"}}, "outputs": [], "source": ["import os\n", "import zipfile\n", "path_to_zip_file = os.getcwd()+'/11-785-s22-hw1p2.zip'\n", "directory_to_extract_to = ''\n", "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n", "    zip_ref.extractall(os.getcwd())"]}, {"cell_type": "code", "execution_count": 1, "id": "96b087ce", "metadata": {"ExecuteTime": {"end_time": "2022-02-15T20:09:12.578754Z", "start_time": "2022-02-15T20:09:12.154263Z"}}, "outputs": [], "source": ["import torch"]}, {"cell_type": "code", "execution_count": 3, "id": "86b8b667", "metadata": {"ExecuteTime": {"end_time": "2022-02-15T20:09:39.099116Z", "start_time": "2022-02-15T20:09:39.094785Z"}}, "outputs": [{"data": {"text/plain": ["False"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["torch.cuda.is_available()"]}, {"cell_type": "code", "execution_count": 4, "id": "236c71dc", "metadata": {"ExecuteTime": {"end_time": "2022-02-15T19:21:54.380819Z", "start_time": "2022-02-15T19:21:54.368221Z"}}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'torch'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-4-905f9044334c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"]}], "source": ["import os\n", "import csv\n", "import random\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from sklearn.metrics import accuracy_score\n", "\n", "class Network(torch.nn.Module):\n", "    def __init__(self, in_size):\n", "        super(Network, self).__init__()\n", "        # TODO: Please try different architectures\n", "        # in_size = 13\n", "        # layers = [\n", "        #     nn.Linear(in_size, 512),\n", "        #     nn.BatchNorm1d(512),\n", "        #     nn.ReLU(),\n", "        #     nn.Linear(512, 256),\n", "        #     nn.BatchNorm1d(256),\n", "        #     nn.ReLU(),\n", "        #     nn.Linear(256, 40)\n", "        # ] # 72%\n", "        layers = [\n", "            nn.Linear(in_size, 1024),\n", "            nn.BatchNorm1d(1024),\n", "            nn.LeakyReLU(),\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(1024, 512),\n", "            nn.BatchNorm1d(512),\n", "            nn.LeakyReLU(),\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(512, 512),\n", "            nn.BatchNorm1d(512),\n", "            nn.LeakyReLU(),\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(512, 256),\n", "            nn.BatchNorm1d(256),\n", "            nn.LeakyReLU(),\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(256, 256),\n", "            nn.BatchNorm1d(256),\n", "            nn.LeakyReLU(),\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(256, 128),\n", "            nn.BatchNorm1d(128),\n", "            nn.LeakyReLU(),\n", "            nn.Dropout(p=0.2),\n", "            nn.Linear(128, 40)\n", "        ] \n", "        #79% with [512,256,128] with relu, BN \n", "        #82% [1024,512,256,256,128] leakyRL,BN,\n", "        #   [1024,512,512,256,256,128] leakyRL, BN, 2DO\n", "        self.laysers = nn.Sequential(*layers)\n", "\n", "    def forward(self, A0):\n", "        x = self.laysers(A0)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "id": "09b11cfa", "metadata": {}, "outputs": [], "source": ["def my_csv_reader(csv_file, col = 0):\n", "    file_content = []\n", "    with open(csv_file) as f:\n", "        f_csv = csv.reader(f)\n", "        for irow,row in enumerate(f_csv):\n", "            if col == 'all':\n", "                file_content.append(row)\n", "            else:\n", "                file_content.append(row[col])\n", "    return file_content\n", "    \n", "class LibriSamples(torch.utils.data.Dataset):\n", "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None, test_order_csv = None):\n", "        # sample represent how many npy files will be preloaded for one __getitem__ call\n", "        self.sample = sample \n", "        \n", "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n", "        self.X_names = os.listdir(self.X_dir)\n", "        if partition == 'test-clean':\n", "            #get test order as X_names\n", "            \n", "            self.X_names = my_csv_reader(test_order_csv)[1:]\n", "\n", "        try:\n", "            self.Y_dir = data_path + \"/\" + partition +\"/transcript/\"\n", "            self.Y_names = os.listdir(self.Y_dir)\n", "            assert(len(self.X_names) == len(self.Y_names))\n", "        except:\n", "            self.Y_names = None\n", "\n", "        # using a small part of the dataset to debug\n", "        if csvpath:\n", "            subset = self.parse_csv(csvpath)\n", "            self.X_names = [i for i in self.X_names if i in subset]\n", "            self.Y_names = [i for i in self.Y_names if i in subset]\n", "        \n", "        if shuffle == True:\n", "            XY_names = list(zip(self.X_names, self.Y_names))\n", "            random.shuffle(XY_names)\n", "            self.X_names, self.Y_names = zip(*XY_names)\n", "        \n", "        \n", "        self.length = len(self.X_names)\n", "        \n", "        self.PHONEMES = [\n", "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n", "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n", "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n", "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n", "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n", "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n", "      \n", "    @staticmethod\n", "    def parse_csv(filepath):\n", "        subset = []\n", "        with open(filepath) as f:\n", "            f_csv = csv.reader(f)\n", "            for row in f_csv:\n", "                subset.append(row[1])\n", "        return subset[1:]\n", "\n", "    def __len__(self):\n", "        return int(np.ceil(self.length / self.sample))\n", "        \n", "    def __getitem__(self, i):\n", "        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n", "        \n", "        X, Y = [], []\n", "        for j in sample_range:\n", "            X_path = self.X_dir + self.X_names[j]\n", "            X_data = np.load(X_path)\n", "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n", "            X.append(X_data)\n", "\n", "            if self.Y_names:\n", "                Y_path = self.Y_dir + self.Y_names[j]\n", "                label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n", "            else:\n", "                label = [self.PHONEMES.index('SIL') for i in range(len(X_data))]\n", "            Y.append(np.array(label))\n", "            \n", "            \n", "        X, Y = np.concatenate(X), np.concatenate(Y)\n", "        return X, Y\n", "    \n", "class LibriItems(torch.utils.data.Dataset):\n", "    def __init__(self, X, Y, context = 0):\n", "        assert(X.shape[0] == Y.shape[0])\n", "        \n", "        self.length  = X.shape[0]\n", "        self.context = context\n", "\n", "        if context == 0:\n", "            self.X, self.Y = X, Y\n", "        else:\n", "            self.X = np.pad(X, ((self.context, self.context), (0, 0)), 'constant', constant_values=0)\n", "            self.Y = Y\n", "            # TODO: self.X, self.Y = ... \n", "             \n", "        \n", "    def __len__(self):\n", "        return self.length\n", "        \n", "    def __getitem__(self, i):\n", "        if self.context == 0:\n", "            xx = self.X[i].flatten()\n", "            yy = self.Y[i]\n", "        else:\n", "            xx = self.X[i:i+2*self.context+1].flatten()\n", "            yy = self.Y[i]\n", "        return xx, yy\n", "    \n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "0bb5e10d", "metadata": {}, "outputs": [], "source": ["\n", "\n", "def train(args, model, device, train_samples, optimizer, criterion, epoch):\n", "    model.train()\n", "    for i in range(len(train_samples)):\n", "        X, Y = train_samples[i]\n", "        train_items = LibriItems(X, Y, context=args['context'])\n", "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], shuffle=True)\n", "\n", "        for batch_idx, (data, target) in enumerate(train_loader):\n", "            data = data.float().to(device)\n", "            target = target.long().to(device)\n", "\n", "            optimizer.zero_grad()\n", "            output = model(data)\n", "            loss = criterion(output, target)\n", "            loss.backward()\n", "            optimizer.step()\n", "            if batch_idx % args['log_interval'] == 0:\n", "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n", "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n", "                    100. * batch_idx / len(train_loader), loss.item()))\n", "\n", "\n", "def test(args, model, device, dev_samples):\n", "    model.eval()\n", "    true_y_list = []\n", "    pred_y_list = []\n", "    total_loss = 0\n", "    with torch.no_grad():\n", "        for i in range(len(dev_samples)):\n", "            X, Y = dev_samples[i]\n", "\n", "            test_items = LibriItems(X, Y, context=args['context'])\n", "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n", "\n", "            for data, true_y in test_loader:\n", "                data = data.float().to(device)\n", "                true_y = true_y.long().to(device)                \n", "                \n", "                output = model(data)\n", "                loss = criterion(output, true_y) \n", "                total_loss = total_loss + loss.item()\n", "                pred_y = torch.argmax(output, axis=1)\n", "\n", "                pred_y_list.extend(pred_y.tolist())\n", "                true_y_list.extend(true_y.tolist())\n", "\n", "    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n", "    ave_loss = total_loss/len(test_loader) #len(test_loader) is the #of batches\n", "    return train_accuracy, ave_loss\n", "\n", "\n", "# def main(args):\n", "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "    \n", "#     model = Network().to(device)\n", "#     optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n", "    \n", "#     criterion = torch.nn.CrossEntropyLoss()\n", "#     # If you want to use full Dataset, please pass None to csvpath\n", "#     #train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"/content/train_filenames_subset_8192_v2.csv\")\n", "#     train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\")\n", "#     dev_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n", "\n", "#     for epoch in range(1, args['epoch'] + 1):\n", "#         train(args, model, device, train_samples, optimizer, criterion, epoch)\n", "#         test_acc, test_loss = test(args, model, device, dev_samples)\n", "#         print('Dev accuracy ', test_acc, ' Dev loss ', test_loss)\n", "         "]}, {"cell_type": "code", "execution_count": null, "id": "a0f8d94b", "metadata": {}, "outputs": [], "source": ["args = {\n", "        'batch_size': 512,\n", "        'context': 20,\n", "        'log_interval': 200,\n", "        'LIBRI_PATH': '/content/hw1p2_student_data',\n", "        'lr': 0.001,\n", "        'epoch': 20\n", "    }\n", "#main(args)\n", "\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(device)\n", "\n", "in_size =  (2 * args['context'] + 1)*13\n", "model = Network(in_size).to(device)\n", "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n", "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=3)\n", "criterion = torch.nn.CrossEntropyLoss()\n", "# If you want to use full Dataset, please pass None to csvpath\n", "#train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"/content/train_filenames_subset_8192_v2.csv\")\n", "train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\")\n", "dev_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n", "\n", "for epoch in range(1, args['epoch'] + 1):\n", "    train(args, model, device, train_samples, optimizer, criterion, epoch)\n", "    test_acc, test_loss = test(args, model, device, dev_samples)\n", "    print('Dev accuracy ', test_acc, ', test loss', test_loss)\n", "    scheduler.step(test_loss)\n", "    #save model\n", "    output_file_name = 'h1p2_model_epoch%d' % epoch + '.pth'\n", "    torch.save(model.state_dict(), output_file_name)\n", "    print(output_file_name)"]}, {"cell_type": "code", "execution_count": null, "id": "788477ef", "metadata": {}, "outputs": [], "source": ["#test\n", "test_samples = LibriSamples(data_path = args['LIBRI_PATH'], sample=20000, shuffle=False, partition=\"test-clean\", test_order_csv=\"/content/test_order.csv\")\n", "print(test_samples.X_names)"]}, {"cell_type": "code", "execution_count": null, "id": "005e8dad", "metadata": {}, "outputs": [], "source": ["model.eval()\n", "pred_y_list = []\n", "with torch.no_grad():\n", "    for i in range(len(test_samples)):\n", "        X, Y = test_samples[i]\n", "\n", "        test_items = LibriItems(X, Y, context=args['context'])\n", "        test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n", "\n", "        for data, true_y in test_loader:\n", "            data = data.float().to(device)\n", "            \n", "            output = model(data)\n", "            pred_y = torch.argmax(output, axis=1)\n", "\n", "            pred_y_list.extend(pred_y.tolist())"]}, {"cell_type": "code", "execution_count": null, "id": "502f9597", "metadata": {}, "outputs": [], "source": ["len(pred_y_list) #1943253"]}, {"cell_type": "code", "execution_count": null, "id": "204f0e1d", "metadata": {}, "outputs": [], "source": ["test_submission = my_csv_reader(\"/content/hw1p2_student_data/test-clean/sample_submission.csv\",'all')\n", "print(test_submission[0],test_submission[-1], len(test_submission)) #['id', 'label'] ['1948492', '0'] 1948494\n", "\n", "test_submission = test_submission[1:(len(pred_y_list)+1)]\n", "for i,sub in enumerate(test_submission):\n", "    test_submission[i][0] = int(test_submission[i][0]) \n", "    test_submission[i][1] = pred_y_list[i]\n", "\n", "#test_submission is an array of size (1948253 by 2) all real values\n", "output_file_name = 'my_submission.csv'\n", "np.savetxt(output_file_name,test_submission, delimiter=',',fmt='%s',header='id,label',comments='')"]}], "metadata": {"has_local_update": false, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "py-gpu_basic_1_0_0", "language": "Python", "name": "py-gpu_basic_1_0_0"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.11"}, "last_sync_time": "2022-02-15T20:05:10.296639"}, "nbformat": 4, "nbformat_minor": 5}