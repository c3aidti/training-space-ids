{"cells": [{"cell_type": "code", "execution_count": 2, "metadata": {"ExecuteTime": {"end_time": "2021-09-10T10:57:14.198060Z", "start_time": "2021-09-10T10:57:13.065555Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Package                           Version\r\n", "--------------------------------- --------------------\r\n", "alembic                           1.4.3\r\n", "argon2-cffi                       20.1.0\r\n", "async-generator                   1.10\r\n", "attrs                             20.3.0\r\n", "backcall                          0.2.0\r\n", "backports.functools-lru-cache     1.6.1\r\n", "beautifulsoup4                    4.9.3\r\n", "bleach                            3.2.1\r\n", "blinker                           1.4\r\n", "bokeh                             2.2.3\r\n", "Bottleneck                        1.3.2\r\n", "brotlipy                          0.7.0\r\n", "c3bootstrap                       2.1.0\r\n", "c3notebook                        2.0.6.dev64+g9876c5a\r\n", "certifi                           2020.12.5\r\n", "certipy                           0.1.3\r\n", "cffi                              1.14.4\r\n", "chardet                           3.0.4\r\n", "click                             7.1.2\r\n", "cloudpickle                       1.6.0\r\n", "conda                             4.7.12\r\n", "conda-package-handling            1.7.2\r\n", "cryptography                      3.3.1\r\n", "cycler                            0.10.0\r\n", "Cython                            0.29.21\r\n", "cytoolz                           0.11.0\r\n", "dask                              2.30.0\r\n", "decorator                         4.4.2\r\n", "defusedxml                        0.6.0\r\n", "dill                              0.3.3\r\n", "distributed                       2.30.1\r\n", "entrypoints                       0.3\r\n", "fastcache                         1.1.0\r\n", "fsspec                            0.8.5\r\n", "glfw                              2.2.0\r\n", "gmpy2                             2.1.0b1\r\n", "gym                               0.19.0\r\n", "h5py                              2.10.0\r\n", "HeapDict                          1.0.1\r\n", "idna                              2.8\r\n", "imagecodecs                       2020.5.30\r\n", "imageio                           2.9.0\r\n", "importlib-metadata                3.3.0\r\n", "ipykernel                         5.4.2\r\n", "ipympl                            0.5.8\r\n", "ipython                           7.16.1\r\n", "ipython-genutils                  0.2.0\r\n", "ipywidgets                        7.5.1\r\n", "jedi                              0.17.2\r\n", "Jinja2                            2.11.2\r\n", "joblib                            1.0.0\r\n", "json5                             0.9.5\r\n", "jsonschema                        3.2.0\r\n", "jupyter                           1.0.0\r\n", "jupyter-client                    6.1.7\r\n", "jupyter-console                   6.2.0\r\n", "jupyter-contrib-core              0.3.3\r\n", "jupyter-contrib-nbextensions      0.5.1\r\n", "jupyter-core                      4.7.0\r\n", "jupyter-highlight-selected-word   0.2.0\r\n", "jupyter-latex-envs                1.4.6\r\n", "jupyter-nbextensions-configurator 0.4.1\r\n", "jupyter-telemetry                 0.1.0\r\n", "jupyterhub                        1.1.0\r\n", "jupyterlab                        2.2.8\r\n", "jupyterlab-pygments               0.1.2\r\n", "jupyterlab-server                 1.2.0\r\n", "kiwisolver                        1.3.1\r\n", "llvmlite                          0.34.0\r\n", "locket                            0.2.0\r\n", "lockfile                          0.12.2\r\n", "lxml                              4.6.2\r\n", "Mako                              1.1.3\r\n", "MarkupSafe                        1.1.1\r\n", "matplotlib                        3.4.3\r\n", "mistune                           0.8.4\r\n", "mock                              4.0.3\r\n", "mpmath                            1.1.0\r\n", "msgpack                           1.0.2\r\n", "nbclient                          0.5.1\r\n", "nbconvert                         5.6.1\r\n", "nbformat                          5.0.7\r\n", "nest-asyncio                      1.4.3\r\n", "networkx                          2.5\r\n", "notebook                          6.1.3\r\n", "numba                             0.51.2\r\n", "numexpr                           2.7.1\r\n", "numpy                             1.19.4\r\n", "oauthlib                          3.0.1\r\n", "olefile                           0.46\r\n", "opencv-python                     4.5.3.56\r\n", "packaging                         20.8\r\n", "pamela                            1.0.0\r\n", "pandas                            1.1.5\r\n", "pandocfilters                     1.4.2\r\n", "parso                             0.7.1\r\n", "partd                             1.1.0\r\n", "patsy                             0.5.1\r\n", "pexpect                           4.8.0\r\n", "pickleshare                       0.7.5\r\n", "Pillow                            8.0.1\r\n", "pip                               20.3.3\r\n", "prometheus-client                 0.9.0\r\n", "prompt-toolkit                    3.0.8\r\n", "protobuf                          3.13.0\r\n", "psutil                            5.8.0\r\n", "ptyprocess                        0.6.0\r\n", "pycosat                           0.6.3\r\n", "pycparser                         2.20\r\n", "pycurl                            7.43.0.6\r\n", "pyglet                            1.5.20\r\n", "Pygments                          2.7.3\r\n", "PyJWT                             1.7.1\r\n", "pyOpenSSL                         20.0.1\r\n", "pyparsing                         2.4.7\r\n", "pyrsistent                        0.17.3\r\n", "PySocks                           1.7.1\r\n", "python-dateutil                   2.8.1\r\n", "python-editor                     1.0.4\r\n", "python-json-logger                2.0.1\r\n", "pytz                              2020.1\r\n", "PyWavelets                        1.1.1\r\n", "PyYAML                            5.2\r\n", "pyzmq                             20.0.0\r\n", "qtconsole                         5.0.2\r\n", "QtPy                              1.9.0\r\n", "requests                          2.22.0\r\n", "ruamel-yaml-conda                 0.15.80\r\n", "ruamel.yaml                       0.16.12\r\n", "ruamel.yaml.clib                  0.2.2\r\n", "scikit-image                      0.17.2\r\n", "scikit-learn                      0.23.2\r\n", "scipy                             1.5.3\r\n", "seaborn                           0.11.1\r\n", "Send2Trash                        1.5.0\r\n", "setuptools                        49.6.0.post20201009\r\n", "six                               1.15.0\r\n", "sortedcontainers                  2.3.0\r\n", "soupsieve                         2.0.1\r\n", "SQLAlchemy                        1.3.22\r\n", "statsmodels                       0.12.1\r\n", "sympy                             1.6.2\r\n", "tables                            3.6.1\r\n", "tblib                             1.6.0\r\n", "terminado                         0.9.1\r\n", "testpath                          0.4.4\r\n", "threadpoolctl                     2.1.0\r\n", "tifffile                          2020.12.8\r\n", "toolz                             0.11.1\r\n", "tornado                           6.1\r\n", "tqdm                              4.54.1\r\n", "traitlets                         5.0.5\r\n", "typing-extensions                 3.7.4.3\r\n", "urllib3                           1.25.11\r\n", "vincent                           0.4.4\r\n", "wcwidth                           0.2.5\r\n", "webencodings                      0.5.1\r\n", "wheel                             0.36.2\r\n", "widgetsnbextension                3.5.1\r\n", "xlrd                              1.2.0\r\n", "zict                              2.0.0\r\n", "zipp                              3.4.0\r\n"]}], "source": ["!pip list"]}, {"cell_type": "markdown", "metadata": {"id": "YeT-tU8AIJ-G"}, "source": ["Installing Packages"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2021-09-10T11:10:57.028874Z", "start_time": "2021-09-10T11:10:49.960577Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Defaulting to user installation because normal site-packages is not writeable\n", "Requirement already satisfied: gym in /home/c3/.local/lib/python3.7/site-packages (0.19.0)\n", "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.19.4)\n", "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.6.0)\n", "Defaulting to user installation because normal site-packages is not writeable\n", "Requirement already satisfied: matplotlib in /home/c3/.local/lib/python3.7/site-packages (3.4.3)\n", "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (8.0.1)\n", "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n", "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n", "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n", "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n", "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.19.4)\n", "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n", "Defaulting to user installation because normal site-packages is not writeable\n", "\u001b[31mERROR: Could not find a version that satisfies the requirement pyhton\u001b[0m\n", "\u001b[31mERROR: No matching distribution found for pyhton\u001b[0m\n", "Defaulting to user installation because normal site-packages is not writeable\n", "\u001b[31mERROR: Could not find a version that satisfies the requirement pylab\u001b[0m\n", "\u001b[31mERROR: No matching distribution found for pylab\u001b[0m\n"]}], "source": ["!pip install gym\n", "!pip install --upgrade matplotlib\n", "!pip install --upgrade pyhton\n", "!pip install pylab"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-10T10:03:17.073622Z", "start_time": "2021-09-10T10:03:05.857221Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "ctNz0lpMk3hh", "outputId": "f91e2295-fd27-4528-95f8-7db2c03c36fc", "scrolled": false}, "outputs": [], "source": ["#================================================================\n", "#   File name   : BipedalWalker-v3_PPO\n", "#   Description : BipedalWalker-v3 PPO continuous agent\n", "#   TensorFlow  : 2.3.1\n", "#================================================================\n", "!pip install tensorflow\n", "!pip install tensorboardX\n", "!pip install --upgrade matplotlib\n", "#!pip install pylab\n", "#!pip3 install mujoco-py --no-cache\n", "#!pip install gym[All]\n", "#!pip install gym\n", "#!pip install box2d-py\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:20:51.783630Z", "start_time": "2021-09-09T18:20:51.779140Z"}}, "outputs": [], "source": ["#!pip install sudo\n", "sudo apt-get install xvfb\n", "!pip install xvfbwrapper"]}, {"cell_type": "markdown", "metadata": {"id": "JXKkkqdjIZLY"}, "source": ["Installing Packages to solve render problem with Colab"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:17:57.763681Z", "start_time": "2021-09-09T18:17:44.722720Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "PIqBzQOR1QBA", "outputId": "afd8ff57-696d-4c35-ba7f-9dede2e13f01"}, "outputs": [], "source": ["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n", "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n", "\n", "!apt-get update > /dev/null 2>&1\n", "!apt-get install cmake > /dev/null 2>&1\n", "!pip install --upgrade setuptools 2>&1\n", "!pip install ez_setup > /dev/null 2>&1\n", "#!pip install gym[atari] > /dev/null 2>&\n", "#!pip install gym[All] > /dev/null 2>&"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"ExecuteTime": {"end_time": "2021-09-10T10:57:32.348470Z", "start_time": "2021-09-10T10:57:32.287577Z"}}, "outputs": [{"ename": "ImportError", "evalue": "cannot import name '_c_internal_utils'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)", "\u001b[0;32m<ipython-input-3-20c1a7c88a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/.local/lib/python3.7/site-packages/pylab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# cbook must import matplotlib only within function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# definitions, so it is safe to import from it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatplotlibDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmplDeprecation\u001b[0m  \u001b[0;31m# deprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_c_internal_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from matplotlib._api.deprecation import (\n\u001b[1;32m     33\u001b[0m     MatplotlibDeprecationWarning, mplDeprecation)\n", "\u001b[0;31mImportError\u001b[0m: cannot import name '_c_internal_utils'"]}], "source": ["import os\n", "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # -1:cpu, 0:first gpu\n", "import sys\n", "sys.path.append('/home/c3/.local/lib/python3.7/site-packages')\n", "import random\n", "import gym\n", "import pylab\n", "import numpy as np\n", "import tensorflow as tf\n", "from tensorboardX import SummaryWriter\n", "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n", "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n", "from tensorflow.keras.models import Model, load_model\n", "from tensorflow.keras.layers import Input, Dense\n", "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad, Adadelta\n", "from tensorflow.keras import backend as K\n", "import copy\n", "\n", "from threading import Thread, Lock\n", "from multiprocessing import Process, Pipe\n", "import time\n", "\n", "gpus = tf.config.experimental.list_physical_devices('GPU')\n", "if len(gpus) > 0:\n", "    print(f'GPUs {gpus}')\n", "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n", "    except RuntimeError: pass"]}, {"cell_type": "markdown", "metadata": {"id": "szmzkNJcIixQ"}, "source": ["Defining wrap_env funnction to solve render problem with Colab"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:18:03.648371Z", "start_time": "2021-09-09T18:18:03.591093Z"}, "id": "lALXZ3gs1XPy"}, "outputs": [], "source": ["import gym\n", "from gym.wrappers import Monitor\n", "import glob\n", "import io\n", "import base64\n", "from IPython.display import HTML\n", "from pyvirtualdisplay import Display\n", "from IPython import display as ipythondisplay\n", "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n", "display = Display(visible=0, size=(1400, 900))\n", "display.start()\n", "\"\"\"\n", "Utility functions to enable video recording of gym environment \n", "and displaying it.\n", "To enable video, just do \"env = wrap_env(env)\"\"\n", "\"\"\"\n", "def show_video(t):\n", "  mp4list = glob.glob('*.mp4')\n", "  if len(mp4list) > 0:\n", "    mp4 = t+'.mp4'\n", "    video = io.open(mp4, 'r+b').read()\n", "    encoded = base64.b64encode(video)\n", "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n", "                loop controls style=\"height: 400px;\">\n", "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n", "             </video>'''.format(encoded.decode('ascii'))))\n", "  else: \n", "    print(\"Could not find this video\")"]}, {"cell_type": "markdown", "metadata": {"id": "L4uzoxZ8IxS5"}, "source": ["Random Action to test import Env and render function"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:14:07.171001Z", "start_time": "2021-09-09T18:14:06.809458Z"}, "colab": {"base_uri": "https://localhost:8080/", "height": 493}, "id": "zGoWK3GhAIpE", "outputId": "800d2f1f-4faa-4169-faa4-c3fc35b450f6"}, "outputs": [], "source": ["import gym\n", "import random\n", "import numpy as np\n", "env = gym.make(\"BipedalWalker-v3\")\n", "video_recorder = None\n", "video_recorder = VideoRecorder(env,'./random.mp4', enabled=True)\n", "env.reset()\n", "def Random_games():\n", "    # Each of this episode is its own game.\n", "    action_size = env.action_space.shape[0]\n", "    for episode in range(10): #40\n", "        env.reset()\n", "        # this is each frame, up to 500...but we wont make it that far with random.\n", "        while True:\n", "            # This will display the environment\n", "            # Only display if you really want to see it.\n", "            # Takes much longer to display it.\n", "            env.render()\n", "            video_recorder.capture_frame() \n", "            # This will just create a sample action in any environment.\n", "            # In this environment, the action can be any of one how in list on 4, for example [0 1 0 0]\n", "            action = np.random.uniform(-1.0, 1.0, size=action_size)\n", "            # this executes the environment with an action, \n", "            # and returns the observation of the environment, \n", "            # the reward, if the env is over, and other info.\n", "            next_state, reward, done, info = env.step(action)\n", "            # lets print everything in one line:\n", "            #print(reward, action)\n", "            if done:\n", "                break\n", "    print('Saved video.')\n", "    video_recorder.close()\n", "    video_recorder.enabled = False\n", "    env.close()\n", "              \n", "Random_games()\n", "show_video('random')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:14:13.857222Z", "start_time": "2021-09-09T18:14:13.849450Z"}, "id": "aXvNJRhblcIj"}, "outputs": [], "source": ["class Environment(Process):\n", "    def __init__(self, env_idx, child_conn, env_name, state_size, action_size, visualize=False):\n", "        super(Environment, self).__init__()\n", "        self.env = gym.make(env_name)\n", "        self.is_render = visualize\n", "        self.env_idx = env_idx\n", "        self.child_conn = child_conn\n", "        self.state_size = state_size\n", "        self.action_size = action_size\n", "\n", "    def run(self):\n", "        super(Environment, self).run()\n", "        state = self.env.reset()\n", "        state = np.reshape(state, [1, self.state_size])\n", "        self.child_conn.send(state)\n", "        while True:\n", "            action = self.child_conn.recv()\n", "            #if self.is_render and self.env_idx == 0:\n", "                #self.env.render()\n", "\n", "            state, reward, done, info = self.env.step(action)\n", "            state = np.reshape(state, [1, self.state_size])\n", "\n", "            if done:\n", "                state = self.env.reset()\n", "                state = np.reshape(state, [1, self.state_size])\n", "\n", "            self.child_conn.send([state, reward, done, info])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:14:21.624774Z", "start_time": "2021-09-09T18:14:21.609177Z"}, "id": "ouWyqyJhlgGt"}, "outputs": [], "source": ["class Actor_Model:\n", "    def __init__(self, input_shape, action_space, lr, optimizer):\n", "        X_input = Input(input_shape)\n", "        self.action_space = action_space\n", "        \n", "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n", "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n", "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n", "        output = Dense(self.action_space, activation=\"tanh\")(X)\n", "\n", "        self.Actor = Model(inputs = X_input, outputs = output)\n", "        self.Actor.compile(loss=self.ppo_loss_continuous, optimizer=optimizer(lr=lr))\n", "        #print(self.Actor.summary())\n", "\n", "    def ppo_loss_continuous(self, y_true, y_pred):\n", "        advantages, actions, logp_old_ph, = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space]\n", "        LOSS_CLIPPING = 0.2\n", "        logp = self.gaussian_likelihood(actions, y_pred)\n", "\n", "        ratio = K.exp(logp - logp_old_ph)\n", "\n", "        p1 = ratio * advantages\n", "        p2 = tf.where(advantages > 0, (1.0 + LOSS_CLIPPING)*advantages, (1.0 - LOSS_CLIPPING)*advantages) # minimum advantage\n", "\n", "        actor_loss = -K.mean(K.minimum(p1, p2))\n", "\n", "        return actor_loss\n", "\n", "    def gaussian_likelihood(self, actions, pred): # for keras custom loss\n", "        log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n", "        pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n", "        return K.sum(pre_sum, axis=1)\n", "\n", "    def predict(self, state):\n", "        return self.Actor.predict(state)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:14:22.927624Z", "start_time": "2021-09-09T18:14:22.919645Z"}, "id": "nJTP5Whllj03"}, "outputs": [], "source": ["class Critic_Model:\n", "    def __init__(self, input_shape, action_space, lr, optimizer):\n", "        X_input = Input(input_shape)\n", "        old_values = Input(shape=(1,))\n", "\n", "        V = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n", "        V = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(V)\n", "        V = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(V)\n", "        value = Dense(1, activation=None)(V)\n", "\n", "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n", "        self.Critic.compile(loss=[self.critic_PPO2_loss(old_values)], optimizer=optimizer(lr=lr))\n", "\n", "    def critic_PPO2_loss(self, values):\n", "        def loss(y_true, y_pred):\n", "            LOSS_CLIPPING = 0.2\n", "            clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n", "            v_loss1 = (y_true - clipped_value_loss) ** 2\n", "            v_loss2 = (y_true - y_pred) ** 2\n", "            \n", "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n", "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n", "            return value_loss\n", "        return loss\n", "\n", "    def predict(self, state):\n", "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:14:25.946709Z", "start_time": "2021-09-09T18:14:24.961641Z"}, "colab": {"base_uri": "https://localhost:8080/", "height": 124}, "id": "Myk12hFxfvQA", "outputId": "fdafc5c9-ee7b-4cc6-8baf-745b23c5f94b"}, "outputs": [], "source": ["class PPOAgent:\n", "    # PPO Main Optimization Algorithm\n", "    def __init__(self, env_name, model_name=\"\"):\n", "        # Initialization\n", "        # Environment and PPO parameters\n", "        self.env_name = env_name\n", "        self.env = env = gym.make(env_name)      \n", "        #self.env = env = wrap_env(gym.make(env_name))\n", "        self.action_size = self.env.action_space.shape[0]\n", "        self.state_size = self.env.observation_space.shape\n", "        self.EPISODES = 10 #200000 # total episodes to train through all environments\n", "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n", "        self.max_average = 0 # when average score is above 0 model will be saved\n", "        self.lr = 0.00025\n", "        self.epochs = 10 # training epochs\n", "        self.shuffle = True #?\n", "        self.Training_batch = 512\n", "        #self.optimizer = RMSprop\n", "        self.optimizer = Adam\n", "\n", "        self.replay_count = 0\n", "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n", "        \n", "        # Instantiate plot memory\n", "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n", "\n", "        # Create Actor-Critic network models\n", "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n", "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n", "        \n", "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n", "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n", "        #self.load() # uncomment to continue training from old weights\n", "\n", "        # do not change bellow\n", "        self.log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n", "        self.std = np.exp(self.log_std)\n", "\n", "\n", "    def act(self, state):\n", "        # Use the network to predict the next action to take, using the model\n", "        pred = self.Actor.predict(state)\n", "\n", "        low, high = -1.0, 1.0 # -1 and 1 are boundaries of tanh\n", "        action = pred + np.random.uniform(low, high, size=pred.shape) * self.std\n", "        action = np.clip(action, low, high)\n", "        \n", "        logp_t = self.gaussian_likelihood(action, pred, self.log_std)\n", "\n", "        return action, logp_t\n", "\n", "    def gaussian_likelihood(self, action, pred, log_std):\n", "        # https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py\n", "        pre_sum = -0.5 * (((action-pred)/(np.exp(log_std)+1e-8))**2 + 2*log_std + np.log(2*np.pi)) \n", "        return np.sum(pre_sum, axis=1)\n", "\n", "    def discount_rewards(self, reward):#gaes is better\n", "        # Compute the gamma-discounted rewards over an episode\n", "        # We apply the discount and normalize it to avoid big variability of rewards\n", "        gamma = 0.99    # discount rate\n", "        running_add = 0\n", "        discounted_r = np.zeros_like(reward)\n", "        for i in reversed(range(0,len(reward))):\n", "            running_add = running_add * gamma + reward[i]\n", "            discounted_r[i] = running_add\n", "\n", "        discounted_r -= np.mean(discounted_r) # normalizing the result\n", "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n", "        return discounted_r\n", "\n", "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.90, normalize=True):\n", "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n", "        deltas = np.stack(deltas)\n", "        gaes = copy.deepcopy(deltas)\n", "        for t in reversed(range(len(deltas) - 1)):\n", "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n", "\n", "        target = gaes + values\n", "        if normalize:\n", "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n", "        return np.vstack(gaes), np.vstack(target)\n", "\n", "    def replay(self, states, actions, rewards, dones, next_states, logp_ts):\n", "        # reshape memory to appropriate shape for training\n", "        states = np.vstack(states)\n", "        next_states = np.vstack(next_states)\n", "        actions = np.vstack(actions)\n", "        logp_ts = np.vstack(logp_ts)\n", "\n", "        # Get Critic network predictions \n", "        values = self.Critic.predict(states)\n", "        next_values = self.Critic.predict(next_states)\n", "\n", "        # Compute discounted rewards and advantages\n", "        #discounted_r = self.discount_rewards(rewards)\n", "        #advantages = np.vstack(discounted_r - values)\n", "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n", "        '''\n", "        pylab.plot(adv,'.')\n", "        pylab.plot(target,'-')\n", "        ax=pylab.gca()\n", "        ax.grid(True)\n", "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n", "        pylab.show()\n", "        if str(episode)[-2:] == \"00\": pylab.savefig(self.env_name+\"_\"+self.episode+\".png\")\n", "        '''\n", "        # stack everything to numpy array\n", "        # pack all advantages, predictions and actions to y_true and when they are received\n", "        # in custom loss function we unpack it\n", "        y_true = np.hstack([advantages, actions, logp_ts])\n", "        \n", "        # training Actor and Critic networks\n", "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n", "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n", "\n", "        # calculate loss parameters (should be done in loss, but couldn't find working way how to do that with disabled eager execution)\n", "        pred = self.Actor.predict(states)\n", "        log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n", "        logp = self.gaussian_likelihood(actions, pred, log_std)\n", "        approx_kl = np.mean(logp_ts - logp)\n", "        approx_ent = np.mean(-logp)\n", "\n", "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n", "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n", "        self.writer.add_scalar('Data/approx_kl_per_replay', approx_kl, self.replay_count)\n", "        self.writer.add_scalar('Data/approx_ent_per_replay', approx_ent, self.replay_count)\n", "        self.replay_count += 1\n", " \n", "    def load(self):\n", "        self.Actor.Actor.load_weights(self.Actor_name)\n", "        self.Critic.Critic.load_weights(self.Critic_name)\n", "\n", "    def save(self):\n", "        self.Actor.Actor.save_weights(self.Actor_name)\n", "        self.Critic.Critic.save_weights(self.Critic_name)\n", "\n", "    pylab.figure(figsize=(18, 9))\n", "    pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n", "    def PlotModel(self, score, episode, save=True):\n", "        self.scores_.append(score)\n", "        self.episodes_.append(episode)\n", "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n", "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n", "            pylab.plot(self.episodes_, self.scores_, 'b')\n", "            pylab.plot(self.episodes_, self.average_, 'r')\n", "            pylab.ylabel('Score', fontsize=18)\n", "            pylab.xlabel('Steps', fontsize=18)\n", "            try:\n", "                pylab.grid(True)\n", "                pylab.savefig(self.env_name+\".png\")\n", "            except OSError:\n", "                pass\n", "        # saving best models\n", "        if self.average_[-1] >= self.max_average and save:\n", "            self.max_average = self.average_[-1]\n", "            self.save()\n", "            SAVING = \"SAVING\"\n", "            # decreaate learning rate every saved model\n", "            #self.lr *= 0.99\n", "            #K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n", "            #K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n", "        else:\n", "            SAVING = \"\"\n", "\n", "        return self.average_[-1], SAVING\n", "    \n", "    def run_batch(self):\n", "        state = self.env.reset()\n", "        state = np.reshape(state, [1, self.state_size[0]])\n", "        done, score, SAVING = False, 0, ''\n", "        while True:\n", "            # Instantiate or reset games memory\n", "            states, next_states, actions, rewards, dones, logp_ts = [], [], [], [], [], []\n", "            for t in range(self.Training_batch):\n", "                self.env.render()\n", "                # Actor picks an action\n", "                action, logp_t = self.act(state)\n", "                # Retrieve new state, reward, and whether the state is terminal\n", "                next_state, reward, done, _ = self.env.step(action[0])\n", "                # Memorize (state, next_states, action, reward, done, logp_ts) for training\n", "                states.append(state)\n", "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n", "                actions.append(action)\n", "                rewards.append(reward)\n", "                dones.append(done)\n", "                logp_ts.append(logp_t[0])\n", "                # Update current state shape\n", "                state = np.reshape(next_state, [1, self.state_size[0]])\n", "                score += reward\n", "                if done:\n", "                    self.episode += 1\n", "                    average, SAVING = self.PlotModel(score, self.episode)\n", "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n", "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n", "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n", "                    self.writer.add_scalar(f'Workers:{1}/average_score',  average, self.episode)\n", "                    \n", "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n", "                    state = np.reshape(state, [1, self.state_size[0]])\n", "\n", "            self.replay(states, actions, rewards, dones, next_states, logp_ts)\n", "            if self.episode >= self.EPISODES:\n", "                break\n", "\n", "        self.env.close()\n", "\n", "\n", "    def run_multiprocesses(self, num_worker = 4):\n", "        works, parent_conns, child_conns = [], [], []\n", "        for idx in range(num_worker):\n", "            parent_conn, child_conn = Pipe()\n", "            work = Environment(idx, child_conn, self.env_name, self.state_size[0], self.action_size, True)\n", "            work.start()\n", "            works.append(work)\n", "            parent_conns.append(parent_conn)\n", "            child_conns.append(child_conn)\n", "\n", "        states =        [[] for _ in range(num_worker)]\n", "        next_states =   [[] for _ in range(num_worker)]\n", "        actions =       [[] for _ in range(num_worker)]\n", "        rewards =       [[] for _ in range(num_worker)]\n", "        dones =         [[] for _ in range(num_worker)]\n", "        logp_ts =       [[] for _ in range(num_worker)]\n", "        score =         [0 for _ in range(num_worker)]\n", "\n", "        state = [0 for _ in range(num_worker)]\n", "        for worker_id, parent_conn in enumerate(parent_conns):\n", "            state[worker_id] = parent_conn.recv()\n", "\n", "        while self.episode < self.EPISODES:\n", "            # get batch of action's and log_pi's\n", "            action, logp_pi = self.act(np.reshape(state, [num_worker, self.state_size[0]]))\n", "            \n", "            for worker_id, parent_conn in enumerate(parent_conns):\n", "                parent_conn.send(action[worker_id])\n", "                actions[worker_id].append(action[worker_id])\n", "                logp_ts[worker_id].append(logp_pi[worker_id])\n", "\n", "            for worker_id, parent_conn in enumerate(parent_conns):\n", "                next_state, reward, done, _ = parent_conn.recv()\n", "\n", "                states[worker_id].append(state[worker_id])\n", "                next_states[worker_id].append(next_state)\n", "                rewards[worker_id].append(reward)\n", "                dones[worker_id].append(done)\n", "                state[worker_id] = next_state\n", "                score[worker_id] += reward\n", "\n", "                if done:\n", "                    average, SAVING = self.PlotModel(score[worker_id], self.episode)\n", "                    print(\"episode: {}/{}, worker: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, worker_id, score[worker_id], average, SAVING))\n", "                    self.writer.add_scalar(f'Workers:{num_worker}/score_per_episode', score[worker_id], self.episode)\n", "                    self.writer.add_scalar(f'Workers:{num_worker}/learning_rate', self.lr, self.episode)\n", "                    self.writer.add_scalar(f'Workers:{num_worker}/average_score',  average, self.episode)\n", "                    score[worker_id] = 0\n", "                    if(self.episode < self.EPISODES):\n", "                        self.episode += 1\n", "                        \n", "                        \n", "            for worker_id in range(num_worker):\n", "                if len(states[worker_id]) >= self.Training_batch:\n", "                    self.replay(states[worker_id], actions[worker_id], rewards[worker_id], dones[worker_id], next_states[worker_id], logp_ts[worker_id])\n", "\n", "                    states[worker_id] = []\n", "                    next_states[worker_id] = []\n", "                    actions[worker_id] = []\n", "                    rewards[worker_id] = []\n", "                    dones[worker_id] = []\n", "                    logp_ts[worker_id] = []\n", "\n", "        # terminating processes after a while loop\n", "        works.append(work)\n", "        for work in works:\n", "            work.terminate()\n", "            print('TERMINATED:', work)\n", "            work.join()\n", "\n", "    def test(self, test_episodes = 100):#evaluate\n", "        self.load()\n", "        video_recorder = None\n", "        video_recorder = VideoRecorder(self.env,'./test.mp4', enabled=True)\n", "        for e in range(test_episodes):\n", "            state = self.env.reset()\n", "            state = np.reshape(state, [1, self.state_size[0]])\n", "            done = False\n", "            score = 0\n", "            while not done:\n", "                self.env.render()\n", "                video_recorder.capture_frame()\n", "                action = self.Actor.predict(state)[0]\n", "                state, reward, done, _ = self.env.step(action)\n", "                state = np.reshape(state, [1, self.state_size[0]])\n", "                score += reward\n", "                if done:\n", "                    average, SAVING = self.PlotModel(score, e, save=False)\n", "                    print(\"episode: {}/{}, score: {}, average{}\".format(e+1, test_episodes, score, average))\n", "                    break\n", "        print('Saved video.')\n", "        video_recorder.close()\n", "        video_recorder.enabled = False\n", "        self.env.close()\n", "        show_video('test')\n", "        \n", "\n", "if __name__ == \"__main__\":\n", "    # newest gym fixed bugs in 'BipedalWalker-v2' and now it's called 'BipedalWalker-v3'\n", "    env_name = 'BipedalWalker-v3'\n", "    agent = PPOAgent(env_name)\n", "    #agent.run_batch() # train as PPO\n", "    #agent.run_multiprocesses(num_worker = 16)  # train PPO multiprocessed (fastest)\n", "    #agent.test()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-09-09T18:15:06.699836Z", "start_time": "2021-09-09T18:15:06.391978Z"}}, "outputs": [], "source": ["agent.run_batch()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 580}, "id": "vEv7BUeH5qFr", "outputId": "9e2696b2-6c29-49aa-f7a6-547f5d2b704a"}, "outputs": [], "source": ["!cp \"/content/drive/MyDrive/weights-BiPedal/BipedalWalker-v3_PPO_Actor.h5\" \"/content/BipedalWalker-v3_PPO_Actor.h5\"\n", "!cp \"/content/drive/MyDrive/weights-BiPedal/BipedalWalker-v3_PPO_Critic.h5\" \"/content/BipedalWalker-v3_PPO_Critic.h5\"\n", "agent.test(5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "g3qGI79oAP_t"}, "outputs": [], "source": []}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "BipedalWalker-v3_PPO.ipynb", "provenance": []}, "has_local_update": true, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "py-tensorflow_3_0_0", "language": "Python", "name": "py-tensorflow_3_0_0"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}, "last_sync_time": "2021-09-10T10:41:54.022138"}, "nbformat": 4, "nbformat_minor": 1}