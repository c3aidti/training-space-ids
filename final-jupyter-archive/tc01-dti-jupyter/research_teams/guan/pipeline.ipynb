{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"ExecuteTime": {"end_time": "2021-12-02T08:51:17.171428Z", "start_time": "2021-12-02T08:51:17.079320Z"}}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'rasterio'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-1-29970a19c835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# geotiff util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rasterio'"]}], "source": ["# downloading utilities\n", "import requests\n", "import json\n", "from requests.auth import HTTPBasicAuth\n", "import urllib.request\n", "# progress and file \n", "from tqdm import tqdm\n", "# file processing utils\n", "import os\n", "from glob import glob\n", "from multiprocessing import Pool\n", "# geotiff util\n", "import rasterio"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:21.034277Z", "start_time": "2021-09-23T21:04:21.024206Z"}}, "outputs": [{"data": {"text/plain": ["'/home/c3/jupyter_root_dir/research_teams/guan'"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["os.getcwd()"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:21.215314Z", "start_time": "2021-09-23T21:04:21.036510Z"}}, "outputs": [], "source": ["from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__ "]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:21.220989Z", "start_time": "2021-09-23T21:04:21.217088Z"}}, "outputs": [], "source": ["connect_str = \"DefaultEndpointsProtocol=https;AccountName=yihongponding;AccountKey=hnvkEWfrIeYEyLUFcYYKyjujkSWiWcPdV/mS8O5GJ51iWPAkni7opzaA7klbwIGpGY0JANb5pRjGBe1ekHbX9Q==;EndpointSuffix=core.windows.net\"\n", "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n", "container_client = blob_service_client.get_container_client(\"ponding-il\")"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:21.360183Z", "start_time": "2021-09-23T21:04:21.222454Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["503-1250.tif\n", "503-1291.tif\n"]}], "source": ["blobs = container_client.list_blobs()\n", "for b in blobs:\n", "    print(b.name)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:23.815327Z", "start_time": "2021-09-23T21:04:21.361962Z"}}, "outputs": [], "source": ["blob_client = blob_service_client.get_blob_client(container=\"ponding-il\", blob=\"503-1291.tif\")\n", "download_file_path = \"503-1291.tif\"\n", "with open(download_file_path, \"wb\") as download_file:\n", "    download_file.write(blob_client.download_blob().readall())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Download file from Planet\n", "\n", "This downloads the images from PlanetLab API in quads. Each quad is approximately 4000 pixel * 4000 pixels and sizes around 100MB. \n", "\n", "For each year, Champaign has 20 quads, Illinois has 978 quads, and the entire US midwest has about 10,000 quads.\n", "\n", "We want to do this for every year from 2017 to 2021."]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2021-09-22T19:51:37.065880Z", "start_time": "2021-09-22T19:51:24.265110Z"}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["  0%|          | 0/978 [00:02<?, ?it/s]\n"]}], "source": ["BASE_URL = 'https://api.planet.com/basemaps/v1/mosaics/'\n", "API_KEY = '8fb5d85cdcfc40f6b4b9d3f44227142b' # obmitted\n", "auth = HTTPBasicAuth(API_KEY, '')\n", "\n", "mosaic_id = '56f00cc2-6be4-4315-9603-c75d6afab225'\n", "url = f'{BASE_URL}{mosaic_id}/quads'\n", "bbox = '-91.51307900019566, 36.970297999852846, -87.49519900023363, 42.50848099959849' #update bbox and page_size\n", "res = requests.get(url=url, auth=auth, params={'bbox':bbox, '_page_size':99999}) \n", "out = json.loads(res.text)\n", "\n", "for i in tqdm(out['items']):\n", "    if i['id'] in os.listdir('./tmp_img/'):\n", "        continue\n", "    urllib.request.urlretrieve(i['_links']['download'], f\"./tmp_img/{i['id']}.tif\")\n", "    break"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:03:46.004125Z", "start_time": "2021-09-23T21:03:44.748685Z"}}, "outputs": [], "source": ["arr = rasterio.open(\"503-1291.tif\").read()"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"ExecuteTime": {"end_time": "2021-12-02T08:51:30.445798Z", "start_time": "2021-12-02T08:51:24.683482Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Please wait a moment while I gather a list of all available modules...\n", "\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/opt/conda/lib/python3.8/site-packages/IPython/kernel/__init__.py:12: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead.\n", "  warn(\"The `IPython.kernel` package has been deprecated since IPython 4.0.\"\n", "/opt/conda/lib/python3.8/pkgutil.py:92: MatplotlibDeprecationWarning: \n", "The matplotlib.compat module was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n", "  __import__(info.name)\n", "/opt/conda/lib/python3.8/pkgutil.py:92: UserWarning: Viewer requires Qt\n", "  __import__(info.name)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Cython              binascii            joblib              requests\n", "IPython             binhex              json                resource\n", "OpenSSL             bisect              json5               rlcompleter\n", "PIL                 bleach              jsonschema          rmagic\n", "__future__          blinker             jupyter             ruamel_yaml\n", "_abc                bokeh               jupyter_client      runpy\n", "_ast                bottleneck          jupyter_console     sched\n", "_asyncio            brotli              jupyter_contrib_core scipy\n", "_bisect             bs4                 jupyter_contrib_nbextensions seaborn\n", "_blake2             builtins            jupyter_core        secrets\n", "_bootlocale         bz2                 jupyter_highlight_selected_word select\n", "_bz2                c3bootstrap         jupyter_nbextensions_configurator selectors\n", "_cffi_backend       c3nbres             jupyter_telemetry   send2trash\n", "_codecs             c3notebook          jupyterhub          setuptools\n", "_codecs_cn          cProfile            jupyterlab          shelve\n", "_codecs_hk          calendar            jupyterlab_pygments shlex\n", "_codecs_iso2022     certifi             jupyterlab_server   shutil\n", "_codecs_jp          certipy             jwt                 signal\n", "_codecs_kr          cffi                keyword             site\n", "_codecs_tw          cgi                 kiwisolver          six\n", "_collections        cgitb               latex_envs          skimage\n", "_collections_abc    chardet             lib2to3             sklearn\n", "_compat_pickle      chunk               linecache           smtpd\n", "_compression        click               llvmlite            smtplib\n", "_contextvars        cloudpickle         locale              sndhdr\n", "_crypt              cmath               locket              socket\n", "_csv                cmd                 logging             socketserver\n", "_ctypes             code                lxml                socks\n", "_ctypes_test        codecs              lzma                sockshandler\n", "_curses             codeop              mailbox             sortedcontainers\n", "_curses_panel       collections         mailcap             soupsieve\n", "_datetime           colorsys            mako                spwd\n", "_decimal            compileall          markupsafe          sqlalchemy\n", "_distutils_hack     concurrent          marshal             sqlite3\n", "_dummy_thread       conda               math                sre_compile\n", "_elementtree        conda_env           matplotlib          sre_constants\n", "_functools          conda_package_handling matplotlib_inline   sre_parse\n", "_hashlib            configparser        mimetypes           ssl\n", "_heapq              contextlib          mistune             stat\n", "_imp                contextvars         mmap                statistics\n", "_io                 copy                mock                statsmodels\n", "_json               copyreg             modulefinder        storemagic\n", "_locale             crypt               mpmath              string\n", "_lsprof             cryptography        msgpack             stringprep\n", "_lzma               csv                 multiprocessing     struct\n", "_markupbase         ctypes              nbclient            subprocess\n", "_md5                curl                nbconvert           sunau\n", "_multibytecodec     curses              nbformat            symbol\n", "_multiprocessing    cycler              nest_asyncio        sympy\n", "_opcode             cython              netrc               sympyprinting\n", "_operator           cythonmagic         networkx            symtable\n", "_osx_support        cytoolz             nis                 sys\n", "_pickle             dask                nntplib             sysconfig\n", "_posixshmem         dataclasses         notebook            syslog\n", "_posixsubprocess    datetime            ntpath              tables\n", "_py_abc             dateutil            nturl2path          tabnanny\n", "_pydecimal          dbm                 numba               tarfile\n", "_pyio               decimal             numbers             tblib\n", "_pyrsistent_version decorator           numexpr             telnetlib\n", "_queue              defusedxml          numpy               tempfile\n", "_random             difflib             oauthlib            terminado\n", "_ruamel_yaml        dill                olefile             termios\n", "_sha1               dis                 opcode              test\n", "_sha256             distributed         operator            test_data\n", "_sha3               distutils           optparse            test_pycosat\n", "_sha512             doctest             os                  testpath\n", "_signal             dummy_threading     ossaudiodev         tests\n", "_sitebuiltins       easy_install        packaging           textwrap\n", "_socket             editor              pamela              this\n", "_sqlite3            email               pandas              threading\n", "_sre                encodings           pandocfilters       threadpoolctl\n", "_ssl                ensurepip           parser              tifffile\n", "_stat               entrypoints         parso               time\n", "_statistics         enum                partd               timeit\n", "_string             errno               pathlib             tkinter\n", "_strptime           fastcache           patsy               tlz\n", "_struct             faulthandler        pdb                 token\n", "_symtable           fcntl               pexpect             tokenize\n", "_sysconfigdata__linux_x86_64-linux-gnu filecmp             pickle              toolz\n", "_sysconfigdata_aarch64_conda_cos7_linux_gnu fileinput           pickleshare         tornado\n", "_sysconfigdata_aarch64_conda_linux_gnu fnmatch             pickletools         tqdm\n", "_sysconfigdata_arm64_apple_darwin20_0_0 formatter           pip                 trace\n", "_sysconfigdata_i686_conda_cos6_linux_gnu fractions           pipes               traceback\n", "_sysconfigdata_i686_conda_linux_gnu fsspec              pkg_resources       tracemalloc\n", "_sysconfigdata_powerpc64le_conda_cos7_linux_gnu ftplib              pkgutil             traitlets\n", "_sysconfigdata_powerpc64le_conda_linux_gnu functools           platform            tty\n", "_sysconfigdata_x86_64_apple_darwin13_4_0 gc                  plistlib            turtle\n", "_sysconfigdata_x86_64_conda_cos6_linux_gnu genericpath         poplib              turtledemo\n", "_sysconfigdata_x86_64_conda_linux_gnu getopt              posix               types\n", "_testbuffer         getpass             posixpath           typing\n", "_testcapi           gettext             pprint              typing_extensions\n", "_testimportmultiple glob                profile             unicodedata\n", "_testinternalcapi   gmpy2               prometheus_client   unittest\n", "_testmultiphase     greenlet            prompt_toolkit      urllib\n", "_thread             grp                 pstats              urllib3\n", "_threading_local    gzip                psutil              uu\n", "_tkinter            h5py                pty                 uuid\n", "_tracemalloc        hashlib             ptyprocess          venv\n", "_warnings           heapdict            pvectorc            vincent\n", "_weakref            heapq               pwd                 warnings\n", "_weakrefset         hmac                py_compile          wave\n", "_xxsubinterpreters  html                pyclbr              wcwidth\n", "_xxtestfuzz         http                pycosat             weakref\n", "_yaml               idlelib             pycparser           webbrowser\n", "abc                 idna                pycurl              webencodings\n", "aifc                imagecodecs         pydoc               wheel\n", "alembic             imageio             pydoc_data          widgetsnbextension\n", "antigravity         imaplib             pyexpat             wsgiref\n", "argon2              imghdr              pygments            xdrlib\n", "argparse            imp                 pylab               xlrd\n", "array               importlib           pyparsing           xml\n", "ast                 importlib_metadata  pyrsistent          xmlrpc\n", "async_generator     inspect             pythonjsonlogger    xxlimited\n", "asynchat            io                  pytz                xxsubtype\n", "asyncio             ipaddress           pywt                yaml\n", "asyncore            ipykernel           pyximport           zict\n", "atexit              ipykernel_launcher  qtconsole           zipapp\n", "attr                ipympl              qtpy                zipfile\n", "audioop             ipython_genutils    queue               zipimport\n", "autoreload          ipywidgets          quopri              zipp\n", "backcall            isympy              random              zlib\n", "backports           itertools           re                  zmq\n", "base64              jedi                readline            \n", "bdb                 jinja2              reprlib             \n", "\n", "Enter any module name to get more help.  Or, type \"modules spam\" to search\n", "for modules whose name or summary contain the string \"spam\".\n", "\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/opt/conda/lib/python3.8/pkgutil.py:107: VisibleDeprecationWarning: zmq.eventloop.minitornado is deprecated in pyzmq 14.0 and will be removed.\n", "    Install tornado itself to use zmq with the tornado IOLoop.\n", "    \n", "  yield from walk_packages(path, info.name+'.', onerror)\n"]}], "source": ["help(\"modules\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Preprocess\n", "\n", "This reprojects quad into a different coordinates and upscales the quad. The upscaled size is around 250MB.\n", "\n", "Note:\n", " - Memory is fine since each quad will take 350MB at max (I think). I have 16Gb on local machine and 10 processes are OK.\n", " - It will probably be faster if we do upscale while downloading.\n", " - Also we probably need to replace the system call to python gdal package (according to the meeting).\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "def upscale(fname):\n", "    print(\"processing \", fname)\n", "    out_fname = fname.replace('original', 'warp')\n", "    if os.path.isfile(out_fname):\n", "        print(\"already exists\")\n", "        return\n", "    os.system(f'gdalwarp -t_srs EPSG:32616 -tr 3 3 -r bilinear {fname} {out_fname}')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# pool multiple thread to preprocess image\n", "files = glob('pathto/2008w2/original/*tif')\n", "print(files)\n", "p = Pool(10)\n", "p.map(upscale, files)"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:42.517060Z", "start_time": "2021-09-23T21:04:42.501328Z"}}, "outputs": [], "source": ["import gdal"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:07:59.911948Z", "start_time": "2021-09-23T21:07:59.898235Z"}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["ERROR 4: ./503-1291-warp.tif: No such file or directory\n"]}, {"ename": "SystemError", "evalue": "<built-in function wrapper_GDALWarpDestName> returned NULL without setting an error", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_6022/3624656553.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWarp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'503-1291.tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./503-1291-warp.tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstSRS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EPSG:32616'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxRes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myRes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m~/.conda/envs/py-image/lib/python3.7/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mWarp\u001b[0;34m(destNameOrDestDS, srcDSOrSrcDSTab, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_str_or_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestNameOrDestDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_GDALWarpDestName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestNameOrDestDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrcDSTab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_GDALWarpDestDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestNameOrDestDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrcDSTab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/.conda/envs/py-image/lib/python3.7/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mwrapper_GDALWarpDestName\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3408\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_GDALWarpDestName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m     \u001b[0;34m\"\"\"wrapper_GDALWarpDestName(char const * dest, int object_list_count, GDALWarpAppOptions warpAppOptions, GDALProgressFunc callback=0, void * callback_data=None) -> Dataset\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3410\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper_GDALWarpDestName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3411\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGDALVectorTranslateOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m     \u001b[0;34m\"\"\"Proxy of C++ GDALVectorTranslateOptions class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mSystemError\u001b[0m: <built-in function wrapper_GDALWarpDestName> returned NULL without setting an error"]}], "source": ["gdal.Warp('503-1291.tif', './503-1291-warp.tif', dstSRS='EPSG:32616', xRes=3, yRes=3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Prediction"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:56.807298Z", "start_time": "2021-09-23T21:04:54.972286Z"}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["2021-09-23 21:04:55.116600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"]}], "source": ["# TF stuffs\n", "import tensorflow as tf\n", "from tensorflow.keras import Model\n", "from tensorflow.keras.layers import (\n", "    Conv2D, \n", "    concatenate, \n", "    Dropout, \n", "    Input, \n", "    Reshape,\n", "    BatchNormalization, \n", "    MaxPooling2D, \n", "    UpSampling2D, \n", "    ReLU, \n", "    Conv2DTranspose\n", ")\n", "# data processing pacakges\n", "import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T20:49:09.324503Z", "start_time": "2021-09-23T20:49:09.319358Z"}}, "outputs": [{"data": {"text/plain": ["'2.4.1'"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["tf.__version__"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T20:49:11.513807Z", "start_time": "2021-09-23T20:49:10.913394Z"}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["2021-09-23 20:49:10.994874: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n", "2021-09-23 20:49:10.995088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n", "2021-09-23 20:49:11.029702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n", "pciBusID: 0001:00:00.0 name: Tesla K80 computeCapability: 3.7\n", "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n", "2021-09-23 20:49:11.029761: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n", "2021-09-23 20:49:11.029817: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n", "2021-09-23 20:49:11.029848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n", "2021-09-23 20:49:11.029876: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n", "2021-09-23 20:49:11.029903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n", "2021-09-23 20:49:11.029931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n", "2021-09-23 20:49:11.030004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n", "2021-09-23 20:49:11.507758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n", "2021-09-23 20:49:11.509402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"]}, {"data": {"text/plain": ["[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n", " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["tf.config.list_physical_devices()"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T20:50:03.486006Z", "start_time": "2021-09-23T20:49:14.446349Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["WARNING:tensorflow:From /tmp/ipykernel_5942/3597539986.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n", "Instructions for updating:\n", "Use `tf.config.list_physical_devices('GPU')` instead.\n", "True\n", "True\n"]}, {"name": "stderr", "output_type": "stream", "text": ["2021-09-23 20:49:14.460879: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n", "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n", "2021-09-23 20:49:14.461528: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n", "2021-09-23 20:49:14.462495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n", "pciBusID: 0001:00:00.0 name: Tesla K80 computeCapability: 3.7\n", "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n", "2021-09-23 20:49:14.462578: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n", "2021-09-23 20:49:14.462599: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n", "2021-09-23 20:49:14.462617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n", "2021-09-23 20:49:14.462633: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n", "2021-09-23 20:49:14.462649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n", "2021-09-23 20:49:14.462666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n", "2021-09-23 20:49:14.462681: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n", "2021-09-23 20:49:14.462741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n", "2021-09-23 20:49:14.464479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n", "2021-09-23 20:49:14.476353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n", "2021-09-23 20:50:03.393916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n", "2021-09-23 20:50:03.393953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n", "2021-09-23 20:50:03.393963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n", "2021-09-23 20:50:03.433202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 10615 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7)\n"]}], "source": ["print(tf.test.is_gpu_available())\n", "print(tf.test.is_built_with_cuda())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Define model"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:04:59.323083Z", "start_time": "2021-09-23T21:04:59.310213Z"}}, "outputs": [], "source": ["# unet model\n", "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n", "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n", "    # first layer\n", "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n", "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n", "    if batchnorm:\n", "        x = BatchNormalization()(x)\n", "    x = ReLU()(x)\n", "    \n", "    # second layer\n", "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n", "              kernel_initializer = 'he_normal', padding = 'same')(x)\n", "    if batchnorm:\n", "        x = BatchNormalization()(x)\n", "    x = ReLU()(x)\n", "    \n", "    return x\n", "\n", "def get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):\n", "    \"\"\"Function to define the UNET Model\"\"\"\n", "    # Contracting Path\n", "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n", "    p1 = MaxPooling2D((2, 2))(c1)\n", "    p1 = Dropout(dropout)(p1)\n", "    \n", "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n", "    p2 = MaxPooling2D((2, 2))(c2)\n", "    p2 = Dropout(dropout)(p2)\n", "    \n", "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n", "    p3 = MaxPooling2D((2, 2))(c3)\n", "    p3 = Dropout(dropout)(p3)\n", "    \n", "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n", "    p4 = MaxPooling2D((2, 2))(c4)\n", "    p4 = Dropout(dropout)(p4)\n", "    \n", "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n", "    \n", "    # Expansive Path\n", "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n", "    u6 = concatenate([u6, c4])\n", "    u6 = Dropout(dropout)(u6)\n", "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n", "    \n", "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n", "    u7 = concatenate([u7, c3])\n", "    u7 = Dropout(dropout)(u7)\n", "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n", "    \n", "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n", "    u8 = concatenate([u8, c2])\n", "    u8 = Dropout(dropout)(u8)\n", "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n", "    \n", "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n", "    u9 = concatenate([u9, c1])\n", "    u9 = Dropout(dropout)(u9)\n", "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n", "    \n", "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n", "    model = Model(input_img, outputs)\n", "    model.summary()\n", "    return model"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:05:04.324757Z", "start_time": "2021-09-23T21:05:04.257498Z"}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["2021-09-23 21:05:04.259359: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n", "2021-09-23 21:05:04.260479: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n", "2021-09-23 21:05:04.307988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n", "pciBusID: 0001:00:00.0 name: Tesla K80 computeCapability: 3.7\n", "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n", "2021-09-23 21:05:04.308040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n", "2021-09-23 21:05:04.310135: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n", "2021-09-23 21:05:04.310188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n", "2021-09-23 21:05:04.311952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n", "2021-09-23 21:05:04.312284: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n", "2021-09-23 21:05:04.314335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n", "2021-09-23 21:05:04.315406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n", "2021-09-23 21:05:04.320128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n", "2021-09-23 21:05:04.321705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"]}], "source": ["# some constant and define tf memory behavior\n", "CKPT_DIR = './model_ckpt/unet_train_100.tf'\n", "FILE_DIR = './tmp_img/*tif'\n", "OUT_DIR = './tmp_pred/'\n", "THRESHOLD = 0.5\n", "img_size = (224, 224, 5)\n", "physical_devices = tf.config.list_physical_devices('GPU')\n", "tf.config.experimental.set_memory_growth(physical_devices[0], True) # this needs to be changed if multiple GPU"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Get model and load trained model"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"ExecuteTime": {"end_time": "2021-09-23T21:05:20.413427Z", "start_time": "2021-09-23T21:05:18.787439Z"}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["2021-09-23 21:05:18.800603: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n", "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n", "2021-09-23 21:05:18.801447: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n", "2021-09-23 21:05:18.802443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n", "pciBusID: 0001:00:00.0 name: Tesla K80 computeCapability: 3.7\n", "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n", "2021-09-23 21:05:18.802512: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n", "2021-09-23 21:05:18.802548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n", "2021-09-23 21:05:18.802577: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n", "2021-09-23 21:05:18.802605: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n", "2021-09-23 21:05:18.802632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n", "2021-09-23 21:05:18.802660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n", "2021-09-23 21:05:18.802687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n", "2021-09-23 21:05:18.802714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n", "2021-09-23 21:05:18.804197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n", "2021-09-23 21:05:18.804259: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n", "2021-09-23 21:05:19.454418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n", "2021-09-23 21:05:19.454465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n", "2021-09-23 21:05:19.454475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n", "2021-09-23 21:05:19.456998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10623 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Model: \"model\"\n", "__________________________________________________________________________________________________\n", "Layer (type)                    Output Shape         Param #     Connected to                     \n", "==================================================================================================\n", "img (InputLayer)                [(None, 224, 224, 5) 0                                            \n", "__________________________________________________________________________________________________\n", "conv2d (Conv2D)                 (None, 224, 224, 32) 1472        img[0][0]                        \n", "__________________________________________________________________________________________________\n", "batch_normalization (BatchNorma (None, 224, 224, 32) 128         conv2d[0][0]                     \n", "__________________________________________________________________________________________________\n", "re_lu (ReLU)                    (None, 224, 224, 32) 0           batch_normalization[0][0]        \n", "__________________________________________________________________________________________________\n", "conv2d_1 (Conv2D)               (None, 224, 224, 32) 9248        re_lu[0][0]                      \n", "__________________________________________________________________________________________________\n", "batch_normalization_1 (BatchNor (None, 224, 224, 32) 128         conv2d_1[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_1 (ReLU)                  (None, 224, 224, 32) 0           batch_normalization_1[0][0]      \n", "__________________________________________________________________________________________________\n", "max_pooling2d (MaxPooling2D)    (None, 112, 112, 32) 0           re_lu_1[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout (Dropout)               (None, 112, 112, 32) 0           max_pooling2d[0][0]              \n", "__________________________________________________________________________________________________\n", "conv2d_2 (Conv2D)               (None, 112, 112, 64) 18496       dropout[0][0]                    \n", "__________________________________________________________________________________________________\n", "batch_normalization_2 (BatchNor (None, 112, 112, 64) 256         conv2d_2[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_2 (ReLU)                  (None, 112, 112, 64) 0           batch_normalization_2[0][0]      \n", "__________________________________________________________________________________________________\n", "conv2d_3 (Conv2D)               (None, 112, 112, 64) 36928       re_lu_2[0][0]                    \n", "__________________________________________________________________________________________________\n", "batch_normalization_3 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_3 (ReLU)                  (None, 112, 112, 64) 0           batch_normalization_3[0][0]      \n", "__________________________________________________________________________________________________\n", "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           re_lu_3[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout_1 (Dropout)             (None, 56, 56, 64)   0           max_pooling2d_1[0][0]            \n", "__________________________________________________________________________________________________\n", "conv2d_4 (Conv2D)               (None, 56, 56, 128)  73856       dropout_1[0][0]                  \n", "__________________________________________________________________________________________________\n", "batch_normalization_4 (BatchNor (None, 56, 56, 128)  512         conv2d_4[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_4 (ReLU)                  (None, 56, 56, 128)  0           batch_normalization_4[0][0]      \n", "__________________________________________________________________________________________________\n", "conv2d_5 (Conv2D)               (None, 56, 56, 128)  147584      re_lu_4[0][0]                    \n", "__________________________________________________________________________________________________\n", "batch_normalization_5 (BatchNor (None, 56, 56, 128)  512         conv2d_5[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_5 (ReLU)                  (None, 56, 56, 128)  0           batch_normalization_5[0][0]      \n", "__________________________________________________________________________________________________\n", "max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 128)  0           re_lu_5[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout_2 (Dropout)             (None, 28, 28, 128)  0           max_pooling2d_2[0][0]            \n", "__________________________________________________________________________________________________\n", "conv2d_6 (Conv2D)               (None, 28, 28, 256)  295168      dropout_2[0][0]                  \n", "__________________________________________________________________________________________________\n", "batch_normalization_6 (BatchNor (None, 28, 28, 256)  1024        conv2d_6[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_6 (ReLU)                  (None, 28, 28, 256)  0           batch_normalization_6[0][0]      \n", "__________________________________________________________________________________________________\n", "conv2d_7 (Conv2D)               (None, 28, 28, 256)  590080      re_lu_6[0][0]                    \n", "__________________________________________________________________________________________________\n", "batch_normalization_7 (BatchNor (None, 28, 28, 256)  1024        conv2d_7[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_7 (ReLU)                  (None, 28, 28, 256)  0           batch_normalization_7[0][0]      \n", "__________________________________________________________________________________________________\n", "max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 256)  0           re_lu_7[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout_3 (Dropout)             (None, 14, 14, 256)  0           max_pooling2d_3[0][0]            \n", "__________________________________________________________________________________________________\n", "conv2d_8 (Conv2D)               (None, 14, 14, 512)  1180160     dropout_3[0][0]                  \n", "__________________________________________________________________________________________________\n", "batch_normalization_8 (BatchNor (None, 14, 14, 512)  2048        conv2d_8[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_8 (ReLU)                  (None, 14, 14, 512)  0           batch_normalization_8[0][0]      \n", "__________________________________________________________________________________________________\n", "conv2d_9 (Conv2D)               (None, 14, 14, 512)  2359808     re_lu_8[0][0]                    \n", "__________________________________________________________________________________________________\n", "batch_normalization_9 (BatchNor (None, 14, 14, 512)  2048        conv2d_9[0][0]                   \n", "__________________________________________________________________________________________________\n", "re_lu_9 (ReLU)                  (None, 14, 14, 512)  0           batch_normalization_9[0][0]      \n", "__________________________________________________________________________________________________\n", "conv2d_transpose (Conv2DTranspo (None, 28, 28, 256)  1179904     re_lu_9[0][0]                    \n", "__________________________________________________________________________________________________\n", "concatenate (Concatenate)       (None, 28, 28, 512)  0           conv2d_transpose[0][0]           \n", "                                                                 re_lu_7[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout_4 (Dropout)             (None, 28, 28, 512)  0           concatenate[0][0]                \n", "__________________________________________________________________________________________________\n", "conv2d_10 (Conv2D)              (None, 28, 28, 256)  1179904     dropout_4[0][0]                  \n", "__________________________________________________________________________________________________\n", "batch_normalization_10 (BatchNo (None, 28, 28, 256)  1024        conv2d_10[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_10 (ReLU)                 (None, 28, 28, 256)  0           batch_normalization_10[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_11 (Conv2D)              (None, 28, 28, 256)  590080      re_lu_10[0][0]                   \n", "__________________________________________________________________________________________________\n", "batch_normalization_11 (BatchNo (None, 28, 28, 256)  1024        conv2d_11[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_11 (ReLU)                 (None, 28, 28, 256)  0           batch_normalization_11[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_transpose_1 (Conv2DTrans (None, 56, 56, 128)  295040      re_lu_11[0][0]                   \n", "__________________________________________________________________________________________________\n", "concatenate_1 (Concatenate)     (None, 56, 56, 256)  0           conv2d_transpose_1[0][0]         \n", "                                                                 re_lu_5[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout_5 (Dropout)             (None, 56, 56, 256)  0           concatenate_1[0][0]              \n", "__________________________________________________________________________________________________\n", "conv2d_12 (Conv2D)              (None, 56, 56, 128)  295040      dropout_5[0][0]                  \n", "__________________________________________________________________________________________________\n", "batch_normalization_12 (BatchNo (None, 56, 56, 128)  512         conv2d_12[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_12 (ReLU)                 (None, 56, 56, 128)  0           batch_normalization_12[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_13 (Conv2D)              (None, 56, 56, 128)  147584      re_lu_12[0][0]                   \n", "__________________________________________________________________________________________________\n", "batch_normalization_13 (BatchNo (None, 56, 56, 128)  512         conv2d_13[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_13 (ReLU)                 (None, 56, 56, 128)  0           batch_normalization_13[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_transpose_2 (Conv2DTrans (None, 112, 112, 64) 73792       re_lu_13[0][0]                   \n", "__________________________________________________________________________________________________\n", "concatenate_2 (Concatenate)     (None, 112, 112, 128 0           conv2d_transpose_2[0][0]         \n", "                                                                 re_lu_3[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout_6 (Dropout)             (None, 112, 112, 128 0           concatenate_2[0][0]              \n", "__________________________________________________________________________________________________\n", "conv2d_14 (Conv2D)              (None, 112, 112, 64) 73792       dropout_6[0][0]                  \n", "__________________________________________________________________________________________________\n", "batch_normalization_14 (BatchNo (None, 112, 112, 64) 256         conv2d_14[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_14 (ReLU)                 (None, 112, 112, 64) 0           batch_normalization_14[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_15 (Conv2D)              (None, 112, 112, 64) 36928       re_lu_14[0][0]                   \n", "__________________________________________________________________________________________________\n", "batch_normalization_15 (BatchNo (None, 112, 112, 64) 256         conv2d_15[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_15 (ReLU)                 (None, 112, 112, 64) 0           batch_normalization_15[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_transpose_3 (Conv2DTrans (None, 224, 224, 32) 18464       re_lu_15[0][0]                   \n", "__________________________________________________________________________________________________\n", "concatenate_3 (Concatenate)     (None, 224, 224, 64) 0           conv2d_transpose_3[0][0]         \n", "                                                                 re_lu_1[0][0]                    \n", "__________________________________________________________________________________________________\n", "dropout_7 (Dropout)             (None, 224, 224, 64) 0           concatenate_3[0][0]              \n", "__________________________________________________________________________________________________\n", "conv2d_16 (Conv2D)              (None, 224, 224, 32) 18464       dropout_7[0][0]                  \n", "__________________________________________________________________________________________________\n", "batch_normalization_16 (BatchNo (None, 224, 224, 32) 128         conv2d_16[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_16 (ReLU)                 (None, 224, 224, 32) 0           batch_normalization_16[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_17 (Conv2D)              (None, 224, 224, 32) 9248        re_lu_16[0][0]                   \n", "__________________________________________________________________________________________________\n", "batch_normalization_17 (BatchNo (None, 224, 224, 32) 128         conv2d_17[0][0]                  \n", "__________________________________________________________________________________________________\n", "re_lu_17 (ReLU)                 (None, 224, 224, 32) 0           batch_normalization_17[0][0]     \n", "__________________________________________________________________________________________________\n", "conv2d_18 (Conv2D)              (None, 224, 224, 1)  33          re_lu_17[0][0]                   \n", "==================================================================================================\n", "Total params: 8,642,849\n", "Trainable params: 8,636,961\n", "Non-trainable params: 5,888\n", "__________________________________________________________________________________________________\n"]}, {"ename": "NotFoundError", "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ckpt/unet_train_100.tf", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)", "\u001b[0;32m~/.conda/envs/py-image/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ckpt/unet_train_100.tf", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_6022/1894222650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCKPT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m~/.conda/envs/py-image/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2197\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m         \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2200\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/.conda/envs/py-image/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m~/.conda/envs/py-image/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ckpt/unet_train_100.tf"]}], "source": ["# load trained model\n", "input_img = Input(img_size, name='img')\n", "model = get_unet(input_img, n_filters=32, dropout=0.15, batchnorm=True)\n", "model.load_weights(CKPT_DIR)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Prediction function that handles a file"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict_tiff(net, tiff_path, add_gcvi=False):\n", "    base_img = rasterio.open(tiff_path).read()\n", "    base_img = np.transpose(base_img, [1, 2, 0])\n", "    base_img = base_img[:, :, :4]\n", "    if add_gcvi:\n", "        gcvi = base_img[:, :, 3] / base_img[:, :, 1]\n", "        gcvi = gcvi - 1\n", "        gcvi[gcvi > 20] = 20\n", "        gcvi[gcvi < 0] = 0\n", "        gcvi = np.nan_to_num(gcvi, False, 0, 0, 0)\n", "        gcvi = np.expand_dims(gcvi, -1)\n", "        base_img = np.concatenate([base_img, gcvi], 2) \n", "    \n", "    # track original size and calc how many cuts\n", "    h, w, c = base_img.shape\n", "    h_count = int(h/img_size) + 1\n", "    w_count = int(w/img_size) + 1\n", "\n", "    # calculate padded height and width\n", "    h_padded = h_count * img_size\n", "    w_padded = w_count * img_size\n", "\n", "    # Pad image and cut into img_size * img_size\n", "    base_img = np.pad(base_img, ((0, h_padded - h), (0, w_padded-w), (0,0)), 'constant')\n", "    base_img = np.reshape(base_img, (h_count, img_size, w_count, img_size, c))\n", "    base_img = np.transpose(base_img, axes=(0, 2, 1, 3, 4))\n", "    base_img = np.reshape(base_img, (-1, img_size, img_size, c)) # nhwc\n", "\n", "    # Predict\n", "    out = net.predict(base_img, batch_size = 8)\n", "\n", "    # Convert back\n", "    combined = np.zeros((h_padded, w_padded))\n", "    x = 0\n", "    idx = 0\n", "    while x < combined.shape[0]:\n", "        y = 0\n", "        while y < combined.shape[1]:\n", "            combined[x:x+img_size, y:y+img_size] = np.squeeze(out[idx], 2)\n", "            y += img_size\n", "            idx += 1\n", "        x += img_size\n", "    combined_cut = np.array(combined)\n", "    # Trim\n", "    return combined_cut[:h, :w]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def export_tif(fn, file_name, mask):\n", "    meta = rasterio.open(fn).meta.copy()\n", "    meta.update({\n", "        'count':1,\n", "        'dtype':np.uint8\n", "    })\n", "    mask[mask > 0.5] = 1\n", "    mask[mask < 1] = 0\n", "    mask = mask.astype(np.uint8)\n", "    with rasterio.open(os.path.join(OUT_DIR,file_name), 'w', **meta) as dest:\n", "        dest.write(np.expand_dims(mask, 0))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Prediction driver\n", "\n", "Essential loops through every file, pipe them through the model and save the output.\n", "\n", "We could streamline is process, i.e., schedule load - predict - save for better performance."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for fn in tqdm(glob(FILE_DIR)):\n", "    if 'udm' in fn:\n", "        continue\n", "\n", "    if 'tif' not in fn:\n", "        continue\n", "\n", "    file_name = fn.split('\\\\')[-1]    \n", "    if file_name in os.listdir(OUT_DIR):\n", "        print(f'Already Found: {file_name}...skipping')\n", "        continue\n", "\n", "    out_fn = os.path.join(OUT_DIR, file_name)\n", "    mask = predict_tiff(net = model,\n", "                tiff_path = fn,\n", "                add_gcvi = True)\n", "    # we can probably throw this to a different thread\n", "    export_tif(fn, file_name, mask)\n", "    # it's optional to save these two formats\n", "    np.save(os.path.join(OUT_DIR, file_name.split('.')[0] + '.npy'), mask)\n", "    plt.imsave(os.path.join(OUT_DIR, file_name.split('.')[0] + '.png'), mask)\n"]}], "metadata": {"has_local_update": false, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}, "last_sync_time": "2022-03-16T19:01:31.757684"}, "nbformat": 4, "nbformat_minor": 2}