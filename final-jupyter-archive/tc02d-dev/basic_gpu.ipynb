{"cells": [{"cell_type": "markdown", "metadata": {"heading_collapsed": true}, "source": ["# Verify NVIDIA/CUDA device"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:28.965697Z", "start_time": "2021-08-12T15:56:28.183132Z"}, "hidden": true}, "outputs": [], "source": ["smi = !which nvidia-smi\n", "if(smi):\n", "    !nvidia-smi\n", "else:\n", "    print('No NVIDIA device found. Are you sure you are on a GPU node?')"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["## Verify device with PyTorch"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:29.478054Z", "start_time": "2021-08-12T15:56:28.968014Z"}, "hidden": true}, "outputs": [], "source": ["import torch\n", "\n", "print('Torch version:', torch.__version__)\n", "use_cuda = torch.cuda.is_available()\n", "if use_cuda:\n", "    print('Number of CUDA Devices:', torch.cuda.device_count())\n", "    print('CUDA Device Name:',torch.cuda.get_device_name(0))\n", "    print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n", "    print('CUDA version:', torch.version.cuda)\n", "else:\n", "    print('No CUDA-GPU available from Torch perspective.')"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["## Verify device with TensorFlow"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:31.748961Z", "start_time": "2021-08-12T15:56:29.479872Z"}, "hidden": true, "scrolled": false}, "outputs": [], "source": ["import tensorflow as tf\n", "from tensorflow.python.client import device_lib\n", "\n", "print(\"TF version:\", tf.__version__)\n", "print(\"TF detected devices:\", device_lib.list_local_devices())\n", "print(\"Is TF built with CUDA?\", tf.test.is_built_with_cuda())\n", "print(\"List GPUs:\", tf.config.list_physical_devices('GPU'))"]}, {"cell_type": "markdown", "metadata": {"heading_collapsed": true}, "source": ["# Testing PyTorch\n", "\n", "Here it is just to test memory allocation in host and device. If you want to add more, go ahead!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:33.891784Z", "start_time": "2021-08-12T15:56:31.750600Z"}, "hidden": true}, "outputs": [], "source": ["import torch\n", "\n", "t1 = torch.tensor([\n", "    [1,2],\n", "    [3,4]\n", "])\n", "\n", "t2 = torch.tensor([\n", "    [5,6],\n", "    [7,8]\n", "])\n", "\n", "print(\"Tensors are allocated on: \", t1.device, \" and \", t2.device)\n", "\n", "t2 = t2.to('cuda')\n", "\n", "print(\"Tensors are allocated on: \", t1.device, \" and \", t2.device)"]}, {"cell_type": "markdown", "metadata": {"heading_collapsed": true}, "source": ["# Testing TensorFlow\n", "\n", "Taken from here: https://gist.github.com/j-min/baae1aa56e861cab9831b3722755ae6d"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["## Matrix operations"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:33.979170Z", "start_time": "2021-08-12T15:56:33.893912Z"}, "hidden": true}, "outputs": [], "source": ["import numpy as np\n", "from time import perf_counter\n", "import tensorflow.compat.v1 as tf\n", "tf.disable_v2_behavior()\n", "\n", "# define matrix operation A^n + B^n\n", "def matpow(M, n):\n", "    \"\"\"\n", "    Takes in matrix M and multiplies it by itself n-1 times.\n", "    \"\"\"\n", "    if n < 1:\n", "        return M\n", "    else:\n", "        return tf.matmul(M, matpow(M,n-1))\n", "    \n", "# define A, B, n    \n", "size = 1000\n", "A = np.random.rand(size, size).astype('float32')\n", "B = np.random.rand(size, size).astype('float32')\n", "n = 10"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["### GPU multiplication"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:34.322281Z", "start_time": "2021-08-12T15:56:33.980751Z"}, "hidden": true}, "outputs": [], "source": ["c1 = []\n", "c2 = []\n", "\n", "with tf.device('/gpu:0'):\n", "    a = tf.placeholder(tf.float32, [size, size])\n", "    b = tf.placeholder(tf.float32, [size, size])\n", "    c1.append(matpow(a, n))\n", "    c2.append(matpow(b, n))\n", "\n", "with tf.device('/cpu:0'):\n", "  sum = tf.add_n(c1)\n", "\n", "start_time = perf_counter()\n", "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n", "    # Run the op.\n", "    sess.run(sum, {a:A, b:B})\n", "stop_time = perf_counter()\n", "\n", "print('GPU time: %g s' % (stop_time-start_time))"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["### CPU multiplication"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:34.326031Z", "start_time": "2021-08-12T15:56:34.323814Z"}, "hidden": true}, "outputs": [], "source": ["# I'm leaving this cell commented \n", "# If you select \"Run All\" cells you're gonna get stuck here for ~5 minutes,\n", "# which is useless cause this is supposed to test GPU usage, not cpu.\n", "# Uncomment this if you want to really see it\n", "\n", "#c1 = []\n", "#c2 = []\n", "#with tf.device('/cpu:0'):\n", "#    a = tf.placeholder(tf.float32, [size, size])\n", "#    b = tf.placeholder(tf.float32, [size, size])\n", "#    c1.append(matpow(a, n))\n", "#    c2.append(matpow(b, n))\n", "#with tf.device('/cpu:0'):\n", "#  sum = tf.add_n(c1)\n", "#start_time = perf_counter()\n", "#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n", "#    sess.run(sum, {a:A, b:B})\n", "#stop_time = perf_counter()\n", "#print('CPU time: %g s' % (stop_time-start_time))"]}, {"cell_type": "markdown", "metadata": {"heading_collapsed": true}, "source": ["# Testing Numba \n", "\n", "Taken from here: https://github.com/keipertk/pygpu-workshop"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:34.663093Z", "start_time": "2021-08-12T15:56:34.328142Z"}, "hidden": true}, "outputs": [], "source": ["import numpy as np\n", "from time import perf_counter\n", "from numba import vectorize"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:35.314101Z", "start_time": "2021-08-12T15:56:34.664779Z"}, "hidden": true}, "outputs": [], "source": ["@vectorize(['float32(float32, float32)'], target='cuda')\n", "def add_vec(v1, v2):\n", "    return v1 + v2\n", "\n", "\n", "\n", "N=1<<22\n", "\n", "a = np.ones(N, dtype=np.float32)\n", "b = np.ones(N, dtype=np.float32)\n", "c = np.empty_like(a, dtype=a.dtype)\n", "\n", "start_time = perf_counter()\n", "c = add_vec(a,b)\n", "stop_time = perf_counter()\n", "\n", "print(c)\n", "print('Elapsed time with target CUDA: %g s' % (stop_time-start_time))\n", "\n", "del a\n", "del b\n", "del c"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:35.474205Z", "start_time": "2021-08-12T15:56:35.315668Z"}, "hidden": true}, "outputs": [], "source": ["@vectorize(['float32(float32, float32)'], target='parallel')\n", "def add_vec(v1, v2):\n", "    return v1 + v2\n", "\n", "\n", "\n", "N=1<<22\n", "\n", "a = np.ones(N, dtype=np.float32)\n", "b = np.ones(N, dtype=np.float32)\n", "c = np.empty_like(a, dtype=a.dtype)\n", "\n", "start_time = perf_counter()\n", "c = add_vec(a,b)\n", "stop_time = perf_counter()\n", "\n", "print(c)\n", "print('Elapsed time with parallel: %g s' % (stop_time-start_time))\n", "\n", "del a\n", "del b\n", "del c"]}, {"cell_type": "markdown", "metadata": {"hidden": true}, "source": ["Notice here that CUDA does not necessarily give you the best performance due to communication via the PCI bus."]}, {"cell_type": "markdown", "metadata": {"heading_collapsed": true}, "source": ["# Testing CuPy\n", "\n", "Taken from: https://www.geeksforgeeks.org/python-cupy/"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:35.599918Z", "start_time": "2021-08-12T15:56:35.475893Z"}, "hidden": true}, "outputs": [], "source": ["import cupy as cp\n", "import numpy as np\n", "import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-08-12T15:56:35.853875Z", "start_time": "2021-08-12T15:56:35.601518Z"}, "hidden": true}, "outputs": [], "source": ["# NumPy and CPU Runtime\n", "s = time.time()\n", "x_cpu = np.ones((1000, 1000, 10))\n", "e = time.time()\n", "print(\"Time consumed by numpy: \", e - s)\n", "  \n", "# CuPy and GPU Runtime\n", "s = time.time()\n", "x_gpu = cp.ones((1000, 1000, 10))\n", "e = time.time()\n", "print(\"\\nTime consumed by cupy: \", e - s)"]}], "metadata": {"has_local_update": false, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "py-gpu_runtime", "language": "Python", "name": "py-gpu_runtime"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.11"}, "last_sync_time": "2021-08-12T16:15:44.101391"}, "nbformat": 4, "nbformat_minor": 2}