{"cells": [{"cell_type": "markdown", "id": "9ad261c3", "metadata": {}, "source": ["# Clustering methods\n", "\n", "For the satellite data parameter constraint problem, we would like to decide which parameters can reasonably be constrained at each GeoSurfaceTimePoint (GSTP). To decide this, we consult the length scales of the Gaussian Process emulator fitted on the data at that GSTP and then select those few which are most important. Those most important parameters are the ones which can reasonably be constrained.\n", "\n", "The problem is that different parameters are important for different GSTPs. We cluster the GSTPs according to their preferred parameters in order to obtain a few general regions over which the same parameters may be constrained. In other words, according to our clustering of GSTPs according to their corresponding length scales, we break the parameter constraint problem into a different problem for each cluster."]}, {"cell_type": "code", "execution_count": 35, "id": "de11c682", "metadata": {"ExecuteTime": {"end_time": "2022-04-27T17:54:55.216958Z", "start_time": "2022-04-27T17:54:55.211073Z"}}, "outputs": [], "source": ["def k_means(length_scales, k=2):\n", "    \"\"\"\n", "    Given a DataFrame of length scales, return a list of DataFrames, each DataFrame corresponding to a cluster and each\n", "    row corresponding to a GSTP belonging to that cluster\n", "    \"\"\"\n", "    from sklearn.cluster import KMeans\n", "    \n", "    my_k_means = KMeans(n_clusters=k).fit(length_scales)\n", "    \n", "    return([my_k_means.predict(length_scales), my_k_means.cluster_centers_])\n", "\n", "\n", "def agglom(length_scales, k=2):\n", "    \"\"\"\n", "    Agglomerative clustering (i.e. hierarchical clustering)\n", "    \"\"\"\n", "    from sklearn.cluster import AgglomerativeClustering as Agglom\n", "    \n", "    my_agglom = Agglom(n_clusters=k).fit(length_scales)\n", "    \n", "    return(my_agglom.labels_)\n", "\n", "\n", "def dbscan(length_scales, eps=3, min_samples=100):\n", "    \"\"\"\n", "    \"\"\"\n", "    from sklearn.cluster import DBSCAN\n", "    \n", "    my_dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(length_scales)\n", "    \n", "    return(my_dbscan.labels_)"]}, {"cell_type": "markdown", "id": "b396a762", "metadata": {}, "source": ["# Helpers"]}, {"cell_type": "code", "execution_count": null, "id": "20699076", "metadata": {}, "outputs": [], "source": ["def flip_length_scales(length_scales):\n", "    \n", "    # Order the length scales by input name, not length so that they are comparable\n", "    \n", "    \n", "    # Since the small length scales indicate more importance than large ones, we perform clustering on the vectors of \n", "    # inverse length scales\n", "    flipped_lengths = [1/x for x in length_scales['length_scale']]\n", "\n", "    return(flipped_lengths)"]}], "metadata": {"has_local_update": false, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}, "last_sync_time": "2022-08-09T15:55:31.027617"}, "nbformat": 4, "nbformat_minor": 5}