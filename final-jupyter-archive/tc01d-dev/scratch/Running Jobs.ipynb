{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Submiting Jobs to the C3 Cluster"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Running Asynchronos Batches of Functions\n", "\n", "In order to take advanatage of more computational resources, one should employ one of the following mechinisms to submit long-running or embarassingly parallel batches:\n", "\n", "    AsyncAction - Run a batch of jobs. Monitor and retrive results form the AsyncAction type and/or the FileSystem.\n", "    BatchJob or DynBatchJob - More sophistaced batch jobs, persist results to your own type and/or the FileSystem\n", "    MapReduce - For parallel jobs the complexity, as with other parallel compute systems, is wrangling the results. C3 uses a \"type\" system that can persist results to a database as well as a clter-wide \"external\" fileSystem (Azure Blob for DTI clusters).\n", "    \n", "Each of these approaches will put jobs in one of the cluster's queue's for execution on worker nodes.\n", "\n", "The method we reccomend for dipatching execution of a python function to worker nodes for parallel execution without provisioning types is `AsyncAction`\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Submit a Single AsyncAction\n", "Define a function and C3 \"Lambda\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Define a function and its C3 \"Lambda\" version\n", "Use the C3 type Lambda and its fromPython method to define a wrapped funtion that can be executed on the nodes of the C3 cluster."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"ExecuteTime": {"end_time": "2022-03-29T16:27:21.647502Z", "start_time": "2022-03-29T16:27:21.632978Z"}, "code_folding": []}, "outputs": [], "source": ["def compute_pi(n):\n", "    import decimal\n", "    decimal.getcontext().prec = n + 1\n", "    C = 426880 * decimal.Decimal(10005).sqrt()\n", "    K = 6.\n", "    M = 1.\n", "    X = 1\n", "    L = 13591409\n", "    S = L\n", "\n", "    for i in range(1, n):\n", "        M = M * (K ** 3 - 16 * K) / ((i + 1) ** 3)\n", "        L += 545140134\n", "        X *= -262537412640768000\n", "        S += decimal.Decimal(M * L) / X\n", "\n", "        pi = C / S\n", "    return str(pi)\n", "compute_pi_c3 = c3.Lambda.fromPython(compute_pi)\n", "\n", "def sleeper(n, say_something, something):\n", "    import time\n", "    time.sleep(n)\n", "    if say_something:\n", "        out = str(n) + '_' + something\n", "    else:\n", "        out = str(n)\n", "    return out\n", "sleeper_c3 = c3.Lambda.fromPython(sleeper)\n", "# example execution locally\n", "#sleeper_c3.apply([2])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Define the `AsyncActionSpec` for a Specific Run\n", "Here, we are specifying how to call the `apply` method in the C3 `Lambda` type.  Which allows us to \"apply\" the Lambda we created from the function we wanted to execute.  \n", "We pass \"args\" to the `apply` method which take the form of a list of inputs [input1, input2, ...].\n", "the `c3.c3Make()` call serializes the native input list so that it plays nice with the API's on the C3 platform."]}, {"cell_type": "code", "execution_count": 3, "metadata": {"ExecuteTime": {"end_time": "2022-03-08T17:55:32.821807Z", "start_time": "2022-03-08T17:55:32.814978Z"}}, "outputs": [{"data": {"text/plain": ["c3.AsyncActionSpec(\n", " typeName='Lambda',\n", " actionName='null',\n", " args=c3.Mapp<string, any>({'args': c3.Arry<any>([2, True, 'string']),\n", "        'this': c3.Lambda<function(n: any, say_something: any, something: any): any>(\n", "                 language='python',\n", "                 implementation='def sleeper(n, say_something, something):\\n'\n", "                                 '    import time\\n'\n", "                                 '    time.sleep(n)\\n'\n", "                                 '    if say_something:\\n'\n", "                                 \"        out = str(n) + '_' + something\\n\"\n", "                                 '    else:\\n'\n", "                                 '        out = str(n)\\n'\n", "                                 '    return out')}),\n", " action='apply')"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["actionSpec = c3.AsyncActionSpec(\n", "    typeName = \"Lambda\",\n", "    action = \"apply\",\n", "    args = {\n", "        \"this\": sleeper_c3,\n", "        \"args\": c3.c3Make(\"[any]\",[2,True,\"string\"])\n", "    }\n", ")\n", "actionSpec"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Submit and Monitor the action"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2022-03-08T17:55:38.347373Z", "start_time": "2022-03-08T17:55:38.247988Z"}}, "outputs": [], "source": ["action = c3.AsyncAction.submit(actionSpec)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2022-03-08T17:55:44.072442Z", "start_time": "2022-03-08T17:55:43.999441Z"}}, "outputs": [], "source": ["action = c3.AsyncAction.get({'id': action.id})\n", "done = action.hasCompleted()\n", "if done:\n", "    print(action.result)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"ExecuteTime": {"end_time": "2022-02-16T20:29:48.648657Z", "start_time": "2022-02-16T20:29:48.643667Z"}}, "outputs": [{"data": {"text/plain": ["c3.AsyncAction(\n", " typeName='Lambda',\n", " actionName='apply',\n", " args=c3.Mapp<string, any>({'args': c3.Arry<any>([2, True, 'string']),\n", "        'this': c3.Lambda<function(n: any, say_something: any, something: any): any>(\n", "                 language='python',\n", "                 implementation='def sleeper(n, say_something, something):\\n'\n", "                                 '    import time\\n'\n", "                                 '    time.sleep(n)\\n'\n", "                                 '    if say_something:\\n'\n", "                                 \"        out = str(n) + '_' + something\\n\"\n", "                                 '    else:\\n'\n", "                                 '        out = str(n)\\n'\n", "                                 '    return out')}),\n", " action='apply',\n", " id='7109ee47-d244-4bad-9fcd-fcee2a1a1e5d',\n", " meta=c3.Meta(\n", "        tenantTagId=150,\n", "        tenant='dev',\n", "        tag='tc01d',\n", "        created=datetime.datetime(2022, 2, 16, 20, 29, 40, tzinfo=datetime.timezone.utc),\n", "        createdBy='dadams@illinois.edu',\n", "        updated=datetime.datetime(2022, 2, 16, 20, 29, 40, tzinfo=datetime.timezone.utc),\n", "        updatedBy='dadams@illinois.edu',\n", "        timestamp=datetime.datetime(2022, 2, 16, 20, 29, 40, tzinfo=datetime.timezone.utc),\n", "        fetchInclude='[]',\n", "        fetchType='AsyncAction'),\n", " version=1,\n", " completed=False)"]}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": ["action"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Submit Async Action to a Particular Hardware Profile"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2022-03-08T17:55:51.226848Z", "start_time": "2022-03-08T17:55:51.208415Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["GPU_V100_SMALL STANDARD_NC6S_V3\n"]}], "source": ["profiles = c3.HardwareProfile.fetch().objs\n", "for p in profiles:\n", "    print(p.name, p.instanceType)"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"ExecuteTime": {"end_time": "2022-03-29T16:42:52.735275Z", "start_time": "2022-03-29T16:42:52.732246Z"}}, "outputs": [], "source": ["def gpuTest():\n", "    import tensorflow as tf\n", "    from tensorflow.python.client import device_lib\n", "    devs = device_lib.list_local_devices()\n", "    print(devs)\n", "    return list(map(str,devs))"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"ExecuteTime": {"end_time": "2022-03-29T16:42:46.624908Z", "start_time": "2022-03-29T16:42:46.579216Z"}}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'tensorflow'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-12-515a032efb3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpuTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-11-b89129411023>\u001b[0m in \u001b[0;36mgpuTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgpuTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}], "source": ["gpuTest()"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"ExecuteTime": {"end_time": "2022-03-29T16:42:55.999752Z", "start_time": "2022-03-29T16:42:55.793110Z"}}, "outputs": [], "source": ["gpuTest_c3 =c3.Lambda.fromPython(gpuTest,runtime=\"tensorflow_3_0_0\")"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"ExecuteTime": {"end_time": "2022-03-29T16:45:02.327509Z", "start_time": "2022-03-29T16:45:02.321764Z"}}, "outputs": [{"data": {"text/plain": ["c3.AsyncActionSpec(\n", " typeName='Lambda',\n", " actionName='null',\n", " args=c3.Mapp<string, any>({'this': c3.Lambda<function(): any>(\n", "                 language='python',\n", "                 implementation='def gpuTest():\\n'\n", "                                 '    import tensorflow as tf\\n'\n", "                                 '    from tensorflow.python.client import '\n", "                                 'device_lib\\n'\n", "                                 '    devs = device_lib.list_local_devices()\\n'\n", "                                 '    print(devs)\\n'\n", "                                 '    return list(map(str,devs))',\n", "                 runtime='py-tensorflow_3_0_0')}),\n", " action='apply',\n", " hardwareProfile='GPU_V100_SMALL')"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["# hardwareProfile = \"GPU_V100_SMALL\"\n", "actionSpec = c3.AsyncActionSpec(\n", "    typeName = \"Lambda\",\n", "    action = \"apply\",\n", "    args = {\n", "        \"this\": gpuTest_c3\n", "    },\n", "    hardwareProfile = \"GPU_V100_SMALL\"\n", ")\n", "actionSpec"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"ExecuteTime": {"end_time": "2022-03-29T16:45:04.758031Z", "start_time": "2022-03-29T16:45:04.641518Z"}}, "outputs": [], "source": ["action = c3.AsyncAction.submit(actionSpec)"]}, {"cell_type": "code", "execution_count": 24, "metadata": {"ExecuteTime": {"end_time": "2022-03-29T18:01:25.369281Z", "start_time": "2022-03-29T18:01:25.137306Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['name: \"/device:CPU:0\"\\ndevice_type: \"CPU\"\\nmemory_limit: 268435456\\nlocality {\\n}\\nincarnation: 14264760189499237810\\n', 'name: \"/device:XLA_CPU:0\"\\ndevice_type: \"XLA_CPU\"\\nmemory_limit: 17179869184\\nlocality {\\n}\\nincarnation: 12365955161697616446\\nphysical_device_desc: \"device: XLA_CPU device\"\\n']\n"]}], "source": ["action = c3.AsyncAction.get({'id': action.id})\n", "done = action.hasCompleted()\n", "if done:\n", "    print(action.result)\n", "else:\n", "    print(action)"]}, {"cell_type": "code", "execution_count": 22, "metadata": {"ExecuteTime": {"end_time": "2022-02-16T20:30:56.153493Z", "start_time": "2022-02-16T20:30:56.148497Z"}}, "outputs": [{"data": {"text/plain": ["c3.AsyncAction(\n", " typeName='Lambda',\n", " actionName='apply',\n", " args=c3.Mapp<string, any>({'args': c3.Arry<any>([3000]),\n", "        'this': c3.Lambda<function(n: any): any>(\n", "                 language='python',\n", "                 implementation='def compute_pi(n):\\n'\n", "                                 '    import decimal\\n'\n", "                                 '    decimal.getcontext().prec = n + 1\\n'\n", "                                 '    C = 426880 * '\n", "                                 'decimal.Decimal(10005).sqrt()\\n'\n", "                                 '    K = 6.\\n'\n", "                                 '    M = 1.\\n'\n", "                                 '    X = 1\\n'\n", "                                 '    L = 13591409\\n'\n", "                                 '    S = L\\n'\n", "                                 '\\n'\n", "                                 '    for i in range(1, n):\\n'\n", "                                 '        M = M * (K ** 3 - 16 * K) / ((i + 1) '\n", "                                 '** 3)\\n'\n", "                                 '        L += 545140134\\n'\n", "                                 '        X *= -262537412640768000\\n'\n", "                                 '        S += decimal.Decimal(M * L) / X\\n'\n", "                                 '\\n'\n", "                                 '        pi = C / S\\n'\n", "                                 '    return str(pi)')}),\n", " action='apply',\n", " id='2430ade7-9a43-4c07-81a1-90c0515aef66',\n", " meta=c3.Meta(\n", "        tenantTagId=150,\n", "        tenant='dev',\n", "        tag='tc01d',\n", "        created=datetime.datetime(2022, 2, 16, 20, 30, 53, tzinfo=datetime.timezone.utc),\n", "        createdBy='dadams@illinois.edu',\n", "        updated=datetime.datetime(2022, 2, 16, 20, 30, 53, tzinfo=datetime.timezone.utc),\n", "        updatedBy='dadams@illinois.edu',\n", "        timestamp=datetime.datetime(2022, 2, 16, 20, 30, 53, tzinfo=datetime.timezone.utc),\n", "        fetchInclude='[]',\n", "        fetchType='AsyncAction'),\n", " version=1,\n", " completed=False)"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": ["action"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Submit a Batch of `AsyncAction`s"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"ExecuteTime": {"end_time": "2021-11-02T17:00:26.309992Z", "start_time": "2021-11-02T17:00:26.169392Z"}}, "outputs": [], "source": ["import numpy as np\n", "inputs = np.random.randint(100,120,100)\n", "specs = [\n", "    c3.AsyncActionSpec(\n", "    typeName = \"Lambda\",\n", "    action = \"apply\",\n", "    args = {\n", "        \"this\": sleeper_c3,\n", "        \"args\": c3.c3Make(\"[int]\",[n])\n", "    }\n", ") for n in inputs\n", "]"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"ExecuteTime": {"end_time": "2021-11-02T17:01:16.953023Z", "start_time": "2021-11-02T17:01:11.446380Z"}}, "outputs": [], "source": ["for spec in specs:\n", "    c3.AsyncAction.submit(spec)"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"ExecuteTime": {"end_time": "2021-11-02T17:39:47.913396Z", "start_time": "2021-11-02T17:39:47.905524Z"}}, "outputs": [{"ename": "TypeError", "evalue": "type.__new__() takes exactly 3 arguments (0 given)", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-27-517a1d5e551b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m c3.Type(**{\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"from\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"SleeperInput\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m }\n\u001b[1;32m      4\u001b[0m     )\n", "\u001b[0;31mTypeError\u001b[0m: type.__new__() takes exactly 3 arguments (0 given)"]}], "source": ["c3.Type(**{\n", "    \"from\": {\"name\": \"SleeperInput\"}\n", "}\n", "    )"]}, {"cell_type": "code", "execution_count": 28, "metadata": {"ExecuteTime": {"end_time": "2021-11-02T17:48:51.998928Z", "start_time": "2021-11-02T17:48:51.989967Z"}}, "outputs": [{"ename": "TypeError", "evalue": "_box() missing 1 required positional argument: 'obj'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-28-2f9e37f237a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mTypeError\u001b[0m: _box() missing 1 required positional argument: 'obj'"]}], "source": ["c3._box() "]}, {"cell_type": "code", "execution_count": 7, "metadata": {"ExecuteTime": {"end_time": "2021-11-08T17:43:17.468771Z", "start_time": "2021-11-08T17:43:17.021322Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n", "True\n"]}], "source": ["for job in c3.AsyncAction.fetch(spec={\"include\":\"id,completed\"}).objs:\n", "    #stat = job.status().status\n", "    stat = job.completed\n", "    print(stat)\n", "#    if not stat:\n", "#         print(stat)\n", "#         #job.cancel()"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"ExecuteTime": {"end_time": "2021-11-08T17:30:07.182686Z", "start_time": "2021-11-08T17:30:07.141571Z"}}, "outputs": [{"data": {"text/plain": ["1238"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["c3.AsyncAction.fetchCount()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"has_local_update": false, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}, "last_sync_time": "2022-05-18T18:01:37.975969"}, "nbformat": 4, "nbformat_minor": 4}