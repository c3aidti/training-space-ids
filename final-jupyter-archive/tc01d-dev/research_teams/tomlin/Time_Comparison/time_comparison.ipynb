{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:05:40.550519Z", "start_time": "2021-11-20T02:05:40.531416Z"}}, "outputs": [], "source": ["from datetime import datetime, timedelta\n", "import time\n", "import netCDF4\n", "import numpy as np\n", "import bisect\n", "import math"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:05:40.622543Z", "start_time": "2021-11-20T02:05:40.555012Z"}, "code_folding": [0]}, "outputs": [], "source": ["#Reference function\n", "def get_current_data_subset(nc_file, x_0, x_T, deg_around_x0_xT_box, fixed_time=None,\n", "                            temporal_stride=1, temp_horizon_in_h=None\n", "):\n", "    \"\"\" Function to read a subset of the nc_file current data bounded by a box spanned by the x_0 and x_T points.\n", "    Inputs:\n", "        nc_file                 full path to nc file\n", "        x_0                     [lon, lat, charge, timestamp in POSIX]\n", "        x_T                     [lon, lat] goal locations\n", "        deg_around_x0_xT_box    float, buffer around the box in degrees\n", "        fixed_time              if None returns time-varying currents, \n", "                                otherwise datetime object of the fixed time -> returns ocean current grid at or before time\n", "                                the time of x_0 is then ignored\n", "        temporal_stride         int, if a stride of the temporal values is used (every temporal_stride hours)\n", "        temp_horizon            if None: all available time of the file will be provided\n", "                                otherwise float, maximum temp_horizon to look ahead of x_0 time in hours\n", "                                \n", "    Outputs:\n", "        grids_dict              dict containing x_grid, y_grid, t_grid, fixed_time_idx\n", "        u_data                  [T, Y, X] matrix of the ocean currents in x direction in m/s\n", "        v_data                  [T, Y, X] matrix of the ocean currents in y direction in m/s\n", "        \n", "    \"\"\"\n", "    \n", "    f = netCDF4.Dataset(nc_file)\n", "\n", "    # extract positiond & start_time for the indexing\n", "    x_0_pos = x_0[:2]\n", "    x_0_posix_time = x_0[3]\n", "    x_T = x_T[:2]\n", "\n", "    # Step 1: get the grids\n", "    xgrid = f.variables['lon'][:]\n", "    ygrid = f.variables['lat'][:]\n", "    t_grid = f.variables['time'][:] # not this is in hours from HYCOM data!\n", "    \n", "    # this is needed because the time origin in hindcast and forecase nc files is different. Very handcrafted.\n", "    try:\n", "        time_origin = datetime.strptime(f.variables['time'].__dict__['time_origin'] + ' +0000',\n", "                                        '%Y-%m-%d %H:%M:%S %z')\n", "    except:\n", "        time_origin = datetime.strptime(f.variables['time'].__dict__['units'] + ' +0000',\n", "                                                 'hours since %Y-%m-%d %H:%M:%S.000 UTC %z')\n", "\n", "    # Step 2: find the sub-setting\n", "    # find the lat & lon sub-set bounds\n", "    lon_bnds = [min(x_0_pos[0], x_T[0]) - deg_around_x0_xT_box, max(x_0_pos[0], x_T[0]) + deg_around_x0_xT_box]\n", "    lat_bnds = [min(x_0_pos[1], x_T[1]) - deg_around_x0_xT_box, max(x_0_pos[1], x_T[1]) + deg_around_x0_xT_box]\n", "\n", "    # get the respective indices from the grids\n", "    ygrid_inds = np.where((ygrid >= lat_bnds[0]) & (ygrid <= lat_bnds[1]))[0]\n", "    xgrid_inds = np.where((xgrid >= lon_bnds[0]) & (xgrid <= lon_bnds[1]))[0]\n", "\n", "    # for time indexing transform to POSIX time\n", "    abs_t_grid = [(time_origin + timedelta(hours=X)).timestamp() for X in t_grid.data]\n", "    \n", "    # get the idx of the value left of the demanded time (for interpolation function)\n", "    t_start_idx = bisect.bisect_right(abs_t_grid, x_0_posix_time) - 1\n", "    if t_start_idx == len(abs_t_grid) - 1 or t_start_idx == -1:\n", "        raise ValueError(\"Requested subset time is outside of the nc4 file.\")\n", "\n", "    # get the max time if provided as input\n", "    if temp_horizon_in_h is None:   # all data provided\n", "        t_end_idx = len(abs_t_grid)-1\n", "    else:\n", "        t_end_idx = bisect.bisect_right(abs_t_grid, x_0_posix_time + temp_horizon_in_h*3600.)\n", "        if t_end_idx == len(abs_t_grid):\n", "            raise ValueError(\"nc4 file does not contain requested temporal horizon.\")\n", "\n", "    # fixed time logic if necessary\n", "    if fixed_time is None:\n", "        slice_for_time_dim = np.s_[t_start_idx:(t_end_idx+1):temporal_stride]\n", "        fixed_time_idx = None\n", "    else:\n", "        fixed_time_idx = bisect.bisect_right(abs_t_grid, fixed_time.timestamp()) - 1\n", "        slice_for_time_dim = np.s_[fixed_time_idx]\n", "\n", "    # Step 2: extract data\n", "    # raw water_u is [tdim, zdim, ydim, xdim]\n", "    if len(f.variables['water_u'].shape) == 4:  # if there is a depth dimension in the dataset\n", "        u_data = f.variables['water_u'][slice_for_time_dim, 0, ygrid_inds, xgrid_inds]\n", "        v_data = f.variables['water_v'][slice_for_time_dim, 0, ygrid_inds, xgrid_inds]\n", "    # raw water_u is [tdim, ydim, xdim]\n", "    elif len(f.variables['water_u'].shape) == 3:  # if there is no depth dimension in the dataset\n", "        u_data = f.variables['water_u'][slice_for_time_dim, ygrid_inds, xgrid_inds]\n", "        v_data = f.variables['water_v'][slice_for_time_dim, ygrid_inds, xgrid_inds]\n", "    else:\n", "        raise ValueError(\"Current data in nc file has neither 3 nor 4 dimensions. Check file.\")\n", "\n", "    # create dict to output\n", "    grids_dict = {'x_grid': xgrid[xgrid_inds], 'y_grid': ygrid[ygrid_inds],\n", "                  't_grid': abs_t_grid[slice_for_time_dim], 'fixed_time_idx': fixed_time_idx}\n", "\n", "    # log what data has been subsetted\n", "    if fixed_time is None:\n", "        print(\"Subsetted data from {start} to {end} in {n_steps} time steps of {time:.2f} hour(s) resolution\".format(\n", "            start=datetime.utcfromtimestamp(grids_dict['t_grid'][0]).strftime('%Y-%m-%d %H:%M:%S UTC'),\n", "            end=datetime.utcfromtimestamp(grids_dict['t_grid'][-1]).strftime('%Y-%m-%d %H:%M:%S UTC'),\n", "            n_steps=len(grids_dict['t_grid']), time=(grids_dict['t_grid'][1] - grids_dict['t_grid'][0])/3600.))\n", "    else:\n", "        print(\"Subsetted data to fixed time at: {time}\".format(\n", "            time=datetime.utcfromtimestamp(grids_dict['t_grid'][0]).strftime('%Y-%m-%d %H:%M:%S UTC')))\n", "\n", "    #TODO: we replace the masked array with fill value 0 because otherwise interpolation doesn't work.\n", "    # Though that means we cannot anymore detect if we're on land or not (need a way to do that/detect stranding)\n", "    # not sure yet if we'll do it in the simulator or where.\n", "    return grids_dict, u_data.filled(fill_value=0.), v_data.filled(fill_value=0.)"]}, {"cell_type": "code", "execution_count": 35, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:21:01.937766Z", "start_time": "2021-11-20T02:21:01.934038Z"}}, "outputs": [{"data": {"text/plain": ["'start>=\"2021-09-02\" && end<=\"2021-09-03T23:00:00.000\" && status==\"downloaded\"'"]}, "execution_count": 35, "metadata": {}, "output_type": "execute_result"}], "source": ["filter_string"]}, {"cell_type": "code", "execution_count": 36, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:21:13.866398Z", "start_time": "2021-11-20T02:21:13.835269Z"}}, "outputs": [], "source": ["start = datetime.utcfromtimestamp(t_interval[0])\n", "end = datetime.utcfromtimestamp(t_interval[1])\n", "# Step 1.2: Getting correct range of nc files from database\n", "filter_string = 'start>=' + '\"'+ start.strftime(\"%Y-%m-%d\") + '\"' + \\\n", "                ' && end<=' + '\"' + end.strftime(\"%Y-%m-%d\") + \"T23:00:00.000\" + '\"'\\\n", "                ' && status==' + '\"' + 'downloaded' + '\"'\n", "objs_list = c3.HindcastFile.fetch({'filter':filter_string, \"order\": \"start\"}).objs\n", "\n", "# some basic sanity checks\n", "if objs_list is None:\n", "    raise ValueError(\"No files in the database for the selected t_interval\")\n", "if len(objs_list) != (end - start).days + 1:\n", "    raise ValueError(\"DB Query didn't return the expected number of files (one per day), check DB and code.\")"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:09:23.798522Z", "start_time": "2021-11-20T02:09:23.794243Z"}}, "outputs": [{"data": {"text/plain": ["datetime.datetime(2021, 9, 2, 12, 0)"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["start_date"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:11:19.236371Z", "start_time": "2021-11-20T02:11:19.232546Z"}}, "outputs": [{"data": {"text/plain": ["1"]}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": ["(end_date - start_date).days"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:06:49.601988Z", "start_time": "2021-11-20T02:06:49.598645Z"}}, "outputs": [{"data": {"text/plain": ["'2021-09-03'"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["end_date"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:06:13.353573Z", "start_time": "2021-11-20T02:06:13.344058Z"}}, "outputs": [{"data": {"text/plain": ["2"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["len(objs_list)"]}, {"cell_type": "code", "execution_count": 38, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:22:16.112565Z", "start_time": "2021-11-20T02:22:16.098406Z"}, "code_folding": []}, "outputs": [], "source": ["#C3 file based function\n", "def get_current_data_subset_from_c3_file(\n", "    t_interval, #temp_res_in_h,   ----> separate function\n", "    lat_interval, #lat_res_in_deg,\n", "    lon_interval, #lon_res_in_deg,\n", "    #depth_interval_to_avg_over\n", "):\n", "    \n", "    # scipy.interpolate.interp1d\n", "    \n", "    \"\"\" Function to get a subset of current data via the C3 data integration.\n", "    \n", "    Inputs:\n", "        t_interval              if time-varying: [t_0, t_T] in POSIX time\n", "                                where t_0 and t_T are the start and end timestamps respectively\n", "                                if fixed_time:   [fixed_timestamp] in POSIX\n", "        temp_res_in_h           which temporal resolution the time-axis should have\n", "                                e.g. if temp_res_in_h = 1, t_grid = [t_0, t_0 + 3600s, ... t_T]\n", "                                if temp_res_in_h = 5,      t_grid = [t_0, t_0 + 5*3600s, ... t_T]\n", "                                if temp_res_in_h = 0.5,      t_grid = [t_0, t_0 + 1800s, ... t_T]\n", "                                => so either averaging or interpolation needs to be done in the backend\n", "        lat_interval            [y_lower, y_upper] in degrees\n", "        lat_res_in_deg          which spatial resolution in y direction in degrees\n", "                                e.g. if lat_res_in_deg = 1, y_grid = [y_lower, y_lower + 1, ... y_upper]\n", "                                 => so either averaging or interpolation needs to be done in the backend\n", "        lon_interval            [x_lower, x_upper] in degrees\n", "        lon_res_in_deg          which spatial resolution in x direction in degrees\n", "                                e.g. if lon_res_in_deg = 1, x_grid = [x_lower, x_lower + 1, ... x_upper]\n", "                                 => so either averaging or interpolation needs to be done in the backend\n", "        depth_interval_to_avg_over\n", "                                Interval to average over the current dimension in meters\n", "                                e.g. [0, 10] then the currents are averaged over the depth 0-10m.\n", "                                \n", "    Outputs:\n", "        grids_dict              dict containing x_grid, y_grid, t_grid\n", "        u_data                  [T, Y, X] matrix of the ocean currents in x direction in m/s\n", "        v_data                  [T, Y, X] matrix of the ocean currents in y direction in m/s\n", "    \"\"\"\n", "    \n", "    # Step 1: get required file references and data from C3 file DB\n", "    # Step 1.1: Getting time and formatting for the db query\n", "    start = datetime.utcfromtimestamp(t_interval[0])\n", "    end = datetime.utcfromtimestamp(t_interval[1])\n", "\n", "    # Step 1.2: Getting correct range of nc files from database\n", "    filter_string = 'start>=' + '\"'+ start.strftime(\"%Y-%m-%d\") + '\"' + \\\n", "                    ' && end<=' + '\"' + end.strftime(\"%Y-%m-%d\") + \"T23:00:00.000\" + '\"'\\\n", "                    ' && status==' + '\"' + 'downloaded' + '\"'\n", "    objs_list = c3.HindcastFile.fetch({'filter':filter_string, \"order\": \"start\"}).objs\n", "\n", "    # some basic sanity checks\n", "    if objs_list is None:\n", "        raise ValueError(\"No files in the database for the selected t_interval\")\n", "    if len(objs_list) != (end - start).days + 1:\n", "        raise ValueError(\"DB Query didn't return the expected number of files (one per day), check DB and code.\")\n", "    \n", "    # Step 1.3: extract url and start list from the query results\n", "    urls_list = [obj.file.url for obj in objs_list]\n", "    start_list = [obj.start for obj in objs_list]\n", "    \n", "    # Step 2: Prepare the stacking loop by getting the x, y grids and subsetting indices in x, y \n", "    # Note: these stay constant across files in this case where all files have same lat-lon range\n", "    \n", "    # Step 2.1: open the file and get the x and y grid\n", "    f = c3.HycomUtil.nc_open(urls_list[0])\n", "    xgrid = f.variables['lon'][:].data\n", "    ygrid = f.variables['lat'][:].data\n", "    \n", "    # Step 2.2: get the respective indices of the lat, lon subset from the file grids\n", "    ygrid_inds = np.where((ygrid >= lat_interval[0]) & (ygrid <= lat_interval[1]))[0]\n", "    xgrid_inds = np.where((xgrid >= lon_interval[0]) & (xgrid <= lon_interval[1]))[0]\n", "    \n", "    # Step 2.3 initialze t_grid stacking variable\n", "    full_t_grid = []\n", "\n", "    # Step 3: iterate over all files in order and stack the current data and absolute t_grids\n", "    for idx in range(len(start_list)):\n", "        # Step 3.0: load the current data file\n", "        f = c3.HycomUtil.nc_open(urls_list[idx])\n", "        # set the default start and end time\n", "        start_hr, end_hr = 0, 24\n", "        \n", "        # Step 3.1: do the time-subsetting\n", "        #Case 1: file is first -- get data from the file from the hour before or at t_0\n", "        if idx == 0:\n", "            start_hr = math.floor((t_interval[0] - start_list[idx].timestamp())/3600)\n", "        #Case 2: file is last -- get data from file until or after the hour t_T\n", "        if idx == len(start_list)-1:\n", "            end_hr = math.ceil((t_interval[1] - start_list[idx].timestamp())/3600)+1\n", "\n", "        # Step 3.2: extract data from the file\n", "        u_data = f.variables['water_u'][start_hr:end_hr, 0, ygrid_inds, xgrid_inds]\n", "        v_data = f.variables['water_v'][start_hr:end_hr, 0, ygrid_inds, xgrid_inds]\n", "\n", "        # Step 3.3: stack the sub-setted abs_t_grid and current data\n", "        full_t_grid = full_t_grid + [start_list[idx].timestamp() + i*3600 for i in range(start_hr, end_hr)]\n", "        \n", "        if idx == 0:\n", "            full_u_data = u_data\n", "            full_v_data = v_data\n", "        else:\n", "            full_u_data = np.concatenate((full_u_data, u_data), axis=0)\n", "            full_v_data = np.concatenate((full_v_data, v_data), axis=0)\n", "\n", "    # Step 4: create dict to output\n", "    grids_dict = {'x_grid': xgrid[xgrid_inds], 'y_grid': ygrid[ygrid_inds], 't_grid': full_t_grid}\n", "    \n", "    # Step 5: # log what data has been subsetted\n", "    print(\"Subsetted data from {start} to {end} in {n_steps} time steps of {time:.2f} hour(s) resolution\".format(\n", "        start=datetime.utcfromtimestamp(grids_dict['t_grid'][0]).strftime('%Y-%m-%d %H:%M:%S UTC'),\n", "        end=datetime.utcfromtimestamp(grids_dict['t_grid'][-1]).strftime('%Y-%m-%d %H:%M:%S UTC'),\n", "        n_steps=len(grids_dict['t_grid']), time=(grids_dict['t_grid'][1] - grids_dict['t_grid'][0])/3600.))\n", "\n", "    # Step 6: return the grids_dict and the stacked data\n", "    # TODO: currently, we just do fill_value =0 but then we can't detect if we're on land. \n", "    # We need a way to do that in the simulator, doing it via the currents could be one way.\n", "    return grids_dict, full_u_data.filled(fill_value=0.), full_v_data.filled(fill_value=0.)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:05:53.097178Z", "start_time": "2021-11-20T02:05:53.088770Z"}, "code_folding": [0, 3]}, "outputs": [], "source": ["#C3 data based function\n", "def get_current_data_subset_from_c3_database(t_interval, lat_interval, lon_interval, \n", "                                             metric=\"TestAverageWaterU\", interval=\"HOUR\"\n", "):\n", "    \"\"\"\n", "    Returns a subset for the given metric data from the given start and end time. \n", "    This version does not using Streaming and so may be slower (?)\n", " \n", "    Args:\n", "        t_interval (float): [t_0, t_T] in POSIX time where t_0 and t_T are the start and end timestamps respectively\n", "        lat_interval (float tuple): [y_lower, y_upper] in degrees\n", "        lon_interval (float tuple): [x_lower, x_upper] in degrees\n", "        \n", "        metric (str): Metric to be extracted from the dataset   \n", "        interval (string): frequency of datapoints to output (only works for \"HOUR\" right now!)\n", " \n", "    Returns:\n", "        array: Numpy array of the subsetted data ordered as [time,lat,lon]\n", " \n", "    Notes:\n", "        - Currently designed/tested for hour resolution\n", "        - Only tested for single metric\n", "        - Comments included with times for different sections of the function. These times are not averaged \n", "          (only one run used) and are there to present an idea of the runtime of different sections of the code.\n", "          NOTE: significnat time variance when fetching from c3\n", "    \"\"\"\n", "    ###PRE-QUERY SECTION###\n", "    #takes 0.1423 seconds for 1x1 lat-lon, 1 day\n", "    #convert times to datetime\n", "    start_time = datetime.fromtimestamp(t_interval[0])\n", "    \n", "    #+ interval_len because EvalMetrics exclusive for end time (NOTE: modify to work for other intervals)\n", "    end_time = datetime.fromtimestamp(t_interval[1] + 3600)\n", "    \n", "\n", "    #filter for query\n", "    filter = \"lat>={} && lat<={} && lon>={} && lon<={}\".format(lat_interval[0], lat_interval[1], \n", "                                                               lon_interval[0], lon_interval[1])\n", "\n", "    #get lat, lon dimensions\n", "    objs_list = c3.HycomLatLongPair.fetch(spec={\"include\": \"id, lat, lon\", \"filter\": filter, \"limit\": -1}).objs\n", "    lat_dim =  len(np.unique([obj.lat for obj in objs_list]))\n", "    lon_dim = len(np.unique([obj.lon for obj in objs_list]))\n", "\n", "    ###QUERY SECTION###\n", "    #takes 17.17 seconds for 1x1 lat-lon, 1 day\n", "    # Query the server for EvalMetrics data\n", "    my_spec = c3.EvalMetricsSpec(\n", "                filter = filter,\n", "                limit = -1,\n", "                expressions = [metric],\n", "                start = start_time.strftime(\"%Y-%m-%dT%H:00:00\"),\n", "                end = end_time.strftime(\"%Y-%m-%dT%H:00:00\"),\n", "                interval = interval\n", "            )\n", "\n", "    evalMetricsResult = c3.HycomLatLongPair.evalMetrics(spec=my_spec)\n", "    \n", "    ###POST_QUERY SECTION###\n", "    #takes 0.02296 seconds for 1x1 lat-lon, 1 day\n", "    \n", "    #calculate number of discrete time points \n", "    #(currently calculates number of hours, need to modify to work for different intervals)\n", "    duration = end_time - start_time\n", "    duration_in_s = duration.total_seconds()\n", "    num_times = int(divmod(duration_in_s, 3600)[0])\n", "\n", "    #extract data into array\n", "    keys = sorted([key for key in evalMetricsResult.result])\n", "    arr = np.zeros(shape=(num_times,len(keys)))\n", "    for i in range(len(keys)):\n", "        arr[:, i] = np.array(evalMetricsResult.result[keys[i]][\"TestAverageWaterU\"].m_data, dtype=\"float32\")\n", "\n", "    arr = np.swapaxes(arr.reshape(num_times, lon_dim, lat_dim), 1, 2)\n", "    \n", "    return arr"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Testing behavior of EvalMetricSpec (candidate for deletion)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"ExecuteTime": {"end_time": "2021-11-12T18:37:57.555462Z", "start_time": "2021-11-12T18:37:55.267937Z"}}, "outputs": [], "source": ["lon_interval = [-90,-89]\n", "lat_interval = [24, 25]\n", "t_interval = [datetime(2021, 9, 2, 12, 0).timestamp(), datetime(2021, 9, 3, 12, 0).timestamp()]\n", "metric=\"TestAverageWaterU\"\n", "interval=\"HOUR\"\n", "\n", "###PRE-QUERY SECTION###\n", "#takes 0.1423 seconds for 1x1 lat-lon, 1 day\n", "#convert times to datetime\n", "start_time = datetime.fromtimestamp(t_interval[0])\n", "\n", "#+interval_len because EvalMetrics exclusive for end time (NOTE: modify to work for other intervals)\n", "end_time = datetime.fromtimestamp(t_interval[1] + 3600) \n", "\n", "\n", "#filter for query\n", "filter = \"lat>={} && lat<={} && lon>={} && lon<={}\".format(lat_interval[0], lat_interval[1], \n", "                                                           lon_interval[0], lon_interval[1])\n", "\n", "#get lat, lon dimensions\n", "objs_list = c3.HycomLatLongPair.fetch(spec={\"include\": \"id, lat, lon\", \"filter\": filter}).objs\n", "lat_dim =  len(np.unique([obj.lat for obj in objs_list]))\n", "lon_dim = len(np.unique([obj.lon for obj in objs_list]))\n", "\n", "my_spec = c3.EvalMetricsSpec(\n", "                filter = filter,\n", "                limit = 10,\n", "                expressions = [metric],\n", "                start = start_time.strftime(\"%Y-%m-%dT%H:00:00\"),\n", "                end = end_time.strftime(\"%Y-%m-%dT%H:00:00\"),\n", "                interval = interval\n", "            )\n", "\n", "evalMetricsResult = c3.HycomLatLongPair.evalMetrics(spec=my_spec)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:05:40.820707Z", "start_time": "2021-11-20T02:05:40.820692Z"}}, "outputs": [], "source": ["# let's check for the point 24, -90 the values in the file"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"ExecuteTime": {"end_time": "2021-11-12T18:48:30.061032Z", "start_time": "2021-11-12T18:48:30.058675Z"}, "code_folding": []}, "outputs": [], "source": ["# evalMetricsResult.result[\"GOMu0.04_200-147\"]"]}, {"cell_type": "code", "execution_count": 26, "metadata": {"ExecuteTime": {"end_time": "2021-11-12T18:51:50.322348Z", "start_time": "2021-11-12T18:51:50.317340Z"}}, "outputs": [{"data": {"text/plain": ["array([-0.42000002, -0.40900001, -0.40700001, -0.40700001, -0.40300003,\n", "       -0.39600003, -0.38100001, -0.36500001, -0.34500003, -0.32800001,\n", "       -0.31800002, -0.32000002, -0.33500001, -0.36300001, -0.40200001,\n", "       -0.44700003, -0.49300003, -0.54200006, -0.59600002, -0.64100003,\n", "       -0.66700006, -0.67200005, -0.65200001, -0.61000001, -0.59100002])"]}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": ["# Note: these values are the same as in the DB so the query and function work correctly..\n", "np.array(evalMetricsResult.result[\"GOMu0.04_200-147\"][\"TestAverageWaterU\"].m_data)"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"ExecuteTime": {"end_time": "2021-11-12T18:52:02.008172Z", "start_time": "2021-11-12T18:52:02.005677Z"}}, "outputs": [], "source": ["# => that means, the data processing didn't work correctly?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Generate reference arrays"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:01:30.120326Z", "start_time": "2021-11-20T02:01:30.117583Z"}}, "outputs": [], "source": ["hindcast_file = 'Sanity_check_data.nc4'"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:01:30.850419Z", "start_time": "2021-11-20T02:01:30.756278Z"}, "code_folding": [0], "scrolled": true}, "outputs": [{"ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: b'Sanity_check_data.nc4'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_41513/3900371501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtemporal_stride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m file_grid_dict, solution_1day_1x1, file_v_water = get_current_data_subset(hindcast_file,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                   \u001b[0mx_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_T\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                   \u001b[0mdeg_around_x0_xT_box\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/tmp/ipykernel_41513/1819198816.py\u001b[0m in \u001b[0;36mget_current_data_subset\u001b[0;34m(nc_file, x_0, x_T, deg_around_x0_xT_box, fixed_time, temporal_stride, temp_horizon_in_h)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetCDF4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# extract positiond & start_time for the indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n", "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'Sanity_check_data.nc4'"]}], "source": ["# settings for directly from subsetted file and old approach\n", "x_0 = [-90.0, 24.0, 1, datetime(2021, 9, 2, 12, 0).timestamp()]  # lon, lat, battery, posix_time\n", "x_T = [-89.0, 25.0]\n", "deg_around_x0_xT_box = 0.\n", "fixed_time = None\n", "temporal_stride = 1\n", "\n", "file_grid_dict, solution_1day_1x1, file_v_water = get_current_data_subset(hindcast_file,\n", "                                                  x_0, x_T,\n", "                                                  deg_around_x0_xT_box,\n", "                                                  fixed_time,\n", "                                                  temporal_stride,\n", "                                                  temp_horizon_in_h=None)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Sanity Check"]}, {"cell_type": "code", "execution_count": 39, "metadata": {"ExecuteTime": {"end_time": "2021-11-20T02:22:24.349693Z", "start_time": "2021-11-20T02:22:22.457274Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n"]}], "source": ["lon_interval = [-90,-89]\n", "lat_interval = [24, 25]\n", "t_interval = [datetime(2021, 9, 2, 12, 0).timestamp(), datetime(2021, 9, 3, 12, 0).timestamp()]\n", "\n", "_, c3_file_u_data, _ = get_current_data_subset_from_c3_file(t_interval, lat_interval, lon_interval)\n", "# c3_database_u_data = get_current_data_subset_from_c3_database(t_interval, lat_interval, lon_interval)"]}, {"cell_type": "code", "execution_count": 51, "metadata": {"ExecuteTime": {"end_time": "2021-11-12T19:16:38.712919Z", "start_time": "2021-11-12T19:16:38.708816Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["c3_file function correct: True\n", "c3_database function correct: False\n"]}], "source": ["print(\"c3_file function correct:\", np.all(c3_file_u_data==solution_1day_1x1))\n", "print(\"c3_database function correct:\", np.all(c3_database_u_data==solution_1day_1x1))"]}, {"cell_type": "code", "execution_count": 114, "metadata": {"ExecuteTime": {"end_time": "2021-11-10T22:44:35.727993Z", "start_time": "2021-11-10T22:44:35.723052Z"}}, "outputs": [{"data": {"text/plain": ["-0.35630895178772637"]}, "execution_count": 114, "metadata": {}, "output_type": "execute_result"}], "source": ["np.mean(c3_database_u_data)"]}, {"cell_type": "code", "execution_count": 115, "metadata": {"ExecuteTime": {"end_time": "2021-11-10T22:44:38.393642Z", "start_time": "2021-11-10T22:44:38.389031Z"}}, "outputs": [{"data": {"text/plain": ["0.118622966"]}, "execution_count": 115, "metadata": {}, "output_type": "execute_result"}], "source": ["np.mean(solution_1day_1x1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Individual lat-lon point sanity check"]}, {"cell_type": "code", "execution_count": 52, "metadata": {"ExecuteTime": {"end_time": "2021-11-12T19:17:16.060447Z", "start_time": "2021-11-12T19:17:16.052946Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["lon  -90.0\n", "lat  24.0\n", "2021-09-02 12:00:00\n"]}, {"data": {"text/plain": ["array([0.25      , 0.294     , 0.34500003, 0.397     , 0.437     ,\n", "       0.46300003, 0.47200003, 0.46800002, 0.44700003, 0.41300002,\n", "       0.372     , 0.335     , 0.305     , 0.284     , 0.279     ,\n", "       0.28500003, 0.293     , 0.296     , 0.28300002, 0.25300002,\n", "       0.20700002, 0.16000001, 0.134     , 0.133     , 0.165     ],\n", "      dtype=float32)"]}, "execution_count": 52, "metadata": {}, "output_type": "execute_result"}], "source": ["# Get the numbers from file-based approach\n", "i, j = 0, 0\n", "t_idx = 0\n", "print(\"lon \", file_grid_dict['x_grid'][i])\n", "print(\"lat \", file_grid_dict['y_grid'][j])\n", "print(datetime.fromtimestamp(file_grid_dict['t_grid'][t_idx]))\n", "# File-based output matrix is [T, Y, X]\n", "# across time for the -90, 24 array!\n", "solution_1day_1x1[t_idx:,i,j]"]}, {"cell_type": "code", "execution_count": 54, "metadata": {"ExecuteTime": {"end_time": "2021-11-12T19:24:12.806791Z", "start_time": "2021-11-12T19:24:12.228113Z"}}, "outputs": [{"ename": "KeyError", "evalue": "'GOMu0.04_147-200'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)", "\u001b[0;32m<ipython-input-54-822ec9dca3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mevalMetricsResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHycomLatLongPair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalMetricsResult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GOMu0.04_147-200\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TestAverageWaterU\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mKeyError\u001b[0m: 'GOMu0.04_147-200'"]}], "source": ["# Get the numbers from c3 pipeline based approach\n", "lon_interval = [-90,-89]\n", "lat_interval = [24, 25]\n", "t_interval = [datetime(2021, 9, 2, 12, 0).timestamp(), datetime(2021, 9, 3, 12, 0).timestamp()]\n", "metric=\"TestAverageWaterU\"\n", "interval=\"HOUR\"\n", "\n", "start_time = datetime.fromtimestamp(t_interval[0])\n", "end_time = datetime.fromtimestamp(t_interval[1] + 3600) \n", "#filter for query\n", "filter = \"lat>={} && lat<={} && lon>={} && lon<={}\".format(lat_interval[0], lat_interval[1], \n", "                                                           lon_interval[0], lon_interval[1])\n", "#get lat, lon dimensions\n", "objs_list = c3.HycomLatLongPair.fetch(spec={\"include\": \"id, lat, lon\", \"filter\": filter}).objs\n", "lat_dim =  len(np.unique([obj.lat for obj in objs_list]))\n", "lon_dim = len(np.unique([obj.lon for obj in objs_list]))\n", "\n", "my_spec = c3.EvalMetricsSpec(\n", "                filter = filter,\n", "                limit = 10,\n", "                expressions = [metric],\n", "                start = start_time.strftime(\"%Y-%m-%dT%H:00:00\"),\n", "                end = end_time.strftime(\"%Y-%m-%dT%H:00:00\"),\n", "                interval = interval\n", "            )\n", "\n", "evalMetricsResult = c3.HycomLatLongPair.evalMetrics(spec=my_spec)\n", "\n", "print(np.array(evalMetricsResult.result[\"GOMu0.04_147-200\"][\"TestAverageWaterU\"].m_data))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Note: these values are the same as in the DB so the query and function work correctly.."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Run timing tests\n", "fixed 1x1 lat-lon grid, varied time frame"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T20:32:43.616255Z", "start_time": "2021-11-13T20:22:09.462340Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "***Trial 0***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 1***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 2***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 3***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 4***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 5***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n"]}], "source": ["num_trials = 6\n", "timeframe = 5\n", "\n", "#these are of shape (num_trials, timeframe)\n", "file_times_1x1 = np.zeros(shape=(num_trials, timeframe))\n", "database_times_1x1 = np.zeros(shape=(num_trials, timeframe))\n", "\n", "lon_interval = [-90,-89]\n", "lat_interval = [24, 25]\n", "\n", "for n in range(num_trials):\n", "    print(\"\\n***Trial {}***\".format(n))\n", "    for i in range(timeframe):\n", "        t_interval = [datetime(2021, 9, 2, 12, 0).timestamp(), datetime(2021, 9, 3 + i, 12, 0).timestamp()]\n", "\n", "        print(\"get data using files for {} days\".format(i+1))\n", "        start = time.time()\n", "        _, c3_file_u_data, _ = get_current_data_subset_from_c3_file(t_interval, lat_interval, lon_interval)\n", "        end = time.time()\n", "        file_times_1x1[n][i] = end - start\n", "        \n", "        print(\"get data using database for {} days\\n\".format(i+1))\n", "        start = time.time()\n", "        c3_database_u_data = get_current_data_subset_from_c3_database(t_interval, lat_interval, lon_interval)\n", "        end = time.time()\n", "        database_times_1x1[n][i] = end - start\n", "          "]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T20:32:43.623775Z", "start_time": "2021-11-13T20:32:43.618071Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["file:\n", "\tmean: [1.8538355  2.31808281 2.89785949 3.69447128 4.02903986]\n", "\tvar: [0.04045876 0.02098448 0.0314754  0.08742363 0.06771048]\n", "\n", "database:\n", "\tmean: [18.34618036 18.04307504 17.7885772  18.62299089 18.09564626]\n", "\tvar: [1.04474556 0.57995429 0.33370975 0.31703722 1.30089722]\n"]}], "source": ["file_var = np.var(file_times_1x1, axis=0)\n", "file_mean = np.mean(file_times_1x1, axis=0)\n", "database_var = np.var(database_times_1x1, axis=0)\n", "database_mean = np.mean(database_times_1x1, axis=0)\n", "\n", "print(\"file:\\n\\tmean: {}\\n\\tvar: {}\\n\\ndatabase:\\n\\tmean: {}\\n\\tvar: {}\".format(file_mean, file_var, database_mean, database_var))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["fixed 2x2 lat-lon grid, varied time frame"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T21:10:31.831798Z", "start_time": "2021-11-13T20:32:43.625797Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "***Trial 0***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "sanity check: file output shape is (25, 51, 51)\n", "sanity check: database output shape is (25, 51, 51)\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "sanity check: file output shape is (49, 51, 51)\n", "sanity check: database output shape is (49, 51, 51)\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "sanity check: file output shape is (73, 51, 51)\n", "sanity check: database output shape is (73, 51, 51)\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "sanity check: file output shape is (97, 51, 51)\n", "sanity check: database output shape is (97, 51, 51)\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "sanity check: file output shape is (121, 51, 51)\n", "sanity check: database output shape is (121, 51, 51)\n", "\n", "***Trial 1***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 2***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 3***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 4***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n", "\n", "***Trial 5***\n", "get data using files for 1 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-03 12:00:00 UTC in 25 time steps of 1.00 hour(s) resolution\n", "get data using database for 1 days\n", "\n", "get data using files for 2 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-04 12:00:00 UTC in 49 time steps of 1.00 hour(s) resolution\n", "get data using database for 2 days\n", "\n", "get data using files for 3 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using database for 3 days\n", "\n", "get data using files for 4 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-06 12:00:00 UTC in 97 time steps of 1.00 hour(s) resolution\n", "get data using database for 4 days\n", "\n", "get data using files for 5 days\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-07 12:00:00 UTC in 121 time steps of 1.00 hour(s) resolution\n", "get data using database for 5 days\n", "\n"]}], "source": ["num_trials = 6\n", "timeframe = 5\n", "\n", "#these are of shape (num_trials, timeframe)\n", "file_times_2x2 = np.zeros(shape=(num_trials, timeframe))\n", "database_times_2x2 = np.zeros(shape=(num_trials, timeframe))\n", "\n", "lon_interval = [-90,-88]\n", "lat_interval = [24, 26]\n", "\n", "for n in range(num_trials):\n", "    print(\"\\n***Trial {}***\".format(n))\n", "    for i in range(timeframe):\n", "        t_interval = [datetime(2021, 9, 2, 12, 0).timestamp(), datetime(2021, 9, 3 + i, 12, 0).timestamp()]\n", "\n", "        print(\"get data using files for {} days\".format(i+1))\n", "        start = time.time()\n", "        _, c3_file_u_data, _ = get_current_data_subset_from_c3_file(t_interval, lat_interval, lon_interval)\n", "        end = time.time()\n", "        file_times_2x2[n][i] = end - start\n", "        \n", "        print(\"get data using database for {} days\\n\".format(i+1))\n", "        start = time.time()\n", "        c3_database_u_data = get_current_data_subset_from_c3_database(t_interval, lat_interval, lon_interval)\n", "        end = time.time()\n", "        database_times_2x2[n][i] = end - start\n", "        \n", "        if n==0:\n", "            print(\"sanity check: file output shape is {}\".format(c3_file_u_data.shape))\n", "            print(\"sanity check: database output shape is {}\".format(c3_database_u_data.shape))\n", "        "]}, {"cell_type": "code", "execution_count": 8, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T21:10:31.838819Z", "start_time": "2021-11-13T21:10:31.833517Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["file:\n", "\tmean: [1.70359941 2.27754378 4.35405966 5.6173619  3.96298444]\n", "\tvar: [1.11089262e-02 6.74401598e-03 8.34421237e+00 2.12287547e+01\n", " 3.41539796e-02]\n", "\n", "database:\n", "\tmean: [70.64219558 74.12515362 69.7883081  71.59635059 73.95672381]\n", "\tvar: [ 8.25638858  7.52758677 12.77819794  9.44663634  2.33355941]\n"]}], "source": ["file_var = np.var(file_times_2x2, axis=0)\n", "file_mean = np.mean(file_times_2x2, axis=0)\n", "database_var = np.var(database_times_2x2, axis=0)\n", "database_mean = np.mean(database_times_2x2, axis=0)\n", "\n", "print(\"file:\\n\\tmean: {}\\n\\tvar: {}\\n\\ndatabase:\\n\\tmean: {}\\n\\tvar: {}\".format(file_mean, file_var, database_mean, database_var))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["varied lat-lon grid size, fixed time scale of 3 days"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T23:03:13.887486Z", "start_time": "2021-11-13T21:28:46.446203Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "***Trial 0***\n", "get data using files for 1x1\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "sanity check: output shape is (73, 26, 26)\n", "get data using files for 2x2\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "sanity check: output shape is (73, 51, 51)\n", "get data using files for 3x3\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "sanity check: output shape is (73, 76, 76)\n", "get data using files for 4x4\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "sanity check: output shape is (73, 101, 101)\n", "get data using files for 5x5\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "sanity check: output shape is (73, 126, 126)\n", "get data using files for 6x6\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "sanity check: output shape is (73, 151, 151)\n", "\n", "***Trial 1***\n", "get data using files for 1x1\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 2x2\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 3x3\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 4x4\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 5x5\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 6x6\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "\n", "***Trial 2***\n", "get data using files for 1x1\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 2x2\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 3x3\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 4x4\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 5x5\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 6x6\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "\n", "***Trial 3***\n", "get data using files for 1x1\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 2x2\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 3x3\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 4x4\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 5x5\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 6x6\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "\n", "***Trial 4***\n", "get data using files for 1x1\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 2x2\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 3x3\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 4x4\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 5x5\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "get data using files for 6x6\n", "Subsetted data from 2021-09-02 12:00:00 UTC to 2021-09-05 12:00:00 UTC in 73 time steps of 1.00 hour(s) resolution\n", "\n", "***Trial 0***\n", "get data using database for 1x1\n", "\n", "sanity check: output shape is (73, 26, 26)\n", "get data using database for 2x2\n", "\n", "sanity check: output shape is (73, 51, 51)\n", "get data using database for 3x3\n", "\n", "sanity check: output shape is (73, 76, 76)\n", "get data using database for 4x4\n", "\n", "sanity check: output shape is (73, 101, 101)\n", "get data using database for 5x5\n", "\n", "sanity check: output shape is (73, 126, 126)\n", "get data using database for 6x6\n", "\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Json request to /api/1/dev/tc01d/HycomLatLongPair?action=evalMetrics failed with response ServerResponse(statusCode=504, content='<html>\\r\\n<head><title>504 Gateway Time-out</title></head>\\r\\n<body>\\r\\n<center><h1>504 Gateway Time-out</h1></center>\\r\\n<hr><center>Microsoft-Azure-Application-Gateway/v2</center>\\r\\n</body>\\r\\n</html>\\r\\n', headers={'Server': 'Microsoft-Azure-Application-Gateway/v2', 'Date': 'Sat, 13 Nov 2021 23:03:13 GMT', 'Content-Type': 'text/html', 'Content-Length': '193', 'Connection': 'keep-alive'})\n"]}, {"ename": "RuntimeError", "evalue": "Json request to /api/1/dev/tc01d/HycomLatLongPair?action=evalMetrics failed with response ServerResponse(statusCode=504, content='<html>\\r\\n<head><title>504 Gateway Time-out</title></head>\\r\\n<body>\\r\\n<center><h1>504 Gateway Time-out</h1></center>\\r\\n<hr><center>Microsoft-Azure-Application-Gateway/v2</center>\\r\\n</body>\\r\\n</html>\\r\\n', headers={'Server': 'Microsoft-Azure-Application-Gateway/v2', 'Date': 'Sat, 13 Nov 2021 23:03:13 GMT', 'Content-Type': 'text/html', 'Content-Length': '193', 'Connection': 'keep-alive'})", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)", "\u001b[0;32m<ipython-input-10-28f06bf5aa62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get data using database for {}x{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mc3_database_u_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_current_data_subset_from_c3_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdatabase_times_3_days\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-4-5b23186662e4>\u001b[0m in \u001b[0;36mget_current_data_subset_from_c3_database\u001b[0;34m(t_interval, lat_interval, lon_interval, metric, interval)\u001b[0m\n\u001b[1;32m     55\u001b[0m             )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mevalMetricsResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHycomLatLongPair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m###POST_QUERY SECTION###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32mZ10ac078826d8534179d455db1588a325M_.TypeSystemBase\u001b[0m in \u001b[0;36mevalMetrics\u001b[0;34m(_cls_f7f11c950a4e, spec)\u001b[0m\n", "\u001b[0;32mZ10ac078826d8534179d455db1588a325M_/TypeSystemBase.py\u001b[0m in \u001b[0;36m_c3_static_impl\u001b[0;34m(_cls, typesys, ___args, method_name, defined_class_name, _overload_idx)\u001b[0m\n", "\u001b[0;32mZ10ac078826d8534179d455db1588a325M_/TypeSystemBase.py\u001b[0m in \u001b[0;36m_box_and_request_json\u001b[0;34m(self, type_name, method_name, payload, validate_args, declared_type_name, current_class)\u001b[0m\n", "\u001b[0;32mZ10ac078826d8534179d455db1588a325M_/TypeSystemBase.py\u001b[0m in \u001b[0;36m_request_json\u001b[0;34m(self, type_name, method_name, payload)\u001b[0m\n", "\u001b[0;32mZ10ac078826d8534179d455db1588a325M_/ServerConnection.py\u001b[0m in \u001b[0;36m_request_json\u001b[0;34m(self, type_name, action_name, payload, extra_headers, encoder, client_side_type_inference)\u001b[0m\n", "\u001b[0;31mRuntimeError\u001b[0m: Json request to /api/1/dev/tc01d/HycomLatLongPair?action=evalMetrics failed with response ServerResponse(statusCode=504, content='<html>\\r\\n<head><title>504 Gateway Time-out</title></head>\\r\\n<body>\\r\\n<center><h1>504 Gateway Time-out</h1></center>\\r\\n<hr><center>Microsoft-Azure-Application-Gateway/v2</center>\\r\\n</body>\\r\\n</html>\\r\\n', headers={'Server': 'Microsoft-Azure-Application-Gateway/v2', 'Date': 'Sat, 13 Nov 2021 23:03:13 GMT', 'Content-Type': 'text/html', 'Content-Length': '193', 'Connection': 'keep-alive'})"]}], "source": ["num_trials = 5\n", "gridsize_range = 6\n", "\n", "#these are of shape (num_trials, gridsize_range)\n", "file_times_3_days = np.zeros(shape=(num_trials, gridsize_range))\n", "database_times_3_days = np.zeros(shape=(1, gridsize_range))\n", "\n", "lon_intervals = [[-90,-89], [-90,-88], [-90,-87], [-91,-87], [-92,-87], [-93,-87]]\n", "lat_intervals = [[24, 25], [24, 26], [24, 27], [23, 27], [22, 27], [22, 28]]\n", "t_interval = [datetime(2021, 9, 2, 12, 0).timestamp(), datetime(2021, 9, 5, 12, 0).timestamp()]\n", "\n", "for n in range(num_trials):\n", "    print(\"\\n***Trial {}***\".format(n))\n", "    for i in range(gridsize_range):\n", "        lon_interval = lon_intervals[i]\n", "        lat_interval = lat_intervals[i]\n", "        print(\"get data using files for {}x{}\".format(i+1, i+1))\n", "        start = time.time()\n", "        _, c3_file_u_data, _ = get_current_data_subset_from_c3_file(t_interval, lat_interval, lon_interval)\n", "        end = time.time()\n", "        file_times_3_days[n][i] = end - start\n", "        \n", "        if n==0:\n", "            print(\"sanity check: output shape is {}\".format(c3_file_u_data.shape))\n", "        \n", "        \n", "for n in range(1):\n", "    print(\"\\n***Trial {}***\".format(n))\n", "    for i in range(gridsize_range):    \n", "        lon_interval = lon_intervals[i]\n", "        lat_interval = lat_intervals[i]\n", "        print(\"get data using database for {}x{}\\n\".format(i+1, i+1))\n", "        start = time.time()\n", "        c3_database_u_data = get_current_data_subset_from_c3_database(t_interval, lat_interval, lon_interval)\n", "        end = time.time()\n", "        database_times_3_days[n][i] = end - start\n", "        \n", "        if n==0:\n", "            print(\"sanity check: output shape is {}\".format(c3_database_u_data.shape))\n", "        "]}, {"cell_type": "code", "execution_count": 16, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T23:22:59.305812Z", "start_time": "2021-11-13T23:22:59.300481Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["file:\n", "\tmean: [2.83002944 2.78199716 2.61506758 2.74448647 3.03419528 2.84414096]\n", "\tvar: [0.02529586 0.01998459 0.00340468 0.00780639 0.61341426 0.01870923]\n", "\n", "database:\n", "\tmean: [  16.83094835   69.68803072  178.31722736  286.96635199 1427.42446494\n", "    0.        ]\n", "\tvar: [0. 0. 0. 0. 0. 0.]\n"]}], "source": ["file_var = np.var(file_times_3_days, axis=0)\n", "file_mean = np.mean(file_times_3_days, axis=0)\n", "database_var = np.var(database_times_3_days, axis=0)\n", "database_mean = np.mean(database_times_3_days, axis=0)\n", "\n", "print(\"file:\\n\\tmean: {}\\n\\tvar: {}\\n\\ndatabase:\\n\\tmean: {}\\n\\tvar: {}\".format(file_mean, file_var, database_mean, database_var))"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T23:34:16.177759Z", "start_time": "2021-11-13T23:34:16.173380Z"}}, "outputs": [], "source": ["#save data\n", "import pickle\n", "d = {\"file_times_1x1\": file_times_1x1, \"file_times_2x2\": file_times_2x2, \n", "     \"database_times_1x1\": database_times_1x1, \"database_times_2x2\": database_times_2x2, \n", "     \"file_times_3_days\": file_times_3_days, \"database_times_3_days\": database_times_3_days}\n", "\n", "filename = \"test_output.txt\"\n", "f = open(filename,'wb')\n", "\n", "pickle.dump(d,f)\n", "f.close()"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"ExecuteTime": {"end_time": "2021-11-13T23:34:30.241929Z", "start_time": "2021-11-13T23:34:30.238651Z"}}, "outputs": [], "source": ["#load data\n", "f = open(filename,'rb')\n", "new_d = pickle.load(f)\n", "f.close()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"has_local_update": false, "is_local": true, "is_remote": true, "kernelspec": {"display_name": "py-ocean_sim_cpu_test", "language": "Python", "name": "py-ocean_sim_cpu_test"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}, "last_sync_time": "2021-11-20T02:00:32.333121"}, "nbformat": 4, "nbformat_minor": 4}