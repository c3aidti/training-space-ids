{
    "type": "JupyterNotebook",
    "contentJson": {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true
                },
                "source": [
                    "# Verify NVIDIA/CUDA device"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 1,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-14T12:13:10.573517Z",
                        "start_time": "2021-08-14T12:13:09.928778Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "/bin/bash: nvidia-smi: command not found\r\n"
                        ]
                    }
                ],
                "source": [
                    "smi = !which nvidia-smi\n",
                    "if(smi):\n",
                    "    !nvidia-smi\n",
                    "else:\n",
                    "    print('No NVIDIA device found. Are you sure you are on a GPU node?')"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true,
                    "hidden": true
                },
                "source": [
                    "## Verify device with PyTorch"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 4,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:58:52.255152Z",
                        "start_time": "2021-08-12T17:58:52.250174Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "Torch version: 1.9.0+cu102\n",
                            "Number of CUDA Devices: 1\n",
                            "CUDA Device Name: Tesla K80\n",
                            "CUDA Device Total Memory [GB]: 11.996954624\n",
                            "CUDA version: 10.2\n"
                        ]
                    }
                ],
                "source": [
                    "import torch\n",
                    "\n",
                    "print('Torch version:', torch.__version__)\n",
                    "use_cuda = torch.cuda.is_available()\n",
                    "if use_cuda:\n",
                    "    print('Number of CUDA Devices:', torch.cuda.device_count())\n",
                    "    print('CUDA Device Name:',torch.cuda.get_device_name(0))\n",
                    "    print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
                    "    print('CUDA version:', torch.version.cuda)\n",
                    "else:\n",
                    "    print('No CUDA-GPU available from Torch perspective.')"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true,
                    "hidden": true
                },
                "source": [
                    "## Verify device with TensorFlow"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 5,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:04.339329Z",
                        "start_time": "2021-08-12T17:58:52.256889Z"
                    },
                    "hidden": true,
                    "scrolled": false
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "TF version: 2.4.1\n",
                            "TF detected devices: [name: \"/device:CPU:0\"\n",
                            "device_type: \"CPU\"\n",
                            "memory_limit: 268435456\n",
                            "locality {\n",
                            "}\n",
                            "incarnation: 2949445171790545685\n",
                            ", name: \"/device:GPU:0\"\n",
                            "device_type: \"GPU\"\n",
                            "memory_limit: 11139884032\n",
                            "locality {\n",
                            "  bus_id: 1\n",
                            "  links {\n",
                            "  }\n",
                            "}\n",
                            "incarnation: 1554928961047557063\n",
                            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\"\n",
                            "]\n",
                            "Is TF built with CUDA? True\n",
                            "List GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
                        ]
                    }
                ],
                "source": [
                    "import tensorflow as tf\n",
                    "from tensorflow.python.client import device_lib\n",
                    "\n",
                    "print(\"TF version:\", tf.__version__)\n",
                    "print(\"TF detected devices:\", device_lib.list_local_devices())\n",
                    "print(\"Is TF built with CUDA?\", tf.test.is_built_with_cuda())\n",
                    "print(\"List GPUs:\", tf.config.list_physical_devices('GPU'))"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true
                },
                "source": [
                    "# Testing PyTorch\n",
                    "\n",
                    "Here it is just to test memory allocation in host and device. If you want to add more, go ahead!"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 6,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:06.467544Z",
                        "start_time": "2021-08-12T17:59:04.341306Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "Tensors are allocated on:  cpu  and  cpu\n",
                            "Tensors are allocated on:  cpu  and  cuda:0\n"
                        ]
                    }
                ],
                "source": [
                    "import torch\n",
                    "\n",
                    "t1 = torch.tensor([\n",
                    "    [1,2],\n",
                    "    [3,4]\n",
                    "])\n",
                    "\n",
                    "t2 = torch.tensor([\n",
                    "    [5,6],\n",
                    "    [7,8]\n",
                    "])\n",
                    "\n",
                    "print(\"Tensors are allocated on: \", t1.device, \" and \", t2.device)\n",
                    "\n",
                    "t2 = t2.to('cuda')\n",
                    "\n",
                    "print(\"Tensors are allocated on: \", t1.device, \" and \", t2.device)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true
                },
                "source": [
                    "# Testing TensorFlow\n",
                    "\n",
                    "Taken from here: https://gist.github.com/j-min/baae1aa56e861cab9831b3722755ae6d"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true,
                    "hidden": true
                },
                "source": [
                    "## Matrix operations"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 7,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:06.551160Z",
                        "start_time": "2021-08-12T17:59:06.469086Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "WARNING:tensorflow:From /home/c3/.conda/envs/py-gpu_basic/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
                            "Instructions for updating:\n",
                            "non-resource variables are not supported in the long term\n"
                        ]
                    }
                ],
                "source": [
                    "import numpy as np\n",
                    "from time import perf_counter\n",
                    "import tensorflow.compat.v1 as tf\n",
                    "tf.disable_v2_behavior()\n",
                    "\n",
                    "# define matrix operation A^n + B^n\n",
                    "def matpow(M, n):\n",
                    "    \"\"\"\n",
                    "    Takes in matrix M and multiplies it by itself n-1 times.\n",
                    "    \"\"\"\n",
                    "    if n < 1:\n",
                    "        return M\n",
                    "    else:\n",
                    "        return tf.matmul(M, matpow(M,n-1))\n",
                    "    \n",
                    "# define A, B, n    \n",
                    "size = 1000\n",
                    "A = np.random.rand(size, size).astype('float32')\n",
                    "B = np.random.rand(size, size).astype('float32')\n",
                    "n = 10"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "hidden": true
                },
                "source": [
                    "### GPU multiplication"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 8,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:10.514804Z",
                        "start_time": "2021-08-12T17:59:06.552709Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "Device mapping:\n",
                            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n",
                            "\n",
                            "GPU time: 3.94389 s\n"
                        ]
                    }
                ],
                "source": [
                    "c1 = []\n",
                    "c2 = []\n",
                    "\n",
                    "with tf.device('/gpu:0'):\n",
                    "    a = tf.placeholder(tf.float32, [size, size])\n",
                    "    b = tf.placeholder(tf.float32, [size, size])\n",
                    "    c1.append(matpow(a, n))\n",
                    "    c2.append(matpow(b, n))\n",
                    "\n",
                    "with tf.device('/cpu:0'):\n",
                    "  sum = tf.add_n(c1)\n",
                    "\n",
                    "start_time = perf_counter()\n",
                    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
                    "    # Run the op.\n",
                    "    sess.run(sum, {a:A, b:B})\n",
                    "stop_time = perf_counter()\n",
                    "\n",
                    "print('GPU time: %g s' % (stop_time-start_time))"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "hidden": true
                },
                "source": [
                    "### CPU multiplication"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 9,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:10.518595Z",
                        "start_time": "2021-08-12T17:59:10.516378Z"
                    },
                    "hidden": true
                },
                "outputs": [],
                "source": [
                    "# I'm leaving this cell commented \n",
                    "# If you select \"Run All\" cells you're gonna get stuck here for ~5 minutes,\n",
                    "# which is useless cause this is supposed to test GPU usage, not cpu.\n",
                    "# Uncomment this if you want to really see it\n",
                    "\n",
                    "#c1 = []\n",
                    "#c2 = []\n",
                    "#with tf.device('/cpu:0'):\n",
                    "#    a = tf.placeholder(tf.float32, [size, size])\n",
                    "#    b = tf.placeholder(tf.float32, [size, size])\n",
                    "#    c1.append(matpow(a, n))\n",
                    "#    c2.append(matpow(b, n))\n",
                    "#with tf.device('/cpu:0'):\n",
                    "#  sum = tf.add_n(c1)\n",
                    "#start_time = perf_counter()\n",
                    "#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
                    "#    sess.run(sum, {a:A, b:B})\n",
                    "#stop_time = perf_counter()\n",
                    "#print('CPU time: %g s' % (stop_time-start_time))"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true
                },
                "source": [
                    "# Testing Numba \n",
                    "\n",
                    "Taken from here: https://github.com/keipertk/pygpu-workshop"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 10,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:13.397141Z",
                        "start_time": "2021-08-12T17:59:10.520088Z"
                    },
                    "hidden": true
                },
                "outputs": [],
                "source": [
                    "import numpy as np\n",
                    "from time import perf_counter\n",
                    "from numba import vectorize"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 11,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:15.188028Z",
                        "start_time": "2021-08-12T17:59:13.398690Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "[2. 2. 2. ... 2. 2. 2.]\n",
                            "Elapsed time with target CUDA: 0.635786 s\n"
                        ]
                    }
                ],
                "source": [
                    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
                    "def add_vec(v1, v2):\n",
                    "    return v1 + v2\n",
                    "\n",
                    "\n",
                    "\n",
                    "N=1<<22\n",
                    "\n",
                    "a = np.ones(N, dtype=np.float32)\n",
                    "b = np.ones(N, dtype=np.float32)\n",
                    "c = np.empty_like(a, dtype=a.dtype)\n",
                    "\n",
                    "start_time = perf_counter()\n",
                    "c = add_vec(a,b)\n",
                    "stop_time = perf_counter()\n",
                    "\n",
                    "print(c)\n",
                    "print('Elapsed time with target CUDA: %g s' % (stop_time-start_time))\n",
                    "\n",
                    "del a\n",
                    "del b\n",
                    "del c"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 12,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:15.396509Z",
                        "start_time": "2021-08-12T17:59:15.190367Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "[2. 2. 2. ... 2. 2. 2.]\n",
                            "Elapsed time with parallel: 0.0016773 s\n"
                        ]
                    }
                ],
                "source": [
                    "@vectorize(['float32(float32, float32)'], target='parallel')\n",
                    "def add_vec(v1, v2):\n",
                    "    return v1 + v2\n",
                    "\n",
                    "\n",
                    "\n",
                    "N=1<<22\n",
                    "\n",
                    "a = np.ones(N, dtype=np.float32)\n",
                    "b = np.ones(N, dtype=np.float32)\n",
                    "c = np.empty_like(a, dtype=a.dtype)\n",
                    "\n",
                    "start_time = perf_counter()\n",
                    "c = add_vec(a,b)\n",
                    "stop_time = perf_counter()\n",
                    "\n",
                    "print(c)\n",
                    "print('Elapsed time with parallel: %g s' % (stop_time-start_time))\n",
                    "\n",
                    "del a\n",
                    "del b\n",
                    "del c"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "hidden": true
                },
                "source": [
                    "Notice here that CUDA does not necessarily give you the best performance due to communication via the PCI bus."
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "heading_collapsed": true
                },
                "source": [
                    "# Testing CuPy\n",
                    "\n",
                    "Taken from: https://www.geeksforgeeks.org/python-cupy/"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 13,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T17:59:15.908674Z",
                        "start_time": "2021-08-12T17:59:15.398142Z"
                    },
                    "hidden": true
                },
                "outputs": [],
                "source": [
                    "import cupy as cp\n",
                    "import numpy as np\n",
                    "import time"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": 15,
                "metadata": {
                    "ExecuteTime": {
                        "end_time": "2021-08-12T18:00:20.353593Z",
                        "start_time": "2021-08-12T18:00:20.322677Z"
                    },
                    "hidden": true
                },
                "outputs": [
                    {
                        "name": "stdout",
                        "output_type": "stream",
                        "text": [
                            "Time consumed by numpy:  0.025693178176879883\n",
                            "\n",
                            "Time consumed by cupy:  0.0007393360137939453\n"
                        ]
                    }
                ],
                "source": [
                    "# NumPy and CPU Runtime\n",
                    "s = time.time()\n",
                    "x_cpu = np.ones((1000, 1000, 10))\n",
                    "e = time.time()\n",
                    "print(\"Time consumed by numpy: \", e - s)\n",
                    "  \n",
                    "# CuPy and GPU Runtime\n",
                    "s = time.time()\n",
                    "x_gpu = cp.ones((1000, 1000, 10))\n",
                    "e = time.time()\n",
                    "print(\"\\nTime consumed by cupy: \", e - s)"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": null,
                "metadata": {
                    "hidden": true
                },
                "outputs": [],
                "source": []
            }
        ],
        "metadata": {
            "has_local_update": false,
            "is_local": true,
            "is_remote": true,
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.7.9"
            },
            "last_sync_time": "2021-08-24T16:38:32.248336"
        },
        "nbformat": 4,
        "nbformat_minor": 2
    },
    "name": "gpu_basic.ipynb",
    "id": "dti-training/gpu_basic.ipynb",
    "path": "dti-training/gpu_basic.ipynb",
    "jupyterContentType": "notebook",
    "format": "json",
    "writable": false,
    "contentSize": 11067,
    "typeIdent": "NTBK",
    "acl": [
        {
            "canUpdate": false,
            "canRemove": false,
            "canModifyAcl": false,
            "member": {
                "id": "C3.Group.DataScience"
            }
        }
    ],
    "mlProject": "C3ai_Tutorials"
}